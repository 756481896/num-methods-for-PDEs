.. Automatically generated Sphinx-extended reStructuredText file from Doconce source
   (https://github.com/hplgit/doconce/)

Introduction to computing with finite difference methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

:Authors: Hans Petter Langtangen
:Date: Aug 21, 2014

Finite difference methods for partial differential equations (PDEs)
employ a range of concepts and tools that can be introduced and
illustrated in the context of simple ordinary differential equation
(ODE) examples.  This is what we do in the present document.  By
first working with ODEs, we keep the mathematical problems to be
solved as simple as possible (but no simpler), thereby allowing full
focus on understanding the key concepts and tools.  The choice of
topics in the forthcoming treatment of ODEs is therefore solely
dominated by what carries over to numerical methods for PDEs.

Theory and practice are primarily illustrated by solving the
very simple ODE :math:`u'=-au`, :math:`u(0)=I`, where :math:`a>0` is a constant,
but we also address the generalized problem :math:`u'=-a(t)u + b(t)`
and the nonlinear problem :math:`u'=f(u,t)`.
The following topics are introduced:

 * How to think when constructing finite difference methods, with special focus
   on the Forward Euler, Backward Euler, and Crank-Nicolson (midpoint)
   schemes

 * How to formulate a computational algorithm and translate it into
   Python code

 * How to make curve plots of the solutions

 * How to compute numerical errors

 * How to compute convergence rates

 * How to verify an implementation and automate verification
   through nose tests in Python

 * How to structure code in terms of functions, classes, and modules

 * How to work with Python concepts such as arrays, lists, dictionaries,
   lambda functions, functions in functions (closures), doctests,
   unit tests, command-line interfaces, graphical user interfaces

 * How to perform array computing and understand the difference from
   scalar computing

 * How to conduct and automate large-scale numerical experiments

 * How to generate scientific reports

 * How to uncover numerical artifacts in the computed solution

 * How to analyze the numerical schemes mathematically to understand
   why artifacts occur

 * How to derive mathematical expressions for various measures of
   the error in numerical methods, frequently by using the ``sympy`` software
   for symbolic computation

 * Introduce concepts such as finite difference operators,
   mesh (grid), mesh functions,
   stability, truncation error, consistency, and convergence

 * Present additional methods for the general nonlinear ODE :math:`u'=f(u,t)`,
   which is either a scalar ODE or a system of ODEs

 * How to access professional packages for solving ODEs

 * How the model equation :math:`u'=-au` arises in a wide range
   of phenomena in physics, biology, and finance


.. admonition:: The exposition in a nutshell

   Everything we cover is put into a practical, hands-on context. All
   mathematics is translated into working computing codes, and all the
   mathematical theory of finite difference methods presented here is
   motivated from a strong need to understand strange behavior of programs.
   Two fundamental questions saturate the text:
   
    * How to we solve a differential equation problem and produce numbers?
   
    * How to we trust the answer?




.. !split

.. _decay:basics:

Finite difference methods
=========================


.. admonition:: Goal

   We explain the basic ideas of finite difference methods
   using a simple ordinary differential equation :math:`u'=-au` as
   primary example.
   Emphasis is put on the reasoning when discretizing the problem and
   introduction of key concepts such as mesh, mesh function,
   finite difference approximations, averaging in a mesh,
   derivation of algorithms, and discrete operator notation.




.. _decay:model:

A basic model for exponential decay
-----------------------------------

.. index:: decay ODE

.. index:: exponential decay

Our model problem is perhaps the simplest ordinary differential
equation (ODE):

.. math::
        
        u'(t) = -au(t),
        

Here, :math:`a>0` is a constant and :math:`u'(t)` means differentiation with
respect to time :math:`t`. This type of equation arises in a number of
widely different phenomena where some quantity :math:`u` undergoes
exponential reduction. Examples include radioactive decay, population
decay, investment decay, cooling of an object, pressure decay in the
atmosphere, and retarded motion in fluids (for some of these models,
:math:`a` can be negative as well), see the section :ref:`decay:app` for details
and motivation.  We have chosen this particular ODE not only because
its applications are relevant, but even more because studying
numerical solution methods for this simple ODE gives important insight
that can be reused in much more complicated settings, in particular
when solving diffusion-type partial differential equations.

The analytical solution of the ODE is found by the method of
separation of variables, which results in

.. math::
         u(t) = Ce^{-at},

for any arbitrary constant :math:`C`.
To formulate a mathematical problem for which there
is a unique solution, we need a condition to fix the value of :math:`C`.
This condition is known as the *initial condition* and stated as
:math:`u(0)=I`. That is, we know the
value :math:`I` of :math:`u` when the process starts at :math:`t=0`. The exact solution
is then :math:`u(t)=Ie^{-at}`.

We seek the solution :math:`u(t)` of the ODE for :math:`t\in (0,T]`. The point :math:`t=0` is not
included since we know :math:`u` here and assume that the equation governs
:math:`u` for :math:`t>0`. The complete ODE problem then reads: find :math:`u(t)`
such that

.. math::
   :label: decay:problem
        
        u' = -au,\ t\in (0,T], \quad u(0)=I{\thinspace .}   
        

This is known as a *continuous problem* because the parameter :math:`t`
varies continuously from :math:`0` to :math:`T`. For each :math:`t` we have a corresponding
:math:`u(t)`. There are hence infinitely many values of :math:`t` and :math:`u(t)`.
The purpose of a numerical method is to formulate a corresponding
*discrete* problem whose solution is characterized by a finite number of values,
which can be computed in a finite number of steps on a computer.

.. _decay:schemes:FE:

The Forward Euler scheme
------------------------

Solving an ODE like :eq:`decay:problem` by a finite difference method
consists of the following four steps:

1. discretizing the domain,

2. fulfilling the equation at discrete time points,

3. replacing derivatives by finite differences,

4. formulating a recursive algorithm.

.. index:: mesh

.. index:: grid

Step 1: Discretizing the domain
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The time domain :math:`[0,T]` is represented by a finite number of
:math:`N_t+1` points

.. math::
        
        0 = t_0 < t_1 < t_2 < \cdots < t_{N_t-1} < t_{N_t} = T{\thinspace .}
        

The collection of points :math:`t_0,t_1,\ldots,t_{N_t}` constitutes a *mesh*
or *grid*. Often the mesh points will be uniformly spaced in
the domain :math:`[0,T]`, which means that the spacing :math:`t_{n+1}-t_n` is
the same for all :math:`n`. This spacing is often denoted by :math:`\Delta t`,
in this case :math:`t_n=n\Delta t`.

.. index:: mesh function

We seek the solution :math:`u` at the mesh points:
:math:`u(t_n)`, :math:`n=1,2,\ldots,N_t`. Note that :math:`u^0` is already known as :math:`I`.
A notational short-form for :math:`u(t_n)`,
which will be used extensively, is :math:`u^{n}`. More precisely, we let
:math:`u^n` be the *numerical approximation* to the exact solution :math:`u(t_n)`
at :math:`t=t_n`. The numerical approximation is a *mesh function*,
here defined only at the mesh points.
When we need to clearly distinguish between the numerical
and the exact solution, we often place a subscript e on the exact
solution, as in :math:`{u_{\small\mbox{e}}}(t_n)`. Figure :ref:`decay:fdu:e` shows the
:math:`t_n` and :math:`u_n` points for :math:`n=0,1,\ldots,N_t=7` as well as :math:`{u_{\small\mbox{e}}}(t)`
as the dashed line. The goal of a numerical method for ODEs is
to compute the mesh function by solving a finite set of
*algebraic equations* derived from the original ODE problem.

.. _decay:fdu:e:

.. figure:: fdm_u_ue.png
   :width: 600

   *Time mesh with discrete solution values*

Since finite difference methods produce solutions at the mesh
points only, it is an open question what the solution is between
the mesh points. One can use methods for interpolation to
compute the value of :math:`u` between mesh points. The simplest
(and most widely used) interpolation method is to assume that
:math:`u` varies linearly between the mesh points, see
Figure :ref:`decay:fdu:ei`. Given :math:`u^{n}`
and :math:`u^{n+1}`, the value of :math:`u` at some :math:`t\in [t_{n}, t_{n+1}]`
is by linear interpolation

.. math::
        
        u(t) \approx u^n + \frac{u^{n+1}-u^n}{t_{n+1}-t_n}(t - t_n){\thinspace .}
        

.. _decay:fdu:ei:

.. figure:: fdm_u_ui.png
   :width: 600

   *Linear interpolation between the discrete solution values (dashed curve is exact solution)*

Step 2: Fulfilling the equation at discrete time points
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ODE is supposed to hold for all :math:`t\in (0,T]`, i.e., at an infinite
number of points. Now we relax that requirement and require that
the ODE is fulfilled at a finite set of discrete points in time.
The mesh points :math:`t_0,t_1,\ldots,t_{N_t}` are a natural
(but not the only) choice of points.
The original ODE is then reduced to  the following :math:`N_t` equations:

.. math::
   :label: decay:step2
        
        u'(t_n) = -au(t_n),\quad n=0,\ldots,N_t{\thinspace .}
        
        

.. index:: finite differences

Step 3: Replacing derivatives by finite differences
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The next and most essential step of the method is to replace the
derivative :math:`u'` by a finite difference approximation. Let us first
try a one-sided difference approximation (see Figure :ref:`decay:sketch:FE`),

.. index:: forward difference

.. index::
   single: finite differences; forward

.. math::
   :label: decay:FEdiff
        
        u'(t_n) \approx \frac{u^{n+1}-u^{n}}{t_{n+1}-t_n}{\thinspace .}
        
        

Inserting this approximation in :eq:`decay:step2` results in

.. math::
   :label: decay:step3
        
        \frac{u^{n+1}-u^{n}}{t_{n+1}-t_n} = -au^{n},\quad n=0,1,\ldots,N_t-1{\thinspace .}
        
        

Later it will be absolutely clear that if we want to compute the solution
up to time level :math:`N_t`,
we only need :eq:`decay:step2` to hold for :math:`n=0,\ldots,N_t-1` since
:eq:`decay:step3` for :math:`n=N_t-1` creates an equation for the final
value :math:`u^{N_t}`.

Equation :eq:`decay:step3`
is the discrete counterpart to the original ODE problem
:eq:`decay:problem`, and often referred to as *finite difference scheme*
or more generally as the *discrete equations* of the problem.
The fundamental feature of these equations is that they are *algebraic*
and can hence be straightforwardly solved to produce the mesh function, i.e.,
the values of :math:`u` at
the mesh points (:math:`u^n`, :math:`n=1,2,\ldots,N_t`).

.. _decay:sketch:FE:

.. figure:: fd_forward.png
   :width: 400

   *Illustration of a forward difference*

.. index:: difference equation

.. index:: discrete equation

.. index:: algebraic equation

.. index:: finite difference scheme

.. index:: Forward Euler scheme

Step 4: Formulating a recursive algorithm
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The final step is to identify the computational algorithm to be implemented
in a program. The key observation here is to realize that
:eq:`decay:step3` can be used to compute :math:`u^{n+1}` if :math:`u^n` is known.
Starting with :math:`n=0`, :math:`u^0` is known since :math:`u^0=u(0)=I`, and
:eq:`decay:step3` gives an equation for :math:`u^1`. Knowing :math:`u^1`,
:math:`u^2` can be found from :eq:`decay:step3`. In general, :math:`u^n`
in :eq:`decay:step3` can be assumed known, and then we can easily solve for
the unknown :math:`u^{n+1}`:

.. math::
   :label: decay:FE
        
        u^{n+1} = u^n - a(t_{n+1} -t_n)u^n{\thinspace .}
        
        

We shall refer to :eq:`decay:FE` as the Forward Euler (FE) scheme
for our model problem. From a mathematical point of view,
equations of the form :eq:`decay:FE` are known as
*difference equations* since they express how differences in
:math:`u`, like :math:`u^{n+1}-u^n`, evolve with :math:`n`.
The finite difference method can be viewed as a method for turning
a differential equation into a difference equation.

Computation with :eq:`decay:FE` is straightforward:

.. math::
        
        u_0 &= I,\\ 
        u_1 & = u^0 - a(t_{1} -t_0)u^0 = I(1-a(t_1-t_0)),\\ 
        u_2 & = u^1 - a(t_{2} -t_1)u^1 = I(1-a(t_1-t_0))(1 - a(t_2-t_1)),\\ 
        u^3 &= u^2 - a(t_{3} -t_2)u^2 = I(1-a(t_1-t_0))(1 - a(t_2-t_1))(1 - a(t_3-t_2)),
        

and so on until we reach :math:`u^{N_t}`.
Very often, :math:`t_{n+1}-t_n` is constant for all :math:`n`, so we can introduce
the common symbol :math:`\Delta t` for the time step:
:math:`\Delta t = t_{n+1}-t_n`, :math:`n=0,1,\ldots,N_t-1`.
Using a constant time step :math:`\Delta t` in the above calculations gives

.. math::
        
        u_0 &= I,\\ 
        u_1 & = I(1-a\Delta t),\\ 
        u_2 & = I(1-a\Delta t)^2,\\ 
        u^3 &= I(1-a\Delta t)^3,\\ 
        &\vdots\\ 
        u^{N_t} &= I(1-a\Delta t)^{N_t}{\thinspace .}
        

This means that we have found a closed formula for :math:`u^n`, and there is
no need to let a computer generate the sequence :math:`u^1, u^2, u^3, \ldots`.
However, finding such a formula for :math:`u^n` is possible only for a few very
simple problems, so in general finite difference equations must be
solved on a computer.

As the next sections will show, the scheme :eq:`decay:FE` is just one
out of many alternative finite difference (and other) methods for
the model problem :eq:`decay:problem`.

.. _decay:schemes:BE:

The Backward Euler scheme
-------------------------

.. index:: backward difference

.. index::
   single: finite differences; backward

There are several choices of difference approximations in step 3 of
the finite difference method as presented in the previous section.
Another alternative is

.. math::
   :label: decay:BEdiff
        
        u'(t_n) \approx \frac{u^{n}-u^{n-1}}{t_{n}-t_{n-1}}{\thinspace .}
        
        

Since this difference is based on going backward in time (:math:`t_{n-1}`)
for information, it is known as the Backward Euler difference.
Figure :ref:`decay:sketch:BE` explains the idea.

.. _decay:sketch:BE:

.. figure:: fd_backward.png
   :width: 400

   *Illustration of a backward difference*

.. index::
   single: backward scheme, 1-step

.. index:: Backward Euler scheme

Inserting :eq:`decay:BEdiff` in :eq:`decay:step2` yields
the Backward Euler (BE) scheme:

.. math::
   :label: decay:BE0
        
        \frac{u^{n}-u^{n-1}}{t_{n}-t_{n-1}} = -a u^n{\thinspace .}
        
        

We assume, as explained under step 4 in the section :ref:`decay:schemes:FE`,
that we have computed :math:`u^0, u^1, \ldots, u^{n-1}` such that
:eq:`decay:BE0` can be used to compute :math:`u^n`.
For direct similarity with the Forward Euler scheme :eq:`decay:FE`
we replace :math:`n` by :math:`n+1` in :eq:`decay:BE0` and solve for the
unknown value :math:`u^{n+1}`:

.. math::
   :label: decay:BE
        
        u^{n+1} = \frac{1}{1+ a(t_{n+1}-t_n)} u^n{\thinspace .}
        
        

.. _decay:schemes:CN:

The Crank-Nicolson scheme
-------------------------

.. index:: Crank-Nicolson scheme

.. index:: centered difference

.. index::
   single: finite differences; centered

The finite difference approximations used to derive the schemes
:eq:`decay:FE` and :eq:`decay:BE` are both one-sided differences,
known to be less accurate than central (or midpoint)
differences. We shall now construct
a central difference at :math:`t_{n+1/2}=\frac{1}{2} (t_n + t_{n+1})`, or
:math:`t_{n+1/2}=(n+\frac{1}{2})\Delta t` if the mesh spacing is uniform in time.
The approximation reads

.. math::
   :label: decay:CNdiff
        
        u'(t_{n+\frac{1}{2}}) \approx \frac{u^{n+1}-u^n}{t_{n+1}-t_n}{\thinspace .}
        
        

Note that the fraction on the right-hand side is the same as for the
Forward Euler approximation :eq:`decay:FEdiff` and
the Backward Euler approximation :eq:`decay:BEdiff` (with
:math:`n` replaced by :math:`n+1`). The accuracy of this fraction as an approximation
to the derivative of :math:`u` depends on *where* we seek the derivative:
in the center of the interval :math:`[t_{n},t_{n+1}]` or at the end points.

With the formula :eq:`decay:CNdiff`, where :math:`u'` is evaluated at
:math:`t_{n+1/2}`, it is natural to demand the
ODE to be fulfilled at the time points *between* the mesh points:

.. math::
   :label: decay:step2m
        
        u'(t_{n+\frac{1}{2}}) = -au(t_{n+\frac{1}{2}}),\quad n=0,
        \ldots,N_t-1{\thinspace .}
        
        

Using :eq:`decay:CNdiff` in :eq:`decay:step2m` results in

.. math::
   :label: decay:CN0
        
        \frac{u^{n+1}-u^n}{t_{n+1}-t_n} = -au^{n+\frac{1}{2}},
        
        

where :math:`u^{n+\frac{1}{2}}` is a short form for :math:`u(t_{n+\frac{1}{2}})`.
The problem is that we aim to compute :math:`u^n` for integer :math:`n`, implying that
:math:`u^{n+\frac{1}{2}}` is not a quantity computed by our method. It must
therefore be
expressed by the quantities that we actually produce, i.e.,
the numerical solution at the
mesh points. One possibility is to approximate :math:`u^{n+\frac{1}{2}}`
as an arithmetic mean of the :math:`u` values at the neighboring mesh points:

.. index::
   single: averaging; arithmetic

.. math::
   :label: decay:uhalfavg
        
        u^{n+\frac{1}{2}} \approx \frac{1}{2} (u^n + u^{n+1}){\thinspace .}
        
        

Using :eq:`decay:uhalfavg` in :eq:`decay:CN0` results in

.. math::
   :label: decay:CN1
        
        \frac{u^{n+1}-u^n}{t_{n+1}-t_n} = -a\frac{1}{2} (u^n + u^{n+1}){\thinspace .}
        
        

Figure :ref:`decay:sketch:CN` sketches the geometric interpretation of
such a centered difference.

.. _decay:sketch:CN:

.. figure:: fd_centered_CN.png
   :width: 400

   *Illustration of a centered difference*

We assume that :math:`u^n` is already computed so that :math:`u^{n+1}` is the
unknown, which we can solve for:

.. math::
   :label: decay:CN
        
        u^{n+1} = \frac{1-\frac{1}{2} a(t_{n+1}-t_n)}{1 + \frac{1}{2} a(t_{n+1}-t_n)}u^n{\thinspace .}
        
        

The finite difference scheme :eq:`decay:CN` is often called
the Crank-Nicolson (CN) scheme or a midpoint or centered scheme.

.. _decay:schemes:theta:

The unifying :math:`\theta`-rule
--------------------------------

.. index:: weighted average

.. index:: theta-rule

.. index:: theta-rule

The Forward Euler, Backward Euler, and Crank-Nicolson schemes can be
formulated as one scheme with a varying parameter :math:`\theta`:

.. math::
   :label: decay:th0
        
        \frac{u^{n+1}-u^{n}}{t_{n+1}-t_n} = -a (\theta u^{n+1} + (1-\theta) u^{n})
        
        {\thinspace .}
        

Observe:

 * :math:`\theta =0` gives the Forward Euler scheme

 * :math:`\theta =1` gives the Backward Euler scheme, and

 * :math:`\theta =\frac{1}{2}` gives the Crank-Nicolson scheme.

 * We may alternatively choose any other value of :math:`\theta` in :math:`[0,1]`.

As before, :math:`u^n` is considered known and :math:`u^{n+1}` unknown, so
we solve for the latter:

.. math::
   :label: decay:th
        
        u^{n+1} = \frac{1 - (1-\theta) a(t_{n+1}-t_n)}{1 + \theta a(t_{n+1}-t_n)}{\thinspace .}
        
        

This scheme is known as the :math:`\theta`-rule, or alternatively written as
the "theta-rule".


.. admonition:: Derivation

   We start with replacing :math:`u'` by the fraction
   
   .. math::
            \frac{u^{n+1}-u^{n}}{t_{n+1}-t_n},
   
   in the Forward Euler, Backward Euler,
   and Crank-Nicolson schemes. Then we observe that
   the difference between the methods concerns which point this
   fraction approximates the derivative. Or in other words, at which point we
   sample the ODE. So far this has been the
   end points or the midpoint of :math:`[t_n,t_{n+1}]`. However, we may choose any point
   :math:`\tilde t \in [t_n,t_{n+1}]`.
   The difficulty
   is that evaluating the right-hand side :math:`-au` at an arbitrary point
   faces the same problem as in
   the section :ref:`decay:schemes:CN`: the point value must be expressed
   by the discrete :math:`u` quantities that we compute by the scheme, i.e.,
   :math:`u^n` and :math:`u^{n+1}`. Following the averaging idea from
   the section :ref:`decay:schemes:CN`,
   the value of :math:`u` at an arbitrary point :math:`\tilde t` can be
   calculated as a *weighted average*, which generalizes the arithmetic mean
   :math:`\frac{1}{2} u^n + {\frac{1}{2}}u^{n+1}`.
   If we express :math:`\tilde t` as a weighted average
   
   .. math::
            t_{n+\theta} = \theta t_{n+1} + (1-\theta) t_{n},
   
   where :math:`\theta\in [0,1]` is the weighting factor, we can write
   
   .. math::
      :label: decay:thetaavg
           
           u(\tilde t) = u(\theta t_{n+1} + (1-\theta) t_{n}) \approx
           \theta u^{n+1} + (1-\theta) u^{n}{\thinspace .}
           
           
   
   We can now let the ODE hold at the point
   :math:`\tilde t\in [t_n,t_{n+1}]`, approximate :math:`u'` by the fraction
   :math:`(u^{n+1}-u^{n})/(t_{n+1}-t_n)`, and approximate the right-hand
   side :math:`-au` by the weighted average :eq:`decay:thetaavg`.
   The result is :eq:`decay:th0`.




Constant time step
------------------

All schemes up to now have been formulated for a general non-uniform
mesh in time: :math:`t_0,t_1,\ldots,t_{N_t}`. Non-uniform meshes are highly relevant
since one can use many points in regions where :math:`u` varies rapidly, and
save points in regions where :math:`u` is slowly varying. This is the key idea
of *adaptive* methods where the spacing of the mesh points
are determined as the computations proceed.

However, a uniformly distributed set of mesh points is very common and
sufficient for many applications. It therefore makes sense to
present the finite difference schemes for a uniform point distribution
:math:`t_n=n\Delta t`, where :math:`\Delta t` is the constant spacing between
the mesh points, also referred to as the *time step*.
The resulting formulas look simpler and are perhaps more
well known.


.. admonition:: Summary of schemes for constant time step

   
   .. math::
      :label: decay:FE:u
           
           u^{n+1} = (1 - a\Delta t )u^n  \quad (\hbox{FE})
           
           
   
   .. math::
      :label: decay:BE:u
             
           u^{n+1} = \frac{1}{1+ a\Delta t} u^n  \quad (\hbox{BE})
           
           
   
   .. math::
      :label: decay:CN:u
             
           u^{n+1} = \frac{1-\frac{1}{2} a\Delta t}{1 + \frac{1}{2} a\Delta t} u^n \quad (\hbox{CN})
           
           
   
   .. math::
      :label: decay:th:u
             
           u^{n+1} = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n \quad (\theta-\hbox{rule})




Not surprisingly, we present these three alternative schemes
because they have different pros and cons, both for the simple ODE
in question (which can easily be solved as accurately as desired), and for
more advanced differential equation problems.


.. admonition:: Test the understanding

   At this point it can be good training to apply the explained
   finite difference discretization techniques to a slightly
   different equation. :ref:`decay:app:exer:cooling:schemes`
   is therefore highly recommended to check that the key concepts
   are understood.




.. _decay:fd:op:

Compact operator notation for finite differences
------------------------------------------------

.. index:: finite difference operator notation

.. index::
   single: operator notation, finite differences

Finite difference formulas can be tedious to write and read,
especially for differential equations with many terms and many
derivatives. To save space and help the reader of the scheme to quickly
see the nature of the difference approximations, we introduce a
compact notation. A forward difference approximation is denoted
by the :math:`D_t^+` operator:

.. math::
   :label: fd:D:f
        
        [D_t^+u]^n = \frac{u^{n+1} - u^{n}}{\Delta t}
        \approx \frac{d}{dt} u(t_n) 
        {\thinspace .}
        

The notation consists of an operator that approximates
differentiation with respect to an independent variable, here :math:`t`.
The operator is built of the symbol :math:`D`, with the variable as subscript
and a superscript denoting the type of difference. The superscript :math:`\,{}^+`
indicates a forward difference.
We place square brackets around the operator and the function it operates
on and specify the mesh point, where the operator is acting, by
a superscript.

The corresponding operator notation for a centered difference and
a backward difference reads

.. math::
   :label: fd:D:c
        
        [D_tu]^n = \frac{u^{n+\frac{1}{2}} - u^{n-\frac{1}{2}}}{\Delta t}
        \approx \frac{d}{dt} u(t_n), 
        

and

.. math::
   :label: fd:D:b
        
        [D_t^-u]^n = \frac{u^{n} - u^{n-1}}{\Delta t}
        \approx \frac{d}{dt} u(t_n) 
        {\thinspace .}
        

Note that the superscript :math:`\,{}^-` denotes the backward
difference, while no superscript implies a central difference.

An averaging operator is also convenient to have:

.. math::
   :label: fd:mean:a
        
        [\overline{u}^{t}]^n = \frac{1}{2} (u^{n-\frac{1}{2}} + u^{n+\frac{1}{2}} )
        \approx u(t_n) 
        

The superscript :math:`t` indicates that the average is taken along the time
coordinate. The common average :math:`(u^n + u^{n+1})/2` can now be
expressed as :math:`[\overline{u}^{t}]^{n+\frac{1}{2}}`. (When also spatial coordinates
enter the problem, we need the explicit specification of the coordinate
after the bar.)

The Backward Euler finite difference approximation to :math:`u'=-au` can be written
as follows utilizing the compact notation:

.. math::
        
        [D_t^-u]^n = -au^n {\thinspace .}
        

In difference equations we often place the square brackets around
the whole equation, to indicate at which mesh point the equation applies,
since each term is supposed to be approximated at the same point:

.. math::
        
        [D_t^- u  = -au]^n {\thinspace .}
        

The Forward Euler scheme takes the form

.. math::
        
        [D_t^+ u  = -au]^n,
        

while the Crank-Nicolson scheme is written as

.. math::
   :label: fd:compact:ex:CN
        
        [D_t u = -a\overline{u}^t]^{n+\frac{1}{2}}{\thinspace .}
        
        


.. admonition:: Question

   Apply :eq:`fd:D:c` and :eq:`fd:mean:a` and write out the
   expressions to see that :eq:`fd:compact:ex:CN` is indeed the
   Crank-Nicolson scheme.




The :math:`\theta`-rule can be specified by

.. math::
   :label: decay:fd1:op:theta
        
        [\bar D_t u = -a\overline{u}^{t,\theta}]^{n+\theta},
        
        

if we define a new time difference

.. math::
   :label: decay:fd1:Du:theta
        
        \lbrack\bar D_t u\rbrack^{n+\theta} = \frac{u^{n+1}-u^n}{t^{n+1}-t^n},
        
        

and a *weighted averaging operator*

.. math::
   :label: decay:fd1:wmean:a
        
        \lbrack\overline{u}^{t,\theta}\rbrack^{n+\theta} = (1-\theta)u^{n} + \theta u^{n+1}
        \approx u(t_{n+\theta}),
        
        

where :math:`\theta\in [0,1]`. Note that for :math:`\theta =\frac{1}{2}` we recover
the standard centered difference and the standard arithmetic mean.
The idea in :eq:`decay:fd1:op:theta` is to sample the equation at
:math:`t_{n+\theta}`, use a skew difference at that
point :math:`[\bar D_t u]^{n+\theta}`, and a skew mean value.
An alternative notation is

.. math::
         [D_t u]^{n+\frac{1}{2}} = \theta [-au]^{n+1} + (1-\theta)[-au]^{n}{\thinspace .} 

Looking at the various examples above and comparing them with the
underlying differential equations, we see immediately which difference
approximations that have been used and at which point they
apply. Therefore, the compact notation effectively communicates the
reasoning behind turning a differential equation into a difference
equation.

.. !split

.. _decay:impl1:

Implementation
==============


.. admonition:: Goal

   We want make a computer program for solving
   
   .. math::
           
           u'(t) = -au(t),\quad t\in (0,T], \quad u(0)=I,
           
   
   by finite difference methods. The program should also display
   the numerical solution as a curve on the
   screen, preferably together with the
   exact solution.




.. index:: directory

.. index:: folder

All programs referred to in this section are found in the
`src/decay <http://tinyurl.com/jvzzcfn/decay>`__ directory (we use the classical
Unix term *directory* for what many others nowadays call *folder*).

**Mathematical problem.**
We want to explore the Forward Euler scheme, the
Backward Euler, and the Crank-Nicolson schemes applied to our model problem.
From an implementational point of view, it is advantageous to
implement the :math:`\theta`-rule

.. math::
        
        u^{n+1} = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n,
        

since it can generate the three other schemes by various of
choices of :math:`\theta`: :math:`\theta=0` for Forward Euler, :math:`\theta =1` for
Backward Euler, and :math:`\theta =1/2` for Crank-Nicolson.
Given :math:`a`, :math:`u^0=I`, :math:`T`, and :math:`\Delta t`,
our task is to use the :math:`\theta`-rule to
compute :math:`u^1, u^2,\ldots,u^{N_t}`, where :math:`t_{N_t}=N_t\Delta t`, and
:math:`N_t` the closest integer to :math:`T/\Delta t`.

**Computer Language: Python.**
Any programming language can be used to generate the :math:`u^{n+1}` values from
the formula above. However, in this document we shall mainly make use of
Python of several reasons:

  * Python has a very clean, readable syntax (often known as
    "executable pseudo-code").

  * Python code is very similar to MATLAB code (and MATLAB has a
    particularly widespread use for scientific computing).

  * Python is a full-fledged, very powerful programming language.

  * Python is similar to, but much simpler to work with and
    results in more reliable code than C++.

  * Python has a rich set of modules for scientific computing, and its
    popularity in scientific computing is rapidly growing.

  * Python was made for being combined with compiled languages
    (C, C++, Fortran) to reuse existing numerical software and to
    reach high computational performance of new implementations.

  * Python has extensive support for administrative task
    needed when doing large-scale computational investigations.

  * Python has extensive support for graphics (visualization,
    user interfaces, web applications).

  * FEniCS, a very powerful tool for solving PDEs by
    the finite element method, is most human-efficient to operate
    from Python.

Learning Python is easy. Many newcomers to the language will probably
learn enough from the forthcoming examples to perform their own computer
experiments. The examples start with simple Python code and gradually
make use of more powerful constructs as we proceed. As long as it is
not inconvenient for the problem at hand, our Python code is made as
close as possible to MATLAB code for easy transition between the two
languages.

Readers who feel the Python examples are too hard to follow will probably
benefit from reading a tutorial, e.g.,

  * `The Official Python Tutorial <http://docs.python.org/2/tutorial/>`__

  * `Python Tutorial on tutorialspoint.com <http://www.tutorialspoint.com/python/>`__

  * `Interactive Python tutorial site <http://www.learnpython.org/>`__

  * `A Beginner's Python Tutorial <http://en.wikibooks.org/wiki/A_Beginner's_Python_Tutorial>`__

The author also has a comprehensive book [Ref1]_ that teaches
scientific programming with Python from the ground up.

.. bumpy list of refs?

.. _decay:py1:

Making a solver function
------------------------

We choose to have an array ``u`` for storing the :math:`u^n` values, :math:`n=0,1,\ldots,N_t`.
The algorithmic steps are

 1. initialize :math:`u^0`

 2. for :math:`t=t_n`, :math:`n=1,2,\ldots,N_t`: compute :math:`u_n` using
    the :math:`\theta`-rule formula

Function for computing the numerical solution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following Python function takes the input data of the problem
(:math:`I`, :math:`a`, :math:`T`, :math:`\Delta t`, :math:`\theta`) as arguments and returns two arrays with
the solution :math:`u^0,\ldots,u^{N_t}` and the mesh points :math:`t_0,\ldots,t_{N_t}`,
respectively:

.. code-block:: python

        from numpy import *
        
        def solver(I, a, T, dt, theta):
            """Solve u'=-a*u, u(0)=I, for t in (0,T] with steps of dt."""
            Nt = int(T/dt)            # no of time intervals
            T = Nt*dt                 # adjust T to fit time step dt
            u = zeros(Nt+1)           # array of u[n] values
            t = linspace(0, T, Nt+1)  # time mesh
        
            u[0] = I                  # assign initial condition
            for n in range(0, Nt):    # n=0,1,...,Nt-1
                u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
            return u, t

The ``numpy`` library contains a lot of functions for array computing. Most
of the function names are similar to what is found
in the alternative scientific computing language MATLAB. Here
we make use of

 * ``zeros(Nt+1)`` for creating an array of a size ``Nt+1``
   and initializing the elements to zero

 * ``linspace(0, T, Nt+1)`` for creating an array with ``Nt+1``
   coordinates uniformly distributed between ``0`` and ``T``

The ``for`` loop deserves a comment, especially for newcomers to Python.
The construction ``range(0, Nt, s)`` generates all integers from ``0`` to ``Nt``
in steps of ``s``, *but not including* ``Nt``. Omitting ``s`` means ``s=1``.
For example, ``range(0, 6, 3)``
gives ``0`` and ``3``, while ``range(0, Nt)`` generates ``0``, ``1``, ..., ``Nt-1``.
Our loop implies the following assignments to ``u[n+1]``: ``u[1]``, ``u[2]``, ...,
``u[Nt]``, which is what we want since ``u`` has length ``Nt+1``.
The first index in Python arrays or lists is *always* ``0`` and the
last is then ``len(u)-1``. The length of an array ``u`` is obtained by
``len(u)`` or ``u.size``.

To compute with the ``solver`` function, we need to *call* it. Here
is a sample call:

.. code-block:: python

        u, t = solver(I=1, a=2, T=8, dt=0.8, theta=1)

Integer division
~~~~~~~~~~~~~~~~

The shown implementation of the ``solver`` may face problems and
wrong results if ``T``, ``a``, ``dt``, and ``theta`` are given as integers,
see :ref:`decay:exer:intdiv` and :ref:`decay:exer:decay1err`.
The problem is related to *integer division* in Python (as well as
in Fortran, C, C++, and many other computer languages): ``1/2`` becomes ``0``,
while ``1.0/2``, ``1/2.0``, or ``1.0/2.0`` all become ``0.5``. It is enough
that at least the nominator or the denominator is a real number
(i.e., a ``float`` object)
to ensure correct mathematical division. Inserting
a conversion ``dt = float(dt)``
guarantees that ``dt`` is
``float`` and avoids problems in :ref:`decay:exer:decay1err`.

Another problem with computing :math:`N_t=T/\Delta t` is that we should
round :math:`N_t` to the nearest integer. With ``Nt = int(T/dt)`` the ``int``
operation picks the largest integer smaller than ``T/dt``. Correct
mathematical rounding as known from school is obtained by

.. code-block:: python

        Nt = int(round(T/dt))

The complete version of our improved, safer ``solver`` function then becomes

.. code-block:: python

        from numpy import *
        
        def solver(I, a, T, dt, theta):
            """Solve u'=-a*u, u(0)=I, for t in (0,T] with steps of dt."""
            dt = float(dt)            # avoid integer division
            Nt = int(round(T/dt))     # no of time intervals
            T = Nt*dt                 # adjust T to fit time step dt
            u = zeros(Nt+1)           # array of u[n] values
            t = linspace(0, T, Nt+1)  # time mesh
        
            u[0] = I                  # assign initial condition
            for n in range(0, Nt):    # n=0,1,...,Nt-1
                u[n+1] = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u[n]
            return u, t

Doc strings
~~~~~~~~~~~

.. index:: doc strings

Right below the header line in the ``solver`` function there is a
Python string enclosed in triple double quotes ``"""``.
The purpose of this string object is to document what the function
does and what the arguments are. In this case the necessary
documentation do not span more than one line, but with triple double
quoted strings the text may span several lines:

.. code-block:: python

        def solver(I, a, T, dt, theta):
            """
            Solve
        
                u'(t) = -a*u(t),
        
            with initial condition u(0)=I, for t in the time interval
            (0,T]. The time interval is divided into time steps of
            length dt.
        
            theta=1 corresponds to the Backward Euler scheme, theta=0
            to the Forward Euler scheme, and theta=0.5 to the Crank-
            Nicolson method.
            """
            ...

Such documentation strings appearing right after the header of
a function are called *doc strings*. There are tools that can automatically
produce nicely formatted documentation by extracting the definition of
functions and the contents of doc strings.

It is strongly recommended to equip any function whose purpose
is not obvious with a doc string. Nevertheless, the forthcoming
text deviates from this rule if the function is explained in the text.

Formatting of numbers
~~~~~~~~~~~~~~~~~~~~~

Having computed the discrete solution ``u``, it is natural to look at
the numbers:

.. code-block:: python

        # Write out a table of t and u values:
        for i in range(len(t)):
            print t[i], u[i]

This compact ``print`` statement gives unfortunately quite ugly output
because the ``t`` and ``u`` values are not aligned in nicely formatted columns.
To fix this problem, we recommend to use the *printf format*, supported most
programming languages inherited from C. Another choice is
Python's recent *format string syntax*.

.. index:: printf format

Writing ``t[i]`` and ``u[i]`` in two nicely formatted columns is done like
this with the printf format:

.. code-block:: python

        print 't=%6.3f u=%g' % (t[i], u[i])

The percentage signs signify "slots" in the text where the variables
listed at the end of the statement are inserted. For each "slot" one
must specify a format for how the variable is going to appear in the
string: ``s`` for pure text, ``d`` for an integer, ``g`` for a real number
written as compactly as possible, ``9.3E`` for scientific notation with
three decimals in a field of width 9 characters (e.g., ``-1.351E-2``),
or ``.2f`` for standard decimal notation with two decimals
formatted with minimum width. The printf syntax provides a quick way
of formatting tabular output of numbers with full control of the
layout.

.. index:: format string syntax (Python)

The alternative *format string syntax* looks like

.. code-block:: python

        print 't={t:6.3f} u={u:g}'.format(t=t[i], u=u[i])

As seen, this format allows logical names in the "slots" where
``t[i]`` and ``u[i]`` are to be inserted. The "slots" are surrounded
by curly braces, and the logical name is followed by a colon and
then the printf-like specification of how to format real numbers,
integers, or strings.

Running the program
~~~~~~~~~~~~~~~~~~~

The function and main program shown above must be placed in a file,
say with name `decay_v1.py <http://tinyurl.com/jvzzcfn/decay/decay_v1.py>`__ (``v1`` for 1st version of this program).  Make sure you
write the code with a suitable text editor (Gedit, Emacs, Vim,
Notepad++, or similar).  The program is run by executing the file this
way:

.. code-block:: text

        Terminal> python decay_v1.py

The text ``Terminal>`` just indicates a prompt in a
Unix/Linux or DOS terminal window. After this prompt, which will look
different in your terminal window, depending on the terminal application
and how it is set up, commands like ``python decay_v1.py`` can be issued.
These commands are interpreted by the operating system.

We strongly recommend to run Python programs within the IPython shell.
First start IPython by typing ``ipython`` in the terminal window.
Inside the IPython shell, our program ``decay_v1.py`` is run by the command
``run decay_v1.py``:

.. code-block:: text

        Terminal> ipython
        
        In [1]: run decay_v1.py
        t= 0.000 u=1
        t= 0.800 u=0.384615
        t= 1.600 u=0.147929
        t= 2.400 u=0.0568958
        t= 3.200 u=0.021883
        t= 4.000 u=0.00841653
        t= 4.800 u=0.00323713
        t= 5.600 u=0.00124505
        t= 6.400 u=0.000478865
        t= 7.200 u=0.000184179
        t= 8.000 u=7.0838e-05
        
        In [2]:

The advantage of running programs in IPython are many: previous commands
are easily recalled with the up arrow, ``%pdb`` turns on debugging so that
variables can be examined if the program
aborts due to an exception, output of commands are stored in variables,
programs and statements can be profiled,
any operating system command can be executed, modules can be loaded
automatically and other customizations can be performed when starting
IPython -- to mention a few of the most
useful features.

Although running programs in IPython is strongly recommended, most
execution examples in the forthcoming text use the standard
Python shell with prompt ``>>>`` and run programs through
a typesetting like

.. code-block:: text

        Terminal> python programname

The reason is that such typesetting
makes the text more compact in the vertical direction
than showing sessions with IPython syntax.

.. index:: plotting curves

.. index:: visualizing curves

Plotting the solution
~~~~~~~~~~~~~~~~~~~~~

Having the ``t`` and ``u`` arrays, the approximate solution ``u`` is visualized
by the intuitive command ``plot(t, u)``:

.. code-block:: python

        from matplotlib.pyplot import *
        plot(t, u)
        show()

It will be illustrative to also plot :math:`{u_{\small\mbox{e}}}(t)` for comparison. We first
need to make a function for computing the analytical solution :math:`{u_{\small\mbox{e}}}(t)=Ie^{-at}`
of the model problem:

.. code-block:: python

        def exact_solution(t, I, a):
            return I*exp(-a*t)

It is tempting to just do

.. code-block:: python

        u_e = exact_solution(t, I, a)
        plot(t, u, t, u_e)

However, this is not exactly what we want: the ``plot`` function draws
straight lines between the discrete points ``(t[n], u_e[n])`` while
:math:`{u_{\small\mbox{e}}}(t)` varies as an exponential function between the mesh points.
The technique for showing the "exact" variation of :math:`{u_{\small\mbox{e}}}(t)` between
the mesh points is to introduce a very fine mesh for :math:`{u_{\small\mbox{e}}}(t)`:

.. code-block:: python

        t_e = linspace(0, T, 1001)      # fine mesh
        u_e = exact_solution(t_e, I, a)

We can also plot the curves with different colors and styles, e.g.,

.. code-block:: python

        plot(t_e, u_e, 'b-',         # blue line for u_e
             t,   u,   'r--o')       # red dashes w/circles

With more than one curve in the plot we need to associate each curve
with a legend. We also want appropriate names on the axis, a title,
and a file containing the plot as an image for inclusion in reports.
The Matplotlib package (``matplotlib.pyplot``) contains functions for
this purpose. The names of the functions are similar to the plotting
functions known from MATLAB.  A complete function for creating
the comparison plot becomes

.. code-block:: python

        from matplotlib.pyplot import *
        
        def plot_numerical_and_exact(theta, I, a, T, dt):
            """Compare the numerical and exact solution in a plot."""
            u, t = solver(I=I, a=a, T=T, dt=dt, theta=theta)
        
            t_e = linspace(0, T, 1001)        # fine mesh for u_e
            u_e = exact_solution(t_e, I, a)
        
            plot(t,   u,   'r--o',            # red dashes w/circles
                 t_e, u_e, 'b-')              # blue line for exact sol.
            legend(['numerical', 'exact'])
            xlabel('t')
            ylabel('u')
            title('theta=%g, dt=%g' % (theta, dt))
            savefig('plot_%s_%g.png' % (theta, dt))
        
        plot_numerical_and_exact(I=1, a=2, T=8, dt=0.8, theta=1)
        show()

Note that ``savefig`` here creates a PNG file whose name reflects the
values of :math:`\theta` and :math:`\Delta t` so that we can easily distinguish
files from different runs with :math:`\theta` and :math:`\Delta t`.

The complete code is found in the file
`decay_v2.py <http://tinyurl.com/jvzzcfn/decay/decay_v2.py>`__. The resulting plot
is shown in Figure :ref:`decay:fig:v2`. As seen, there is quite some
discrepancy between the exact and the numerical solution.
Fortunately, the numerical solution approaches the exact one as
:math:`\Delta t` is reduced.

.. figure:: decay_v2.png
   :width: 500

Verifying the implementation
----------------------------

It is easy to make mistakes while deriving and implementing numerical
algorithms, so we should never believe in the solution before it has
been thoroughly verified.  The most obvious idea to verify the
computations is to compare the numerical solution with the exact
solution, when that exists, but there will always be a discrepancy
between these two solutions because of the numerical
approximations. The challenging question is whether we have the
mathematically correct discrepancy or if we have another, maybe small,
discrepancy due to both an approximation error and an error in the
implementation. When looking at Figure :ref:`decay:fig:v2`, it is
impossible to judge whether the program is correct or not.

The purpose of *verifying* a program is to bring evidence for the
property that there are no errors in the implementation. To avoid
mixing unavoidable approximation errors and undesired implementation
errors, we should try to make tests where we have some exact
computation of the discrete solution or at least parts of it.
Examples will show how this can be done.

Running a few algorithmic steps by hand
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The simplest approach to produce a correct reference for the discrete
solution :math:`u` of finite difference equations is to compute a few
steps of the algorithm by hand. Then we can compare the hand
calculations with numbers produced by the program.

A straightforward approach is to use a calculator and
compute :math:`u^1`, :math:`u^2`, and :math:`u^3`. With :math:`I=0.1`, :math:`\theta=0.8`,
and :math:`\Delta t =0.8` we get

.. math::
         A\equiv \frac{1 - (1-\theta) a\Delta t}{1 + \theta a \Delta t} = 0.298245614035

.. math::
        
        u^1 &= AI=0.0298245614035,\\ 
        u^2 &= Au^1= 0.00889504462912,\\ 
        u^3 &=Au^2= 0.00265290804728
        

Comparison of these manual calculations with the result of the
``solver`` function is carried out in the function

.. code-block:: python

        def test_solver_three_steps():
            """Compare three steps with known manual computations."""
            theta = 0.8; a = 2; I = 0.1; dt = 0.8
            u_by_hand = array([I,
                               0.0298245614035,
                               0.00889504462912,
                               0.00265290804728])
        
            Nt = 3  # number of time steps
            u, t = solver(I=I, a=a, T=Nt*dt, dt=dt, theta=theta)
        
            tol = 1E-15  # tolerance for comparing floats
            diff = abs(u - u_by_hand).max()
            success = diff <= tol
            assert success

The ``test_solver_three_steps`` function follows widely used conventions
for *unit testing*. By following such conventions we can at a later
stage easily execute a big test suite for our software. The
conventions are three-fold:

 * The test function starts with ``test_`` and takes no arguments.

 * The test ends up in a boolean expression that is ``True`` if
   the test passed and ``False`` if it failed.

 * The function runs ``assert`` on the boolean expression, resulting
   in program abortion (due to an ``AssertionError`` exception) if
   the test failed.

The main program can routinely run the verification test prior to
solving the real problem:

.. code-block:: python

        test_solver_three_steps()
        plot_numerical_and_exact(I=1, a=2, T=8, dt=0.8, theta=1)
        show()

(Rather than calling ``test_*()`` functions explicitly, one will
normally ask a testing framework like nose
or pytest to find and run such functions.)
The complete program including the verification above is
found in the file `decay_v3.py <http://tinyurl.com/jvzzcfn/decay/decay_v3.py>`__.

.. _decay:computing:error:

Computing the numerical error as a mesh function
------------------------------------------------

Now that we have some evidence for a correct implementation, we are in a
position to compare the computed :math:`u^n` values in the ``u`` array with
the exact :math:`u` values at the mesh points, in order to study the error
in the numerical solution.

.. index:: representative (mesh function)

A natural way to compare the exact and discrete solutions is to
calculate their difference as a mesh function:

.. math::
        
        e^n = {u_{\small\mbox{e}}}(t_n) - u^n,\quad n=0,1,\ldots,N_t {\thinspace .}
        

We may view :math:`{u_{\small\mbox{e}}}^n = {u_{\small\mbox{e}}}(t_n)` as the representation of :math:`{u_{\small\mbox{e}}}(t)`
as a mesh function rather than a continuous function defined for all
:math:`t\in [0,T]` (:math:`{u_{\small\mbox{e}}}^n` is often called the *representative* of
:math:`{u_{\small\mbox{e}}}` on the mesh). Then, :math:`e^n = {u_{\small\mbox{e}}}^n - u^n` is clearly
the difference of two mesh functions. This interpretation of :math:`e^n`
is natural when programming.

The error mesh function :math:`e^n` can be computed by

.. code-block:: python

        u, t = solver(I, a, T, dt, theta)  # Numerical sol.
        u_e = exact_solution(t, I, a)      # Representative of exact sol.
        e = u_e - u

Note that the mesh functions ``u`` and ``u_e`` are represented by arrays
and associated with the points in the array ``t``.

.. index:: array arithmetics

.. index:: array computing


.. admonition:: Array arithmetics

   The last statements
   
   .. code-block:: python
   
           u_e = exact_solution(t, I, a)
           e = u_e - u
   
   are primary examples of array arithmetics: ``t`` is an
   array of mesh points that we pass to ``exact_solution``. This function
   evaluates ``-a*t``, which is a scalar times an array, meaning that
   the scalar is multiplied with each array element.
   The result is an array, let us call it ``tmp1``. Then
   ``exp(tmp1)`` means applying the exponential function to each element in
   ``tmp``, resulting an array, say ``tmp2``. Finally, ``I*tmp2`` is computed
   (scalar times array) and ``u_e`` refers to this array returned from
   ``exact_solution``. The expression ``u_e - u`` is the difference between
   two arrays, resulting in a new array referred to by ``e``.




.. _decay:computing:error:norm:

Computing the norm of the numerical error
-----------------------------------------

.. index:: continuous function norms

.. index::
   single: norm; continuous

Instead of working with the error :math:`e^n` on the entire mesh, we
often want one number expressing the size of the error.
This is obtained by taking the norm of the error function.

Let us first define norms of a function :math:`f(t)`
defined for all :math:`t\in [0,T]`.
Three common norms are

.. math::
   :label: decay:norms:L2
        
        ||f||_{L^2} = \left( \int_0^T f(t)^2 dt\right)^{1/2},
        
        

.. math::
   :label: decay:norms:L1
          
        ||f||_{L^1} = \int_0^T |f(t)| dt,
        
        

.. math::
   :label: decay:norms:Linf
          
        ||f||_{L^\infty} = \max_{t\in [0,T]}|f(t)|{\thinspace .}
        
        

The :math:`L^2` norm :eq:`decay:norms:L2` ("L-two norm")
has nice mathematical properties and
is the most popular norm. It is a generalization
of the well-known Eucledian norm of vectors to functions.
The :math:`L^\infty` is also called the max norm or the supremum norm.
In fact, there is a whole family of norms,

.. math::
        
        ||f||_{L^p} = \left(\int_0^T f(t)^pdt\right)^{1/p},
        

with :math:`p` real. In particular,
:math:`p=1` corresponds to the :math:`L^1` norm above while :math:`p=\infty` is the
:math:`L^\infty` norm.

.. index:: discrete function norms

.. index:: mesh function norms

.. index::
   single: norm; discrete (mesh function)

Numerical computations involving mesh functions need corresponding norms.
Given a set of function values, :math:`f^n`, and some associated mesh points, :math:`t_n`,
a numerical integration rule can be used to calculate the :math:`L^2` and
:math:`L^1` norms defined above. Imagining that the mesh function is extended
to vary linearly between the mesh points, the Trapezoidal rule is
in fact an exact integration rule. A possible modification of the :math:`L^2`
norm for a mesh function :math:`f^n` on a uniform mesh with spacing :math:`\Delta t`
is therefore the well-known Trapezoidal integration formula

.. math::
         ||f^n|| = \left(\Delta t\left(\frac{1}{2}(f^0)^2 + \frac{1}{2}(f^{N_t})^2
        + \sum_{n=1}^{N_t-1} (f^n)^2\right)\right)^{1/2} 

A common approximation of this expression, motivated by the
convenience of having a simpler formula, is

.. math::
         ||f^n||_{\ell^2} = \left(\Delta t\sum_{n=0}^{N_t} (f^n)^2\right)^{1/2} {\thinspace .}

This is called the discrete :math:`L^2` norm and denoted by :math:`\ell^2`.
The error in :math:`||f||_{\ell^2}^2` compared with the Trapezoidal
integration formula
is :math:`\Delta t((f^0)^2 + (f^{N_t})^2)/2`, which means perturbed weights
at the end points of the mesh function, and the error goes to zero as
:math:`\Delta t\rightarrow 0`. As long as we are consistent and
stick to one kind of integration
rule for the norm of a mesh function, the details and accuracy of this
rule is not of concern.

The three discrete norms for a mesh function :math:`f^n`, corresponding to
the :math:`L^2`, :math:`L^1`, and :math:`L^\infty` norms of :math:`f(t)` defined above, are
defined by

.. math::
   :label: decay:norms:l2
        
        ||f^n||_{\ell^2}  \left( \Delta t\sum_{n=0}^{N_t} (f^n)^2\right)^{1/2},
        
        

.. math::
   :label: decay:norms:l1
          
        ||f^n||_{\ell^1}  \Delta t\sum_{n=0}^{N_t} |f^n|
        
        

.. math::
   :label: decay:norms:linf
          
        ||f^n||_{\ell^\infty}  \max_{0\leq n\leq N_t}|f^n|{\thinspace .}
        
        

Note that the :math:`L^2`, :math:`L^1`, :math:`\ell^2`, and :math:`\ell^1` norms depend on the
length of the interval of interest (think of :math:`f=1`, then the
norms are proportional to :math:`\sqrt{T}` or :math:`T`). In some applications it
is convenient to think of a mesh function as just a vector of function
values and neglect the information of the mesh points. Then we can
replace :math:`\Delta t` by :math:`T/N_t` and drop :math:`T`. Moreover, it is convenient
to divide by the total length of the vector, :math:`N_t+1`, instead of :math:`N_t`.
This reasoning gives rise to the *vector norms* for a vector
:math:`f=(f_0,\ldots,f_{N})`:

.. math::
   :label: decay:norms:vl2
        
        ||f||_2 = \left( \frac{1}{N+1}\sum_{n=0}^{N} (f_n)^2\right)^{1/2},
        
        

.. math::
   :label: decay:norms:vl1
          
        ||f||_1 = \frac{1}{N+1}\sum_{n=0}^{N} |f_n|
        
        

.. math::
   :label: decay:norms:vlinf
          
        ||f||_{\ell^\infty} = \max_{0\leq n\leq N}|f_n|{\thinspace .}
        
        

Here we have used the common vector component notation with subscripts
(:math:`f_n`) and :math:`N` as length. We will mostly work with mesh functions
and use the discrete :math:`\ell^2`
norm :eq:`decay:norms:l2` or the max norm :math:`\ell^\infty`
:eq:`decay:norms:linf`, but the corresponding vector norms
:eq:`decay:norms:vl2`-:eq:`decay:norms:vlinf` are also much used
in numerical computations, so it is important to know the different
norms and the relations between them.

.. index::
   single: error; norms

A single number that expresses the size of the numerical error
will be taken as :math:`||e^n||_{\ell^2}` and called :math:`E`:

.. math::
   :label: decay:E
        
        E = \sqrt{\Delta t\sum_{n=0}^{N_t} (e^n)^2}
        
        

The corresponding Python code, using array arithmetics, reads

.. code-block:: python

        E = sqrt(dt*sum(e**2))

The ``sum`` function comes from ``numpy`` and computes the sum of the elements
of an array. Also the ``sqrt`` function is from ``numpy`` and computes the
square root of each element in the array argument.

.. index:: scalar computing

Scalar computing
~~~~~~~~~~~~~~~~

Instead of doing array computing ``sqrt(dt*sum(e**2))`` we can compute with
one element at a time:

.. code-block:: python

        m = len(u)     # length of u array (alt: u.size)
        u_e = zeros(m)
        t = 0
        for i in range(m):
            u_e[i] = exact_solution(t, a, I)
            t = t + dt
        e = zeros(m)
        for i in range(m):
            e[i] = u_e[i] - u[i]
        s = 0  # summation variable
        for i in range(m):
            s = s + e[i]**2
        error = sqrt(dt*s)

Such element-wise computing, often called *scalar* computing, takes
more code, is less readable, and runs much slower than what we
can achieve with array computing.

.. _decay:plotting:

Plotting solutions
------------------

.. index:: plotting curves

.. index:: visualizing curves

Experiments with computing and plotting
---------------------------------------

Let us wrap up the computation of the error measure and all the
plotting statements for comparing the exact and numerical
solution in a new function ``explore``. This function
can be called for various :math:`\theta` and :math:`\Delta t` values
to see how the error varies with the method and the mesh resolution:

.. code-block:: python

        def explore(I, a, T, dt, theta=0.5, makeplot=True):
            """
            Run a case with the solver, compute error measure,
            and plot the numerical and exact solutions (if makeplot=True).
            """
            u, t = solver(I, a, T, dt, theta)    # Numerical solution
            u_e = exact_solution(t, I, a)
            e = u_e - u
            E = sqrt(dt*sum(e**2))
            if makeplot:
                figure()                         # create new plot
                t_e = linspace(0, T, 1001)       # fine mesh for u_e
                u_e = exact_solution(t_e, I, a)
                plot(t,   u,   'r--o')           # red dashes w/circles
                plot(t_e, u_e, 'b-')             # blue line for exact sol.
                legend(['numerical', 'exact'])
                xlabel('t')
                ylabel('u')
                title('theta=%g, dt=%g' % (theta, dt))
                theta2name = {0: 'FE', 1: 'BE', 0.5: 'CN'}
                savefig('%s_%g.png' % (theta2name[theta], dt))
                savefig('%s_%g.pdf' % (theta2name[theta], dt))
                show()
            return E

The ``figure()`` call is key: without it, a new ``plot`` command will
draw the new pair of curves in the same plot window, while we want
the different pairs to appear in separate windows and files.
Calling ``figure()`` ensures this.

Filenames with the method name (FE, BE, or CN) rather than the
:math:`\theta` value embedded in the name, can easily be created with
the aid of a little Python dictionary for mapping :math:`\theta` to
method acronyms:

.. code-block:: python

        theta2name = {0: 'FE', 1: 'BE', 0.5: 'CN'}
        savefig('%s_%g.png' % (theta2name[theta], dt))

.. index:: PNG plot

.. index:: PDF plot

.. index:: EPS plot

.. index:: viewing graphics files

The ``explore`` function stores the plot in two different image file formats:
PNG and PDF. The PNG format is aimed
at being included in HTML files and the PDF format in LaTeX documents
(more precisely, in pdfLaTeX documents).
Frequently used viewers for these
image files on Unix systems are ``gv`` (comes with Ghostscript)
for the PDF format and
``display`` (from the ImageMagick) suite for PNG files:

.. code-block:: text

        Terminal> gv BE_0.5.pdf
        Terminal> display BE_0.5.png

A main program may run a loop over the three methods (:math:`\theta` values)
and call ``explore`` to compute errors and make plots:

.. code-block:: python

        def main(I, a, T, dt_values, theta_values=(0, 0.5, 1)):
            for theta in theta_values:
                for dt in dt_values:
                    E = explore(I, a, T, dt, theta, makeplot=True)
                    print '%3.1f %6.2f: %12.3E' % (theta, dt, E)

The complete code containing the functions above
resides in the file `decay_plot_mpl.py <http://tinyurl.com/jvzzcfn/decay/decay_plot_mpl.py>`__.
Running this program results in

.. code-block:: text

        Terminal> python decay_plot_mpl.py
        0.0   0.40:    2.105E-01
        0.0   0.04:    1.449E-02
        0.5   0.40:    3.362E-02
        0.5   0.04:    1.887E-04
        1.0   0.40:    1.030E-01
        1.0   0.04:    1.382E-02

We observe that reducing :math:`\Delta t` by a factor of 10 increases the
accuracy for all three methods (:math:`\theta` values). We also see that
the combination of :math:`\theta=0.5` and a small time step :math:`\Delta t =0.04`
gives a much more accurate solution, and that :math:`\theta=0` and :math:`\theta=1`
with :math:`\Delta t = 0.4` result in the least accurate solutions.

Figure :ref:`decay:fig:FE1` demonstrates that the numerical solution for
:math:`\Delta t=0.4` clearly lies below the exact curve, but that the
accuracy improves considerably by reducing the time step by a factor
of 10.

.. _decay:fig:FE1:

.. figure:: FE1.png
   :width: 600

   *The Forward Euler scheme for two values of the time step*

.. index:: cropping images

.. index:: montage program

Combining plot files
~~~~~~~~~~~~~~~~~~~~

Mounting two PNG files, as done in the figure, is easily done by the
`montage <http://www.imagemagick.org/script/montage.php>`__ program
from the ImageMagick suite:

.. code-block:: text

        Terminal> montage -background white -geometry 100% -tile 2x1 \ 
                  FE_0.4.png FE_0.04.png FE1.png
        Terminal> convert -trim FE1.png FE1.png

The ``-geometry`` argument is used to specify the size of the image, and here
we preserve the individual sizes of the images. The ``-tile HxV`` option
specifies ``H`` images in the horizontal direction and ``V`` images in
the vertical direction. A series of image files to be combined are then listed,
with the name of the resulting combined image, here ``FE1.png`` at the end.
The ``convert -trim`` command removes surrounding white areas in the figure
(an operation usually known as *cropping* in image manipulation programs).

.. index:: pdftk program

.. index:: pdfnup program

.. index:: pdfcrop program

For LaTeX reports it is not recommended to use ``montage`` and PNG files
as the result has too low resolution. Instead, plots should be made
in the PDF format and combined using the ``pdftk``, ``pdfnup``, and ``pdfcrop`` tools
(on Linux/Unix):

.. code-block:: text

        Terminal> pdftk FE_0.4.png FE_0.04.png output tmp.pdf
        Terminal> pdfnup --nup 2x1 --outfile tmp.pdf tmp.pdf
        Terminal> pdfcrop tmp.pdf FE1.png  # output in FE1.png

Here, ``pdftk`` combines images into a multi-page PDF file, ``pdfnup``
combines the images in individual pages to a table of images (pages),
and ``pdfcrop`` removes white margins in the resulting combined image file.

The behavior of the two other schemes is shown in Figures :ref:`decay:fig:BE1`
and :ref:`decay:fig:CN1`. Crank-Nicolson is obviously the most accurate
scheme from this visual point of view.

.. _decay:fig:BE1:

.. figure:: BE1.png
   :width: 600

   *The Backward Euler scheme for two values of the time step*

.. _decay:fig:CN1:

.. figure:: CN1.png
   :width: 600

   *The Crank-Nicolson scheme for two values of the time step*

Plotting with SciTools
~~~~~~~~~~~~~~~~~~~~~~

The `SciTools package <http://code.google.com/p/scitools>`__ provides a
unified plotting interface, called Easyviz, to many different plotting
packages, including Matplotlib, Gnuplot, Grace, MATLAB,
VTK, OpenDX, and VisIt. The syntax is very similar to that of
Matplotlib and MATLAB. In fact, the plotting commands shown above look
the same in SciTool's Easyviz interface, apart from the import
statement, which reads

.. code-block:: python

        from scitools.std import *

This statement performs a ``from numpy import *`` as well as an import
of the most common pieces of the Easyviz (``scitools.easyviz``) package,
along with some additional numerical functionality.

With Easyviz one can
merge several plotting commands into a single one
using keyword arguments:

.. code-block:: python

        plot(t,   u,   'r--o',           # red dashes w/circles
             t_e, u_e, 'b-',             # blue line for exact sol.
             legend=['numerical', 'exact'],
             xlabel='t',
             ylabel='u',
             title='theta=%g, dt=%g' % (theta, dt),
             savefig='%s_%g.png' % (theta2name[theta], dt),
             show=True)

The `decay_plot_st.py <http://tinyurl.com/jvzzcfn/decay/decay_plot_st.py>`__ file
contains such a demo.

By default, Easyviz employs Matplotlib for plotting, but `Gnuplot <http://www.gnuplot.info/>`__ and `Grace <http://plasma-gate.weizmann.ac.il/Grace/>`__ are viable alternatives:

.. code-block:: text

        Terminal> python decay_plot_st.py --SCITOOLS_easyviz_backend gnuplot
        Terminal> python decay_plot_st.py --SCITOOLS_easyviz_backend grace

The backend used for creating plots (and numerous other options)
can be permanently set in SciTool's configuration file.

All the Gnuplot windows are launched without any need to kill one before
the next one pops up (as is the case with Matplotlib) and one can
press the key 'q' anywhere in a plot window to kill it.
Another advantage of Gnuplot is the automatic choice of sensible
and distinguishable line types in black-and-white PDF and PostScript
files.

Regarding functionality for annotating plots with title, labels on the
axis, legends, etc., we refer to the documentation of Matplotlib and
SciTools for more detailed information on the syntax. The hope is that
the programming syntax explained so far suffices for understanding the
code and learning more from a combination of the forthcoming examples
and other resources such as books and web pages.


.. admonition:: Test the understanding

   :ref:`decay:app:exer:cooling:py` asks you to implement
   a solver for a problem that is slightly different from the
   one above. You may use the ``solver`` and ``explore`` functions
   explained above as a starting point. Apply the new solver
   to :ref:`decay:app:exer:cooling:murder`.




Memory-saving implementation
----------------------------

The computer memory requirements of our implementations so far
consists mainly of the ``u`` and ``t`` arrays, both of length :math:`N_t+1`,
plus some other temporary arrays that Python needs for intermediate
results if we do array arithmetics in our program (e.g., ``I*exp(-a*t)``
needs to store ``a*t`` before ``-`` can be applied to it and then ``exp``).
Regardless of how we implement simple ODE problems, storage
requirements are very modest and put not restriction on how we choose
our data structures and algorithms.  Nevertheless, when the methods
for ODEs used here are applied to three-dimensional partial
differential equation (PDE) problems, memory storage requirements
suddenly become a challenging issue.

The PDE counterpart to our model problem :math:`u'=-a` is a diffusion
equation :math:`u_t = a\nabla^2 u` posed on a space-time domain. The
discrete representation of this domain may in 3D be a spatial mesh of
:math:`M^3` points and a time mesh of :math:`N_t` points. A typical desired value
for :math:`M` is 100 in many applications, or even :math:`1000`.  Storing all the
computed :math:`u` values, like we have done in the programs so far, demands
storage of some arrays of size :math:`M^3N_t`, giving a factor of :math:`M^3`
larger storage demands compared to our ODE programs. Each real number
in the array for :math:`u` requires 8 bytes (b) of storage. With :math:`M=100` and
:math:`N_t=1000`, there is a storage demand of :math:`(10^3)^3\cdot 1000\cdot 8 =
8` Gb for the solution array.  Fortunately, we can usually get rid of
the :math:`N_t` factor, resulting in 8 Mb of storage.  Below we explain how
this is done, and the technique is almost always applied in
implementations of PDE problems.

.. Fortunately, the methods we use to solve ODEs

.. and PDEs were mostly developed in a time where the size of a computer's

.. memory was very small compared to today's standards, and researchers

.. were therefore forced to always minimize the memory usage. As a result of

.. these circumstances, there is still a very strong focus on reducing

.. memory requirements in scientific computing algorithms.

Let us critically evaluate how much we really need to store in the
computer's memory in our implementation of the :math:`\theta` method. To
compute a new :math:`u^{n+1}`, all we need is :math:`u^n`. This implies that the
previous :math:`u^{n-1},u^{n-2},\dots,u^0` values do not need to be stored
in an array, although this is convenient for plotting and data
analysis in the program.  Instead of the ``u`` array we can work with
two variables for real numbers, ``u`` and ``u_1``, representing :math:`u^{n+1}`
and :math:`u^n` in the algorithm, respectively.  At each time level, we
update ``u`` from ``u_1`` and then set ``u_1 = u`` so that the computed
:math:`u^{n+1}` value becomes the "previous" value :math:`u^n` at the next time
level. The downside is that we cannot plot the solution after the
simulation is done since only the last two numbers are available.  The
remedy is to store computed values in a file and use the file for
visualizing the solution later.

We have implemented this memory saving idea in the file
`decay_memsave.py <http://tinyurl.com/jvzzcfn/decay/decay_memsave.py>`__, which is a
slight modification of `decay_plot_mpl.py <http://tinyurl.com/jvzzcfn/decay/decay_plot_mpl.py>`__
program.

The following function demonstrates how we work with the two most
recent values of the unknown:

.. code-block:: python

        def solver_memsave(I, a, T, dt, theta, filename='sol.dat'):
            """
            Solve u'=-a*u, u(0)=I, for t in (0,T] with steps of dt.
            Minimum use of memory. The solution is stored in a file
            (with name filename) for later plotting.
            """
            dt = float(dt)         # avoid integer division
            Nt = int(round(T/dt))  # no of intervals
        
            outfile = open(filename, 'w')
            # u: time level n+1, u_1: time level n
            t = 0
            u_1 = I
            outfile.write('%.16E  %.16E\n' % (t, u_1))
            for n in range(1, Nt+1):
                u = (1 - (1-theta)*a*dt)/(1 + theta*dt*a)*u_1
                u_1 = u
                t += dt
                outfile.write('%.16E  %.16E\n' % (t, u))
            outfile.close()
            return u, t

This code snippet serves as a quick introduction to file writing in Python.
Reading the data in the file into arrays ``t`` and ``u`` are done by the
function

.. code-block:: python

        def read_file(filename='sol.dat'):
            infile = open(filename, 'r')
            u = [];  t = []
            for line in infile:
                words = line.split()
                if len(words) != 2:
                    print 'Found more than two numbers on a line!', words
                    sys.exit(1)  # abort
                t.append(float(words[0]))
                u.append(float(words[1]))
            return np.array(t), np.array(u)

This type of file with numbers in rows and columns is very common, and
``numpy`` has a function ``loadtxt`` which loads such tabular data into a
two-dimensional array, say with name ``data``. The number in row ``i`` and
column ``j`` is then ``data[i,j]``.  The whole column number ``j`` can be
extracted by ``data[:,j]``.  A version of ``read_file`` using ``np.loadtxt``
reads

.. code-block:: python

        def read_file_numpy(filename='sol.dat'):
            data = np.loadtxt(filename)
            t = data[:,0]
            u = data[:,1]
            return t, u

The present counterpart to the ``explore`` function from
`decay_plot_mpl.py <http://tinyurl.com/jvzzcfn/decay/decay_plot_mpl.py>`__ must run
``solver_memsave`` and then load data from file before we can compute
the error measure and make the plot:

.. code-block:: python

        def explore(I, a, T, dt, theta=0.5, makeplot=True):
            filename = 'u.dat'
            u, t = solver_memsave(I, a, T, dt, theta, filename)
        
            t, u = read_file(filename)
            u_e = exact_solution(t, I, a)
            e = u_e - u
            E = sqrt(dt*np.sum(e**2))
            if makeplot:
                figure()
                ...

Apart from the internal implementation, where :math:`u^n` values are
stored in a file rather than in an array, ``decay_memsave.py``
file works exactly as the ``decay_plot_mpl.py`` file.

.. !split

.. _decay:analysis:

Analysis of finite difference equations
=======================================

We address the ODE for exponential decay,

.. math::
        
        u'(t) = -au(t),\quad u(0)=I,
        

where :math:`a` and :math:`I` are given constants. This problem is solved
by the :math:`\theta`-rule finite difference scheme, resulting in
the recursive equations

.. math::
   :label: decay:analysis:scheme
        
        u^{n+1} = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}u^n
        
        

for the numerical solution :math:`u^{n+1}`, which approximates the exact
solution :math:`{u_{\small\mbox{e}}}` at time point :math:`t_{n+1}`. For constant mesh spacing,
which we assume here, :math:`t_{n+1}=(n+1)\Delta t`.

.. Cannot use === because it fools sphinx

**Discouraging numerical solutions.**
Choosing :math:`I=1`, :math:`a=2`, and running experiments with :math:`\theta =1,0.5, 0`
for :math:`\Delta t=1.25, 0.75, 0.5, 0.1`, gives the results in
Figures :ref:`decay:analysis:BE4c`, :ref:`decay:analysis:CN4c`, and
:ref:`decay:analysis:FE4c`.

.. _decay:analysis:BE4c:

.. figure:: BE4c.png
   :width: 600

   *Backward Euler*

.. _decay:analysis:CN4c:

.. figure:: CN4c.png
   :width: 600

   *Crank-Nicolson*

.. _decay:analysis:FE4c:

.. figure:: FE4c.png
   :width: 600

   *Forward Euler*

The characteristics of the displayed curves can be summarized as follows:

  * The Backward Euler scheme always gives a monotone solution, lying above
    the exact curve.

  * The Crank-Nicolson scheme gives the most accurate results, but for
    :math:`\Delta t=1.25` the solution oscillates.

  * The Forward Euler scheme gives a growing, oscillating solution for
    :math:`\Delta t=1.25`; a decaying, oscillating solution for :math:`\Delta t=0.75`;
    a strange solution :math:`u^n=0` for :math:`n\geq 1` when :math:`\Delta t=0.5`; and
    a solution seemingly as accurate as the one by the Backward Euler
    scheme for :math:`\Delta t = 0.1`, but the curve lies below the exact
    solution.

Since the exact solution of our model problem is a monotone function,
:math:`u(t)=Ie^{-at}`, some of these qualitatively wrong results are indeed alarming!


.. admonition:: Goal

   We ask the question
   
     * Under what circumstances, i.e., values of
       the input data :math:`I`, :math:`a`, and :math:`\Delta t` will the Forward Euler and
       Crank-Nicolson schemes result in undesired oscillatory solutions?
   
   The question will be investigated both by numerical experiments and
   by precise mathematical theory. The latter will help establish
   general criteria on :math:`\Delta t` for avoiding non-physical oscillatory
   or growing solutions.
   
   Another question to be raised is
   
    * How does :math:`\Delta t` impact the error in the numerical solution?
   
   For our simple model problem we can answer this question very precisely, but
   we will also look at simplified formulas for small :math:`\Delta t`
   and touch upon important concepts such as *convergence rate* and
   *the order of a scheme*. Other fundamental concepts mentioned are
   stability, consistency, and convergence.




Experimental investigation of oscillatory solutions
---------------------------------------------------

To address the first question above,
we may set up an experiment where we loop over values of :math:`I`, :math:`a`,
and :math:`\Delta t`. For each experiment, we flag the solution as
oscillatory if

.. math::
         u^{n} > u^{n-1},

for some value of :math:`n`,
since we expect :math:`u^n` to decay with :math:`n`, but oscillations make
:math:`u` increase over a time step. We will quickly see that
oscillations are independent of :math:`I`, but do depend on :math:`a` and
:math:`\Delta t`. Therefore, we introduce a two-dimensional
function :math:`B(a,\Delta t)` which is 1 if oscillations occur
and 0 otherwise. We can visualize :math:`B` as a contour plot
(lines for which :math:`B=\hbox{const}`). The contour :math:`B=0.5`
corresponds to the borderline between oscillatory regions with :math:`B=1`
and monotone regions with :math:`B=0` in the :math:`a,\Delta t` plane.

The :math:`B` function is defined at discrete :math:`a` and :math:`\Delta t` values.
Say we have given :math:`P` $a$ values, :math:`a_0,\ldots,a_{P-1}`, and
:math:`Q` $\Delta t$ values, :math:`\Delta t_0,\ldots,\Delta t_{Q-1}`.
These :math:`a_i` and :math:`\Delta t_j` values, :math:`i=0,\ldots,P-1`,
:math:`j=0,\ldots,Q-1`, form a rectangular mesh of :math:`P\times Q` points
in the plane. At each point :math:`(a_i, \Delta t_j)`, we associate
the corresponding value of :math:`B(a_i,\Delta t_j)`, denoted :math:`B_{ij}`.
The :math:`B_{ij}` values are naturally stored in a two-dimensional
array. We can thereafter create a plot of the
contour line :math:`B_{ij}=0.5` dividing the oscillatory and monotone
regions. The file `decay_osc_regions.py <http://tinyurl.com/jvzzcfn/decay/decay_osc_regions.py>`__  ``osc_regions`` stands for "oscillatory regions") contains all nuts and
bolts to produce the :math:`B=0.5` line in Figures :ref:`decay:analysis:B:FE`
and :ref:`decay:analysis:B:CN`. The oscillatory region is above this line.

.. code-block:: python

        from decay_mod import solver
        import numpy as np
        import scitools.std as st
        
        def non_physical_behavior(I, a, T, dt, theta):
            """
            Given lists/arrays a and dt, and numbers I, dt, and theta,
            make a two-dimensional contour line B=0.5, where B=1>0.5
            means oscillatory (unstable) solution, and B=0<0.5 means
            monotone solution of u'=-au.
            """
            a = np.asarray(a); dt = np.asarray(dt)  # must be arrays
            B = np.zeros((len(a), len(dt)))         # results
            for i in range(len(a)):
                for j in range(len(dt)):
                    u, t = solver(I, a[i], T, dt[j], theta)
                    # Does u have the right monotone decay properties?
                    correct_qualitative_behavior = True
                    for n in range(1, len(u)):
                        if u[n] > u[n-1]:  # Not decaying?
                            correct_qualitative_behavior = False
                            break  # Jump out of loop
                    B[i,j] = float(correct_qualitative_behavior)
            a_, dt_ = st.ndgrid(a, dt)  # make mesh of a and dt values
            st.contour(a_, dt_, B, 1)
            st.grid('on')
            st.title('theta=%g' % theta)
            st.xlabel('a'); st.ylabel('dt')
            st.savefig('osc_region_theta_%s.png' % theta)
            st.savefig('osc_region_theta_%s.pdf' % theta)
        
        non_physical_behavior(
            I=1,
            a=np.linspace(0.01, 4, 22),
            dt=np.linspace(0.01, 4, 22),
            T=6,
            theta=0.5)

.. _decay:analysis:B:FE:

.. figure:: osc_region_FE.png
   :width: 500

   *Forward Euler scheme: oscillatory solutions occur for points above the curve*

.. _decay:analysis:B:CN:

.. figure:: osc_region_CN.png
   :width: 500

   *Crank-Nicolson scheme: oscillatory solutions occur for points above the curve*

By looking at the curves in the figures one may guess that :math:`a\Delta t`
must be less than a critical limit to avoid the undesired
oscillations.  This limit seems to be about 2 for Crank-Nicolson and 1
for Forward Euler.  We shall now establish a precise mathematical
analysis of the discrete model that can explain the observations in
our numerical experiments.

Exact numerical solution
------------------------

Starting with :math:`u^0=I`, the simple recursion :eq:`decay:analysis:scheme`
can be applied repeatedly :math:`n` times, with the result that

.. math::
   :label: decay:analysis:unex
        
        u^{n} = IA^n,\quad A = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}{\thinspace .}
        
        


.. admonition:: Solving difference equations

   Difference equations where all terms are linear in
   :math:`u^{n+1}`, :math:`u^n`, and maybe :math:`u^{n-1}`, :math:`u^{n-2}`, etc., are
   called *homogeneous, linear* difference equations, and their solutions
   are generally of the form :math:`u^n=A^n`. Inserting this expression
   and dividing by :math:`A^{n+1}` gives
   a polynomial equation in :math:`A`. In the present case we get
   
   .. math::
            A = \frac{1 - (1-\theta) a\Delta t}{1 + \theta a\Delta t}{\thinspace .} 
   
   This is a solution technique of wider applicability than repeated use of
   the recursion :eq:`decay:analysis:scheme`.




Regardless of the solution approach, we have obtained a formula for
:math:`u^n`.  This formula can explain everything what we see in the figures
above, but it also gives us a more general insight into accuracy and
stability properties of the three schemes.

Stability
---------

.. index:: stability

Since :math:`u^n` is a factor :math:`A`
raised to an integer power :math:`n`, we realize that :math:`A<0`
will for odd powers imply :math:`u^n<0` and for even power result in :math:`u^n>0`.
That is, the solution oscillates between the mesh points.
We have oscillations due to :math:`A<0` when

.. math::
   :label: decay:th:stability
        
        (1-\theta)a\Delta t > 1 {\thinspace .}
        
        

Since :math:`A>0` is a requirement for having a numerical solution with the
same basic property (monotonicity) as the exact solution, we may say
that :math:`A>0` is a *stability criterion*. Expressed in terms of :math:`\Delta t`
the stability criterion reads

.. math::
        
        \Delta t < \frac{1}{(1-\theta)a}{\thinspace .}
        

The Backward
Euler scheme is always stable since :math:`A<0` is impossible for :math:`\theta=1`, while
non-oscillating solutions for Forward Euler and Crank-Nicolson
demand :math:`\Delta t\leq 1/a` and :math:`\Delta t\leq 2/a`, respectively.
The relation between :math:`\Delta t` and :math:`a` look reasonable: a larger
:math:`a` means faster decay and hence a need for smaller time steps.

Looking at Figure :ref:`decay:analysis:FE4c`, we see that with :math:`a\Delta
t= 2\cdot 1.25=2.5`, :math:`A=-1.5`, and the solution :math:`u^n=(-1.5)^n`
oscillates *and* grows. With :math:`a\Delta t = 2\cdot 0.75=1.5`, :math:`A=-0.5`,
:math:`u^n=(-0.5)^n` decays but oscillates. The peculiar case :math:`\Delta t =
0.5`, where the Forward Euler scheme produces a solution that is stuck
on the :math:`t` axis, corresponds to :math:`A=0` and therefore :math:`u^0=I=1` and
:math:`u^n=0` for :math:`n\geq 1`.  The decaying oscillations in the Crank-Nicolson scheme
for :math:`\Delta t=1.25` are easily explained by the fact that :math:`A\approx -0.11<0`.

.. index:: amplification factor

The factor :math:`A` is called the *amplification factor* since the solution
at a new time level is :math:`A` times the solution at the previous time
level. For a decay process, we must obviously have :math:`|A|\leq 1`, which
is fulfilled for all :math:`\Delta t` if :math:`\theta \geq 1/2`. Arbitrarily
large values of :math:`u` can be generated when :math:`|A|>1` and :math:`n` is large
enough. The numerical solution is in such cases totally irrelevant to
an ODE modeling decay processes! To avoid this situation, we must
for :math:`\theta < 1/2` have

.. math::
        
        \Delta t \leq \frac{2}{(1-2\theta)a},
        

which means :math:`\Delta t < 2/a` for the Forward Euler scheme.

.. index:: A-stable methods

.. index:: L-stable methods


.. admonition:: Stability properties

   We may summarize the stability investigations as follows:
   
   1. The Forward Euler method is a *conditionally stable* scheme because
      it requires :math:`\Delta t < 2/a` for avoiding growing solutions
      and :math:`\Delta t < 1/a` for avoiding oscillatory solutions.
   
   2. The Crank-Nicolson is *unconditionally stable* with respect to
      growing solutions, while it is conditionally stable with
      the criterion :math:`\Delta t < 2/a` for avoiding oscillatory solutions.
   
   3. The Backward Euler method is unconditionally stable with respect
      to growing and oscillatory solutions - any :math:`\Delta t` will work.
   
   Much literature on ODEs speaks about L-stable and A-stable methods.
   In our case A-stable methods ensures non-growing solutions, while
   L-stable methods also avoids oscillatory solutions.




Comparing amplification factors
-------------------------------

After establishing how :math:`A` impacts the qualitative features of the
solution, we shall now look more into how well the numerical amplification
factor approximates the exact one. The exact solution reads
:math:`u(t)=Ie^{-at}`, which can be rewritten as

.. math::
        
        {{u_{\small\mbox{e}}}}(t_n) = Ie^{-a n\Delta t} = I(e^{-a\Delta t})^n {\thinspace .}
        

From this formula we see that the exact amplification factor is

.. math::
        
        {A_{\small\mbox{e}}} = e^{-a\Delta t} {\thinspace .}
        

We realize that the exact and numerical amplification factors depend
on :math:`a` and :math:`\Delta t` through the product :math:`a\Delta t`. Therefore, it
is convenient to introduce a symbol for this product, :math:`p=a\Delta t`,
and view :math:`A` and :math:`{A_{\small\mbox{e}}}` as functions of :math:`p`. Figure
:ref:`decay:analysis:fig:A` shows these functions. Crank-Nicolson is
clearly closest to the exact amplification factor, but that method has
the unfortunate oscillatory behavior when :math:`p>2`.

.. _decay:analysis:fig:A:

.. figure:: A_factors.png
   :width: 500

   *Comparison of amplification factors*

Series expansion of amplification factors
-----------------------------------------

As an alternative to the visual understanding inherent in Figure
:ref:`decay:analysis:fig:A`, there is a strong tradition in numerical
analysis to establish formulas for the approximation errors when the
discretization parameter, here :math:`\Delta t`, becomes small. In the
present case we let :math:`p` be our small discretization parameter, and it
makes sense to simplify the expressions for :math:`A` and :math:`{A_{\small\mbox{e}}}` by using
Taylor polynomials around :math:`p=0`.  The Taylor polynomials are accurate
for small :math:`p` and greatly simplifies the comparison of the analytical
expressions since we then can compare polynomials, term by term.

Calculating the Taylor series for :math:`{A_{\small\mbox{e}}}` is easily done by hand, but
the three versions of :math:`A` for :math:`\theta=0,1,{\frac{1}{2}}` lead to more
cumbersome calculations.
Nowadays, analytical computations can benefit greatly by
symbolic computer algebra software. The Python package ``sympy``
represents a powerful computer algebra system, not yet as sophisticated as
the famous Maple and Mathematica systems, but free and
very easy to integrate with our numerical computations in Python.

When using ``sympy``, it is convenient to enter the interactive Python
mode where we can write expressions and statements and immediately see
the results.  Here is a simple example. We strongly recommend to use
``isympy`` (or ``ipython``) for such interactive sessions.

Let us illustrate ``sympy`` with a standard Python shell syntax
(``>>>`` prompt) to compute a Taylor polynomial approximation to :math:`e^{-p}`:

.. code-block:: ipy

        >>> from sympy import *
        >>> # Create p as a mathematical symbol with name 'p'
        >>> p = Symbol('p')
        >>> # Create a mathematical expression with p
        >>> A_e = exp(-p)
        >>>
        >>> # Find the first 6 terms of the Taylor series of A_e
        >>> A_e.series(p, 0, 6)
        1 + (1/2)*p**2 - p - 1/6*p**3 - 1/120*p**5 + (1/24)*p**4 + O(p**6)

Lines with ``>>>`` represent input lines and lines without
this prompt represents the result of computations (note that
``isympy`` and ``ipython`` apply other prompts, but in this text
we always apply ``>>>`` for interactive Python computing).
Apart from the order of the powers, the computed formula is easily
recognized as the beginning of the Taylor series for :math:`e^{-p}`.

Let us define the numerical amplification factor where :math:`p` and :math:`\theta`
enter the formula as symbols:

.. code-block:: ipy

        >>> theta = Symbol('theta')
        >>> A = (1-(1-theta)*p)/(1+theta*p)

To work with the factor for the Backward Euler scheme we
can substitute the value 1 for ``theta``:

.. code-block:: ipy

        >>> A.subs(theta, 1)
        1/(1 + p)

Similarly, we can replace ``theta`` by 1/2 for Crank-Nicolson,
preferably using an exact rational representation of 1/2 in ``sympy``:

.. code-block:: ipy

        >>> half = Rational(1,2)
        >>> A.subs(theta, half)
        1/(1 + (1/2)*p)*(1 - 1/2*p)

The Taylor series of the amplification factor for the Crank-Nicolson
scheme can be computed as

.. code-block:: ipy

        >>> A.subs(theta, half).series(p, 0, 4)
        1 + (1/2)*p**2 - p - 1/4*p**3 + O(p**4)

We are now in a position to compare Taylor series:

.. code-block:: ipy

        >>> FE = A_e.series(p, 0, 4) - A.subs(theta, 0).series(p, 0, 4)
        >>> BE = A_e.series(p, 0, 4) - A.subs(theta, 1).series(p, 0, 4)
        >>> CN = A_e.series(p, 0, 4) - A.subs(theta, half).series(p, 0, 4 )
        >>> FE
        (1/2)*p**2 - 1/6*p**3 + O(p**4)
        >>> BE
        -1/2*p**2 + (5/6)*p**3 + O(p**4)
        >>> CN
        (1/12)*p**3 + O(p**4)

From these expressions we see that the error :math:`A-{A_{\small\mbox{e}}}\sim {\mathcal{O}(p^2)}`
for the Forward and Backward Euler schemes, while
:math:`A-{A_{\small\mbox{e}}}\sim {\mathcal{O}(p^3)}` for the Crank-Nicolson scheme.
It is the *leading order term*,
i.e., the term of the lowest order (polynomial degree),
that is of interest, because as :math:`p\rightarrow 0`, this term is
(much) bigger than the higher-order terms (think of :math:`p=0.01`:
:math:`p` is a hundred times larger than :math:`p^2`).

Now, :math:`a` is a given parameter in the problem, while :math:`\Delta t` is
what we can vary. One therefore usually writes the error expressions in
terms :math:`\Delta t`. When then have

.. math::
        
        A-{A_{\small\mbox{e}}} = \left\lbrace\begin{array}{ll}
        {\mathcal{O}(\Delta t^2)}, & \hbox{Forward and Backward Euler},\\ 
        {\mathcal{O}(\Delta t^3)}, & \hbox{Crank-Nicolson}
        \end{array}\right.
        

We say that the Crank-Nicolson scheme has an error in the amplification
factor of order :math:`\Delta t^3`, while the two other schemes are
of order :math:`\Delta t^2` in the same quantity.
What is the significance of the order expression? If we halve :math:`\Delta t`,
the error in amplification factor at a time level will be reduced
by a factor of 4 in the Forward and Backward Euler schemes, and by
a factor of 8 in the Crank-Nicolson scheme. That is, as we
reduce :math:`\Delta t` to obtain more accurate results, the Crank-Nicolson
scheme reduces the error more efficiently than the other schemes.

The fraction of numerical and exact amplification factors
---------------------------------------------------------

.. index::
   single: error; amplification factor

An alternative comparison of the schemes is to look at the
ratio :math:`A/{A_{\small\mbox{e}}}`, or the error :math:`1-A/{A_{\small\mbox{e}}}` in this ratio:

.. code-block:: ipy

        >>> FE = 1 - (A.subs(theta, 0)/A_e).series(p, 0, 4)
        >>> BE = 1 - (A.subs(theta, 1)/A_e).series(p, 0, 4)
        >>> CN = 1 - (A.subs(theta, half)/A_e).series(p, 0, 4)
        >>> FE
        (1/2)*p**2 + (1/3)*p**3 + O(p**4)
        >>> BE
        -1/2*p**2 + (1/3)*p**3 + O(p**4)
        >>> CN
        (1/12)*p**3 + O(p**4)

The leading-order terms have the same powers as
in the analysis of :math:`A-{A_{\small\mbox{e}}}`.

.. _decay:analysis:gobal:error:

The global error at a point
---------------------------

.. index::
   single: error; global

The error in the amplification factor reflects the error when
progressing from time level :math:`t_n` to :math:`t_{n-1}`.
To investigate the real error at a point, known as the *global error*,
we look at :math:`e^n = u^n-{u_{\small\mbox{e}}}(t_n)` for some :math:`n` and Taylor expand the
mathematical expressions as functions of :math:`p=a\Delta t`:

.. code-block:: ipy

        >>> n = Symbol('n')
        >>> u_e = exp(-p*n)
        >>> u_n = A**n
        >>> FE = u_e.series(p, 0, 4) - u_n.subs(theta, 0).series(p, 0, 4)
        >>> BE = u_e.series(p, 0, 4) - u_n.subs(theta, 1).series(p, 0, 4)
        >>> CN = u_e.series(p, 0, 4) - u_n.subs(theta, half).series(p, 0, 4)
        >>> FE
        (1/2)*n*p**2 - 1/2*n**2*p**3 + (1/3)*n*p**3 + O(p**4)
        >>> BE
        (1/2)*n**2*p**3 - 1/2*n*p**2 + (1/3)*n*p**3 + O(p**4)
        >>> CN
        (1/12)*n*p**3 + O(p**4)

For a fixed time :math:`t`, the parameter :math:`n` in these expressions increases
as :math:`p\rightarrow 0` since :math:`t=n\Delta t =\mbox{const}` and hence
:math:`n` must increase like :math:`\Delta t^{-1}`. With :math:`n` substituted by
:math:`t/\Delta t` in
the leading-order error terms, these become :math:`\frac{1}{2} na^2\Delta
t^2 = {\frac{1}{2}}ta^2\Delta t` for the Forward and Backward Euler
scheme, and :math:`\frac{1}{12}na^3\Delta t^3 = \frac{1}{12}ta^3\Delta t^2`
for the Crank-Nicolson scheme.  The global error is therefore of
second order (in :math:`\Delta t`) for the latter scheme and of first order for
the former schemes.

When the global error :math:`e^n\rightarrow 0` as :math:`\Delta t\rightarrow 0`,
we say that the scheme is *convergent*. It means that the numerical
solution approaches the exact solution as the mesh is refined, and
this is a much desired property of a numerical method.

Integrated errors
-----------------

It is common to study the norm of the numerical error, as
explained in detail in the section :ref:`decay:computing:error:norm`.
The :math:`L^2` norm can be computed by treating :math:`e^n` as a function
of :math:`t` in ``sympy`` and performing symbolic integration. For
the Forward Euler scheme we have

.. code-block:: python

        p, n, a, dt, t, T, theta = symbols('p n a dt t T 'theta')
        A = (1-(1-theta)*p)/(1+theta*p)
        u_e = exp(-p*n)
        u_n = A**n
        error = u_e.series(p, 0, 4) - u_n.subs(theta, 0).series(p, 0, 4)
        # Introduce t and dt instead of n and p
        error = error.subs('n', 't/dt').subs(p, 'a*dt')
        error = error.as_leading_term(dt) # study only the first term
        print error
        error_L2 = sqrt(integrate(error**2, (t, 0, T)))
        print error_L2

The output reads

.. code-block:: text

        sqrt(30)*sqrt(T**3*a**4*dt**2*(6*T**2*a**2 - 15*T*a + 10))/60

which means that the :math:`L^2` error behaves like :math:`a^2\Delta t`.

Strictly speaking, the numerical error is only defined at the
mesh points so it makes most sense to compute the
:math:`\ell^2` error

.. math::
         ||e^n||_{\ell^2} = \sqrt{\Delta t\sum_{n=0}^{N_t} ({{u_{\small\mbox{e}}}}(t_n) - u^n)^2}
        {\thinspace .} 

We have obtained an exact analytical expressions for the error at :math:`t=t_n`,
but here we use the leading-order error term only since we are mostly
interested in how the error behaves as a polynomial in :math:`\Delta t`, and then
the leading order term will dominate.
For the Forward Euler scheme,
:math:`{u_{\small\mbox{e}}}(t_n) - u^n \approx {\frac{1}{2}}np^2`, and we have

.. math::
         ||e^n||_{\ell^2}^2 = \Delta t\sum_{n=0}^{N_t} \frac{1}{4}n^2p^4
        =\Delta t\frac{1}{4}p^4 \sum_{n=0}^{N_t} n^2{\thinspace .}

Now, :math:`\sum_{n=0}^{N_t} n^2\approx \frac{1}{3}N_t^3`. Using this approximation,
setting :math:`N_t =T/\Delta t`, and taking the square root gives the expression

.. math::
         ||e^n||_{\ell^2} = \frac{1}{2}\sqrt{\frac{T^3}{3}} a^2\Delta t{\thinspace .}

Calculations for the Backward Euler scheme are very similar and provide
the same result, while the Crank-Nicolson scheme leads to

.. math::
         ||e^n||_{\ell^2} = \frac{1}{12}\sqrt{\frac{T^3}{3}}a^3\Delta t^2{\thinspace .}


.. admonition:: Summary of errors

   Both the point-wise and the time-integrated true errors are of
   second order in :math:`\Delta t` for the Crank-Nicolson scheme and of
   first order in :math:`\Delta t` for the Forward Euler and Backward Euler schemes.




Truncation error
----------------

The truncation error is a very frequently used error measure for
finite difference methods. It is defined as *the error
in the difference equation that arises when inserting the exact
solution*. Contrary to many other error measures, e.g., the
true error :math:`e^n={u_{\small\mbox{e}}}(t_n)-u^n`, the truncation error is a quantity that
is easily computable.

Let us illustrate the calculation of the truncation error
for the Forward Euler scheme.
We start with the difference equation on operator form,

.. math::
         \lbrack D_t u = -au\rbrack^n,

i.e.,

.. math::
         \frac{u^{n+1}-u^n}{\Delta t} = -au^n{\thinspace .}

The idea is to see how well the exact solution :math:`{u_{\small\mbox{e}}}(t)` fulfills
this equation. Since :math:`{u_{\small\mbox{e}}}(t)` in general will not obey the
discrete equation, error in the discrete equation, called
a *residual*, denoted here by :math:`R^n`:

.. math::
   :label: decay:analysis:trunc:Req
        
        R^n = \frac{{u_{\small\mbox{e}}}(t_{n+1})-{u_{\small\mbox{e}}}(t_n)}{\Delta t} + a{u_{\small\mbox{e}}}(t_n)
        {\thinspace .}
        
        

The residual is defined at each mesh point and is therefore a mesh
function with a superscript :math:`n`.

The interesting feature of :math:`R^n` is to see how it
depends on the discretization parameter :math:`\Delta t`.
The tool for reaching
this goal is to Taylor expand :math:`{u_{\small\mbox{e}}}` around the point where the
difference equation is supposed to hold, here :math:`t=t_n`.
We have that

.. math::
         {u_{\small\mbox{e}}}(t_{n+1}) = {u_{\small\mbox{e}}}(t_n) + {u_{\small\mbox{e}}}'(t_n)\Delta t + \frac{1}{2}{u_{\small\mbox{e}}}''(t_n)
        \Delta t^2 + \cdots 

Inserting this Taylor series in :eq:`decay:analysis:trunc:Req` gives

.. math::
         R^n = {u_{\small\mbox{e}}}'(t_n) + \frac{1}{2}{u_{\small\mbox{e}}}''(t_n)\Delta t + \ldots + a{u_{\small\mbox{e}}}(t_n){\thinspace .}

Now, :math:`{u_{\small\mbox{e}}}` fulfills the ODE :math:`{u_{\small\mbox{e}}}'=-a{u_{\small\mbox{e}}}` such that the first and last
term cancels and we have

.. math::
         R^n \approx \frac{1}{2}{u_{\small\mbox{e}}}''(t_n)\Delta t {\thinspace .} 

This :math:`R^n` is the *truncation error*, which for the Forward Euler is seen
to be of first order in :math:`\Delta t`.

The above procedure can be repeated for the Backward Euler and the
Crank-Nicolson schemes. We start with the scheme in operator notation,
write it out in detail, Taylor expand :math:`{u_{\small\mbox{e}}}` around the point :math:`\tilde t`
at which the difference equation is defined, collect terms that
correspond to the ODE (here :math:`{u_{\small\mbox{e}}}' + a{u_{\small\mbox{e}}}`), and identify the remaining
terms as the residual :math:`R`, which is the truncation error.
The Backward Euler scheme leads to

.. math::
         R^n \approx -\frac{1}{2}{u_{\small\mbox{e}}}''(t_n)\Delta t, 

while the Crank-Nicolson scheme gives

.. math::
         R^{n+\frac{1}{2}} \approx \frac{1}{24}{u_{\small\mbox{e}}}'''(t_{n+\frac{1}{2}})\Delta t^2{\thinspace .}

The *order* :math:`r` of a finite difference scheme is often defined through
the leading term :math:`\Delta t^r` in the truncation error. The above
expressions point out that the Forward and Backward Euler schemes are
of first order, while Crank-Nicolson is of second order.  We have
looked at other error measures in other sections, like the error in
amplification factor and the error :math:`e^n={u_{\small\mbox{e}}}(t_n)-u^n`, and expressed
these error measures in terms of :math:`\Delta t` to see the order of the
method. Normally, calculating the truncation error is more
straightforward than deriving the expressions for other error measures
and therefore the easiest way to establish the order of a scheme.

Consistency, stability, and convergence
---------------------------------------

.. index:: consistency

.. index:: stability

.. index:: convergence

Three fundamental concepts when solving differential equations by
numerical methods are consistency, stability, and convergence.  We
shall briefly touch these concepts below in the context of the present
model problem.

Consistency means that the error in the difference equation, measured
through the truncation error, goes to zero as :math:`\Delta t\rightarrow
0`. Since the truncation error tells how well the exact solution
fulfills the difference equation, and the exact solution fulfills the
differential equation, consistency ensures that the difference
equation approaches the differential equation in the limit. The
expressions for the truncation errors in the previous section are all
proportional to :math:`\Delta t` or :math:`\Delta t^2`, hence they vanish as
:math:`\Delta t\rightarrow 0`, and all the schemes are consistent.  Lack of
consistency implies that we actually solve a different differential
equation in the limit :math:`\Delta t\rightarrow 0` than we aim at.

Stability means that the numerical solution exhibits the same
qualitative properties as the exact solution. This is obviously a
feature we want the numerical solution to have. In the present
exponential decay model, the exact solution is monotone and
decaying. An increasing numerical solution is not in accordance with
the decaying nature of the exact solution and hence unstable. We can
also say that an oscillating numerical solution lacks the property of
monotonicity of the exact solution and is also unstable. We have seen
that the Backward Euler scheme always leads to monotone and decaying
solutions, regardless of :math:`\Delta t`, and is hence stable. The Forward
Euler scheme can lead to increasing solutions and oscillating
solutions if :math:`\Delta t` is too large and is therefore unstable unless
:math:`\Delta t` is sufficiently small.  The Crank-Nicolson can never lead
to increasing solutions and has no problem to fulfill that stability
property, but it can produce oscillating solutions and is unstable in
that sense, unless :math:`\Delta t` is sufficiently small.

Convergence implies that the global (true) error mesh function :math:`e^n =
{u_{\small\mbox{e}}}(t_n)-u^n\rightarrow 0` as :math:`\Delta t\rightarrow 0`. This is really
what we want: the numerical solution gets as close to the exact
solution as we request by having a sufficiently fine mesh.

Convergence is hard to establish theoretically, except in quite simple
problems like the present one. Stability and consistency are much
easier to calculate. A major breakthrough in the understanding of
numerical methods for differential equations came in 1956 when Lax and
Richtmeyer established equivalence between convergence on one hand and
consistency and stability on the other (the `Lax equivalence theorem <http://en.wikipedia.org/wiki/Lax_equivalence_theorem>`__).  In practice
it meant that one can first establish that a method is stable and
consistent, and then it is automatically convergent (which is much
harder to establish).  The result holds for linear problems only, and
in the world of nonlinear differential equations the relations between
consistency, stability, and convergence are much more complicated.

We have seen in the previous analysis that the Forward Euler,
Backward Euler, and Crank-Nicolson schemes are convergent (:math:`e^n\rightarrow 0`),
that they are consistent (:math:`R^n\rightarrow 0`, and that they are
stable under certain conditions on the size of :math:`\Delta t`.
We have also derived explicit mathematical expressions for :math:`e^n`,
the truncation error, and the stability criteria.

.. Look in Asher and Petzold, p 40

Exercises  (1)
==============

.. --- begin exercise ---

.. _decay:analysis:exer:fd:exp:plot:

Exercise 1: Visualize the accuracy of finite differences
--------------------------------------------------------

The purpose of this exercise is to visualize the accuracy of finite difference
approximations of the derivative of a given function.
For any finite difference approximation, take the Forward Euler difference
as an example, and any specific function, take  :math:`u=e^{-at}`,
we may introduce an error fraction

.. math::
         E = \frac{[D_t^+ u]^n}{u'(t_n)} = \frac{\exp{(-a(t_n+\Delta t))} - \exp{(-at_n)}}{-a\exp{(-at_n)\Delta t}} = \frac{1}{a\Delta t}\left(1 -\exp{(-a\Delta t)}\right),
        

and view :math:`E` as a function of :math:`\Delta t`. We expect that
:math:`\lim_{\Delta t\rightarrow 0}E=1`, while :math:`E` may deviate significantly from
unity for large :math:`\Delta t`. How the error depends on :math:`\Delta t` is best
visualized in a graph where we use a logarithmic scale for :math:`\Delta t`,
so we can cover many orders of magnitude of that quantity. Here is
a code segment creating an array of 100 intervals, on the logarithmic
scale, ranging from :math:`10^{-6}` to :math:`10^{-0.5}` and then plotting :math:`E` versus
:math:`p=a\Delta t` with logarithmic scale on the :math:`p` axis:

.. code-block:: python

        from numpy import logspace, exp
        from matplotlib.pyplot import plot, semilogx
        p = logspace(-6, -0.5, 101)
        y = (1-exp(-p))/p
        semilogx(p, y)

Illustrate such errors for the finite difference operators :math:`[D_t^+u]^n`
(forward), :math:`[D_t^-u]^n` (backward), and :math:`[D_t u]^n` (centered) in
the same plot.

Perform a Taylor series expansions of the error fractions and find
the leading order :math:`r` in the expressions of type
:math:`1 + Cp^r + {\mathcal{O}(p^{r+1)}}`, where :math:`C` is some constant.

.. --- begin hint in exercise ---

**Hint.**
To save manual calculations and learn more about symbolic computing,
make functions for the three difference operators and use ``sympy``
to perform the symbolic differences, differentiation, and Taylor series
expansion. To plot a symbolic expression ``E`` against ``p``, convert the
expression to a Python function first: ``E = sympy.lamdify([p], E)``.

.. --- end hint in exercise ---

Filename: ``decay_plot_fd_error.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:analysis:exer:growth:

Exercise 2: Explore the :math:`\theta`-rule for exponential growth
------------------------------------------------------------------

This exercise asks you to solve the ODE :math:`u'=-au` with :math:`a<0` such that
the ODE models exponential growth instead of exponential decay.  A
central theme is to investigate numerical artifacts and non-physical
solution behavior.

**a)**
Set :math:`a=-1` and run experiments with :math:`\theta=0, 0.5, 1` for
various values of :math:`\Delta t` to uncover numerical artifacts.
Recall that the exact solution is a
monotone, growing function when :math:`a<0`. Oscillations or significantly
wrong growth are signs of wrong qualitative behavior.

From the experiments, select four values of :math:`\Delta t` that
demonstrate the kind of numerical solutions that are characteristic
for this model.
Filename: ``growth_demo.py``.

**b)**
Write up the amplification factor and plot it for :math:`\theta=0,0.5,1`
together with the exact one for :math:`a\Delta t <0`. Use the plot to
explain the observations made in the experiments.

.. --- begin hint in exercise ---

**Hint.**
Modify the `decay_ampf_plot.py <http://tinyurl.com/jvzzcfn/decay/decay_ampf_plot.py>`__ code.

.. --- end hint in exercise ---

Filename: ``growth_ampf_plot.py``.

.. --- end exercise ---

.. !split

Model extensions
================

It is time to consider generalizations of the simple decay model
:math:`u=-au` and also to look at additional numerical solution methods.

Generalization: including a variable coefficient
------------------------------------------------

In the ODE for decay, :math:`u'=-au`, we now consider the case where :math:`a`
depends on time:

.. math::
   :label: decay:problem:a
        
        u'(t) = -a(t)u(t),\quad t\in (0,T],\quad u(0)=I {\thinspace .}
        
        

A Forward Euler scheme consist of evaluating :eq:`decay:problem:a`
at :math:`t=t_n` and approximating the derivative with a forward
difference :math:`[D^+_t u]^n`:

.. math::
        
        \frac{u^{n+1} - u^n}{\Delta t} = -a(t_n)u^n
        {\thinspace .}
        

The Backward Euler scheme becomes

.. math::
        
        \frac{u^{n} - u^{n-1}}{\Delta t} = -a(t_n)u^n
        {\thinspace .}
        

The Crank-Nicolson method builds on sampling the ODE at
:math:`t_{n+\frac{1}{2}}`. We can evaluate :math:`a` at :math:`t_{n+\frac{1}{2}}`
and use an average for :math:`u` at
times :math:`t_n` and :math:`t_{n+1}`:

.. math::
        
        \frac{u^{n+1} - u^{n}}{\Delta t} = -a(t_{n+\frac{1}{2}})\frac{1}{2}(u^n + u^{n+1})
        {\thinspace .}
        

Alternatively, we can use an average for the product :math:`au`:

.. math::
        
        \frac{u^{n+1} - u^{n}}{\Delta t} = -\frac{1}{2}(a(t_n)u^n + a(t_{n+1})u^{n+1})
        {\thinspace .}
        

The :math:`\theta`-rule unifies the three mentioned schemes. One version is to
have :math:`a` evaluated at :math:`t_{n+\theta}`,

.. math::
        
        \frac{u^{n+1} - u^{n}}{\Delta t} = -a((1-\theta)t_n + \theta t_{n+1})((1-\theta) u^n + \theta u^{n+1})
        {\thinspace .}
        

Another possibility is to apply a weighted average for the product :math:`au`,

.. math::
        
        \frac{u^{n+1} - u^{n}}{\Delta t} = -(1-\theta) a(t_n)u^n - \theta
        a(t_{n+1})u^{n+1}
        {\thinspace .}
        

With the finite difference operator notation the Forward Euler and Backward
Euler schemes can be summarized as

.. math::
        
        \lbrack D^+_t u = -au\rbrack^n,
        

.. math::
          
        \lbrack D^-_t u = -au\rbrack^n
        {\thinspace .}
        

The Crank-Nicolson and :math:`\theta` schemes depend on whether we evaluate
:math:`a` at the sample point for the ODE or if we use an average. The
various versions are written as

.. math::
        
        \lbrack D_t u = -a\overline{u}^t\rbrack^{n+\frac{1}{2}},
        

.. math::
          
        \lbrack D_t u = -\overline{au}^t\rbrack^{n+\frac{1}{2}},
        

.. math::
          
        \lbrack D_t u = -a\overline{u}^{t,\theta}\rbrack^{n+\theta},
        

.. math::
          
        \lbrack D_t u = -\overline{au}^{t,\theta}\rbrack^{n+\theta}
        {\thinspace .}
        

.. _decay:source:

Generalization: including a source term
---------------------------------------

A further extension of the model ODE is to include a source term :math:`b(t)`:

.. math::
   :label: decay:problem:ab
        
        u'(t) = -a(t)u(t) + b(t),\quad t\in (0,T],\quad u(0)=I
        {\thinspace .}
        
        

Schemes
~~~~~~~

The time point where we sample the ODE determines where :math:`b(t)` is
evaluated. For the Crank-Nicolson scheme and the :math:`\theta`-rule we
have a choice of whether to evaluate :math:`a(t)` and :math:`b(t)` at the
correct point or use an average. The chosen strategy becomes
particularly clear if we write up the schemes in the operator notation:

.. math::
        
        \lbrack D^+_t u = -au + b\rbrack^n,
        

.. math::
          
        \lbrack D^-_t u = -au + b\rbrack^n,
        

.. math::
          
        \lbrack D_t u   = -a\overline{u}^t + b\rbrack^{n+\frac{1}{2}},
        

.. math::
          
        \lbrack D_t u   = \overline{-au+b}^t\rbrack^{n+\frac{1}{2}},
        

.. math::
          
        \lbrack D_t u   = -a\overline{u}^{t,\theta} + b\rbrack^{n+\theta},
        

.. math::
   :label: decay:problem:ab:theta:avg:all:op
          
        \lbrack D_t u   = \overline{-au+b}^{t,\theta}\rbrack^{n+\theta}
        
        {\thinspace .}
        

.. _decay:general:

Implementation of the generalized model problem
-----------------------------------------------

Deriving the :math:`\theta`-rule formula
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Writing out the :math:`\theta`-rule in :eq:`decay:problem:ab:theta:avg:all:op`,
using :eq:`decay:fd1:Du:theta`
and :eq:`decay:fd1:wmean:a`, we get

.. math::
   :label: decay:problem:ab:theta:avg:all
        
        \frac{u^{n+1}-u^n}{\Delta t} = \theta(-a^{n+1}u^{n+1} + b^{n+1}))
        + (1-\theta)(-a^nu^{n} + b^n)),
        
        

where :math:`a^n` means evaluating :math:`a` at :math:`t=t_n` and similar for
:math:`a^{n+1}`, :math:`b^n`, and :math:`b^{n+1}`.
We solve for :math:`u^{n+1}`:

.. math::
        
        u^{n+1} = ((1 - \Delta t(1-\theta)a^n)u^n
        + \Delta t(\theta b^{n+1} + (1-\theta)b^n))(1 + \Delta t\theta a^{n+1})^{-1}
        {\thinspace .}
        

The Python code
~~~~~~~~~~~~~~~

Here is a suitable implementation of :eq:`decay:problem:ab:theta:avg:all`
where :math:`a(t)` and :math:`b(t)` are given as
Python functions:

.. code-block:: python

        def solver(I, a, b, T, dt, theta):
            """
            Solve u'=-a(t)*u + b(t), u(0)=I,
            for t in (0,T] with steps of dt.
            a and b are Python functions of t.
            """
            dt = float(dt)            # avoid integer division
            Nt = int(round(T/dt))     # no of time intervals
            T = Nt*dt                 # adjust T to fit time step dt
            u = zeros(Nt+1)           # array of u[n] values
            t = linspace(0, T, Nt+1)  # time mesh
        
            u[0] = I                  # assign initial condition
            for n in range(0, Nt):    # n=0,1,...,Nt-1
                u[n+1] = ((1 - dt*(1-theta)*a(t[n]))*u[n] + \ 
                          dt*(theta*b(t[n+1]) + (1-theta)*b(t[n])))/\ 
                          (1 + dt*theta*a(t[n+1]))
            return u, t

This function is found in the file `decay_vc.py <http://tinyurl.com/jvzzcfn/decay/decay_vc.py>`__ (``vc`` stands for "variable coefficients").

Coding of variable coefficients
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``solver`` function shown above demands the arguments ``a`` and ``b`` to
be Python functions of time ``t``, say

.. code-block:: python

        def a(t):
            return a_0 if t < tp else k*a_0
        
        def b(t):
            return 1

Here, ``a(t)`` has three parameters ``a0``, ``tp``, and ``k``,
which must be global variables.
A better implementation is to represent ``a`` by a class where the
parameters are attributes and a *special method* ``__call__`` evaluates :math:`a(t)`:

.. code-block:: python

        class A:
            def __init__(self, a0=1, k=2):
                self.a0, self.k = a0, k
        
            def __call__(self, t):
                return self.a0 if t < self.tp else self.k*self.a0
        
        a = A(a0=2, k=1)  # a behaves as a function a(t)

.. index:: lambda functions

For quick tests it is cumbersome to write a complete function or a class.
The *lambda function* construction in Python is then convenient. For example,

.. code-block:: python

        a = lambda t: a_0 if t < tp else k*a_0

is equivalent to the ``def a(t):`` definition above. In general,

.. code-block:: python

        f = lambda arg1, arg2, ...: expressin

is equivalent to

.. code-block:: python

        def f(arg1, arg2, ...):
            return expression

One can use lambda functions directly in calls. Say we want to
solve :math:`u'=-u+1`, :math:`u(0)=2`:

.. code-block:: python

        u, t = solver(2, lambda t: 1, lambda t: 1, T, dt, theta)

A lambda function can appear anywhere where a variable can appear.

.. _decay:verify:trivial:

Verifying a constant solution
-----------------------------

A very useful partial verification method is to construct a test
problem with a very simple solution, usually :math:`u=\hbox{const}`.
Especially the initial debugging of a program code can benefit greatly
from such tests, because 1) all relevant numerical methods will
exactly reproduce a constant solution, 2) many of the intermediate
calculations are easy to control for a constant :math:`u`, and 3) even a
constant :math:`u` can uncover many bugs in an implementation.

The only constant solution for the problem :math:`u'=-au` is :math:`u=0`, but too
many bugs can escape from that trivial solution.  It is much better to
search for a problem where :math:`u=C=\hbox{const}\neq 0`.  Then :math:`u'=-a(t)u
+ b(t)` is more appropriate: with :math:`u=C` we can choose any :math:`a(t)` and
set :math:`b=a(t)C` and :math:`I=C`. An appropriate test function is

.. code-block:: python

        def test_constant_solution():
            """
            Test problem where u=u_const is the exact solution, to be
            reproduced (to machine precision) by any relevant method.
            """
            def exact_solution(t):
                return u_const
        
            def a(t):
                return 2.5*(1+t**3)  # can be arbitrary
        
            def b(t):
                return a(t)*u_const
        
            u_const = 2.15
            theta = 0.4; I = u_const; dt = 4
            Nt = 4  # enough with a few steps
            u, t = solver(I=I, a=a, b=b, T=Nt*dt, dt=dt, theta=theta)
            print u
            u_e = exact_solution(t)
            difference = abs(u_e - u).max()  # max deviation
            tol = 1E-14
            assert difference < tol

An interesting question is what type of bugs that will make the
computed :math:`u^n` deviate from the exact solution :math:`C`.
Fortunately, the updating formula and the initial condition must
be absolutely correct for the test to pass! Any attempt to make
a wrong indexing in terms like ``a(t[n])`` or any attempt to
introduce an erroneous factor in the formula creates a solution
that is different from :math:`C`.

.. _decay:MMS:

Verification via manufactured solutions
---------------------------------------

.. index:: method of manufactured solutions

.. index:: MMS (method of manufactured solutions)

Following the idea of the previous section, we can choose any formula
as the exact solution, insert the formula in the ODE problem and fit
the data :math:`a(t)`, :math:`b(t)`, and :math:`I` to make the chosen
formula fulfill the equation. This
powerful technique for generating exact solutions is very useful for
verification purposes and known as the *method of manufactured
solutions*, often abbreviated MMS.

One common choice of solution is a linear function in the independent
variable(s). The rationale behind such a simple variation is that
almost any relevant numerical solution method for differential
equation problems is able to reproduce the linear function exactly to
machine precision (if :math:`u` is about unity in size; precision is lost if
:math:`u` take on large values, see :ref:`decay:fd2:exer:precision`).
The linear solution also makes some stronger demands to the
numerical method and the implementation than the constant solution
used in the section :ref:`decay:verify:trivial`, at least in more
complicated applications. However, the constant solution is often
ideal for initial debugging before proceeding with a linear solution.

We choose a linear solution :math:`u(t) = ct + d`. From the initial condition it
follows that :math:`d=I`.
Inserting this :math:`u` in the ODE results in

.. math::
         c = -a(t)u + b(t) {\thinspace .}  

Any function :math:`u=ct+I` is then a correct solution if we choose

.. math::
         b(t) = c + a(t)(ct + I) {\thinspace .}  

With this :math:`b(t)` there are no restrictions on :math:`a(t)` and :math:`c`.

Let prove that such a linear solution obeys the numerical
schemes. To this end, we must check that :math:`u^n = ca(t_n)(ct_n+I)`
fulfills the discrete equations. For these calculations, and
later calculations involving linear solutions inserted in
finite difference schemes, it is convenient to
compute the action of a difference operator on a linear function :math:`t`:

.. math::
   :label: decay:fd2:Dop:tn:fw
        
        \lbrack D_t^+ t\rbrack^n = \frac{t_{n+1}-t_n}{\Delta t}=1,
        
        

.. math::
   :label: decay:fd2:Dop:tn:bw
          
        \lbrack D_t^- t\rbrack^n = \frac{t_{n}-t_{n-1}}{\Delta t}=1,
        
        

.. math::
   :label: decay:fd2:Dop:tn:cn
          
        \lbrack D_t t\rbrack^n = \frac{t_{n+\frac{1}{2}}-t_{n-\frac{1}{2}}}{\Delta t}=\frac{(n+\frac{1}{2})\Delta t - (n-\frac{1}{2})\Delta t}{\Delta t}=1
        
        {\thinspace .}
        

Clearly, all three finite difference approximations to the derivative are
exact for :math:`u(t)=t` or its mesh function counterpart :math:`u^n = t_n`.

The difference equation for the Forward Euler scheme

.. math::
         [D^+_t u = -au + b]^n, 

with :math:`a^n=a(t_n)`, :math:`b^n=c + a(t_n)(ct_n + I)`, and :math:`u^n=ct_n + I`
then results in

.. math::
         c = -a(t_n)(ct_n+I) + c + a(t_n)(ct_n + I) = c 

which is always fulfilled. Similar calculations can be done for the
Backward Euler and Crank-Nicolson schemes, or the :math:`\theta`-rule for
that matter. In all cases, :math:`u^n=ct_n +I` is an exact solution of
the discrete equations. That is why we should expect that
:math:`u^n - {u_{\small\mbox{e}}}(t_n) =0` mathematically and :math:`|u^n - {u_{\small\mbox{e}}}(t_n)|` less
than a small number about the machine precision for :math:`n=0,\ldots,N_t`.

The following function offers an implementation of this verification
test based on a linear exact solution:

.. code-block:: python

        def test_linear_solution():
            """
            Test problem where u=c*t+I is the exact solution, to be
            reproduced (to machine precision) by any relevant method.
            """
            def exact_solution(t):
                return c*t + I
        
            def a(t):
                return t**0.5  # can be arbitrary
        
            def b(t):
                return c + a(t)*exact_solution(t)
        
            theta = 0.4; I = 0.1; dt = 0.1; c = -0.5
            T = 4
            Nt = int(T/dt)  # no of steps
            u, t = solver(I=I, a=a, b=b, T=Nt*dt, dt=dt, theta=theta)
            u_e = exact_solution(t)
            difference = abs(u_e - u).max()  # max deviation
            print difference
            tol = 1E-14  # depends on c!
            assert difference < tol

Any error in the updating formula makes this test fail!

Choosing more complicated formulas as the exact solution, say
:math:`\cos(t)`, will not make the numerical and exact solution
coincide to machine precision, because finite differencing of
:math:`\cos(t)` does not exactly yield the exact derivative :math:`-\sin(t)`.
In such cases, the verification procedure
must be based on measuring the convergence rates as exemplified in
the section :ref:`decay:convergence:rate`. Convergence rates can be
computed as long as one has
an exact solution of a problem that the solver can be tested on, but
this can always be obtained by the method of manufactured solutions.

Extension to systems of ODEs
----------------------------

Many ODE models involves more than one unknown function and more
than one equation. Here is an example of two unknown functions :math:`u(t)`
and :math:`v(t)`:

.. math::
        
        u' = a u + bv,
        

.. math::
          
        v' = cu +  dv,
        

for constants :math:`a,b,c,d`.
Applying the Forward Euler method to each equation results in simple
updating formula

.. math::
        
        u^{n+1} = u^n + \Delta t (a u^n + b v^n),
        

.. math::
          
        v^{n+1} = u^n + \Delta t (cu^n + dv^n)
        {\thinspace .}
        

On the other hand, the Crank-Nicolson or Backward Euler schemes result in a
:math:`2\times 2` linear system for the new unknowns. The latter schemes gives

.. math::
        
        u^{n+1} = u^n + \Delta t (a u^{n+1} + b v^{n+1}),
        

.. math::
          
        v^{n+1} = v^n + \Delta t (c u^{n+1} + d v^{n+1}){\thinspace .}
        

Collecting :math:`u^{n+1}` as well as :math:`v^{n+1}` on the left-hand side results
in

.. math::
        
        (1 - \Delta t a)u^{n+1} + bv^{n+1} = u^n ,
        

.. math::
          
        c u^{n+1} + (1 - \Delta t d) v^{n+1} = v^n ,
        

which is a system of two coupled, linear, algebraic equations in two
unknowns.

General first-order ODEs
========================

We now turn the attention to general, nonlinear ODEs and systems of
such ODEs.  Our focus is on numerical methods that can be readily
reused for time-discretization PDEs, and diffusion PDEs in particular.
The methods are just briefly listed, and we refer to the rich literature
for more detailed descriptions and analysis - the books
[Ref2]_ [Ref3]_ [Ref4]_ [Ref5]_ are all excellent resources on numerical methods for ODEs.
We also demonstrate the Odespy Python interface to a range
of different software for general first-order ODE systems.

Generic form of first-order ODEs
--------------------------------

ODEs are commonly written in the generic form

.. math::
   :label: decay:ode:general
        
        u' = f(u,t),\quad u(0)=I,
        
        

where :math:`f(u,t)`  is some prescribed function.
As an example, our most
general exponential decay model :eq:`decay:problem:ab` has
:math:`f(u,t)=-a(t)u(t) + b(t)`.

The unknown :math:`u` in :eq:`decay:ode:general` may either be
a scalar function of time :math:`t`, or a vector valued function of :math:`t` in
case of a *system of ODEs* with :math:`m` unknown components:

.. math::
         u(t) = (u^{(0)}(t),u^{(1)}(t),\ldots,u^{(m-1)}(t)) {\thinspace .}  

In that case, the right-hand side is vector-valued function with :math:`m`
components,

.. math::
        
        f(u, t) = ( & f^{(0)}(u^{(0)}(t),\ldots,u^{(m-1)}(t)),\\ 
                    & f^{(1)}(u^{(0)}(t),\ldots,u^{(m-1)}(t)),\\ 
                    & \vdots,\\ 
                    & f^{(m-1)}(u^{(0)}(t),\ldots,u^{(m-1)}(t)))
        {\thinspace .}
        

Actually, any system of ODEs can
be written in the form :eq:`decay:ode:general`, but higher-order
ODEs then need auxiliary unknown functions to enable conversion to
a first-order system.

Next we list some well-known methods for :math:`u'=f(u,t)`, valid both for
a single ODE (scalar :math:`u`) and systems of ODEs (vector :math:`u`).
The choice of methods is inspired by the kind of schemes that are
popular also for time discretization of partial differential equations.

The :math:`\theta`-rule
-----------------------

The :math:`\theta`-rule scheme applied to :math:`u'=f(u,t)` becomes

.. math::
   :label: decay:fd2:theta
        
        \frac{u^{n+1}-u^n}{\Delta t} = \theta f(u^{n+1},t_{n+1}) +
        (1-\theta)f(u^n, t_n){\thinspace .}
        
        

Bringing the unknown :math:`u^{n+1}` to the left-hand side and the known terms
on the right-hand side gives

.. index:: implicit schemes

.. index:: explicit schemes

.. index:: theta-rule

.. index:: theta-rule

.. math::
        
        u^{n+1} - \Delta t \theta f(u^{n+1},t_{n+1}) =
        u^n + \Delta t(1-\theta)f(u^n, t_n){\thinspace .}
        

For a general :math:`f` (not linear in :math:`u`), this equation is *nonlinear* in
the unknown :math:`u^{n+1}` unless :math:`\theta = 0`. For a scalar ODE (:math:`m=1`),
we have to solve a single nonlinear algebraic equation for :math:`u^{n+1}`,
while for a system of ODEs, we get a system of coupled, nonlinear
algebraic equations. Newton's method is a popular solution approach
in both cases. Note that with the Forward Euler scheme (:math:`\theta =0`)
we do not have to deal with nonlinear equations, because in that
case we have an explicit updating formula for :math:`u^{n+1}`. This is known
as an *explicit* scheme. With :math:`\theta\neq 1` we have to solve
(systems of) algebraic equations, and the scheme is said to be *implicit*.

An implicit 2-step backward scheme
----------------------------------

.. index::
   single: backward scheme, 2-step

.. index:: BDF2 scheme

The implicit backward method with 2 steps applies a
three-level backward difference as approximation to :math:`u'(t)`,

.. math::
         u'(t_{n+1}) \approx \frac{3u^{n+1} - 4u^{n} + u^{n-1}}{2\Delta t},

which is an approximation of order :math:`\Delta t^2` to the first derivative.
The resulting scheme for :math:`u'=f(u,t)` reads

.. math::
   :label: decay:fd2:bw:2step
        
        u^{n+1} = \frac{4}{3}u^n - \frac{1}{3}u^{n-1} +
        \frac{2}{3}\Delta t f(u^{n+1}, t_{n+1})
        {\thinspace .}
        
        

Higher-order versions of the scheme :eq:`decay:fd2:bw:2step` can
be constructed by including more time levels. These schemes are known
as the Backward Differentiation Formulas (BDF), and the particular
version :eq:`decay:fd2:bw:2step` is often referred to as BDF2.

Note that the scheme :eq:`decay:fd2:bw:2step` is implicit and requires
solution of nonlinear equations when :math:`f` is nonlinear in :math:`u`.  The
standard 1st-order Backward Euler method or the Crank-Nicolson scheme
can be used for the first step.

Leapfrog schemes
----------------

.. index:: Leapfrog scheme

The ordinary Leapfrog scheme
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The derivative of :math:`u` at some point :math:`t_n` can be approximated by
a central difference over two time steps,

.. math::
        
        u'(t_n)\approx \frac{u^{n+1}-u^{n-1}}{2\Delta t} = [D_{2t}u]^n
        

which is an approximation of second order in :math:`\Delta t`. The scheme
can then be written as

.. math::
         [D_{2t}u=f(u,t)]^n, 

in operator notation. Solving for :math:`u^{n+1}` gives

.. math::
   :label: decay:fd2:leapfrog
        
        u^{n+1} = u^{n-1} + \Delta t f(u^n, t_n)
        {\thinspace .}
        
        

Observe that :eq:`decay:fd2:leapfrog` is an explicit scheme, and that
a nonlinear :math:`f` (in :math:`u`) is trivial to handle since it only involves
the known :math:`u^n` value.
Some other scheme must be used as starter to compute :math:`u^1`, preferably
the Forward Euler scheme since it is also explicit.

.. index::
   single: Leapfrog scheme, filtered

The filtered Leapfrog scheme
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Unfortunately, the Leapfrog scheme :eq:`decay:fd2:leapfrog`
will develop growing oscillations with time (see :ref:`decay:fd2:exer:leapfrog1`)[[[. A remedy for such undesired oscillations
is to introduce a *filtering technique*. First, a standard Leapfrog
step is taken, according to :eq:`decay:fd2:leapfrog`, and then
the previous :math:`u^n` value is adjusted according to

.. math::
   :label: decay:fd2:leapfrog:filtered
        
        u^n\ \leftarrow\ u^n + \gamma (u^{n-1} - 2u^n + u^{n+1})
        
        {\thinspace .}
        

The :math:`\gamma`-terms will effectively damp oscillations in the solution,
especially those with short wavelength (like point-to-point oscillations).
A common choice of :math:`\gamma` is 0.6 (a value used in the
famous NCAR Climate Model).

.. Need to elaborate more on this:

.. The difference in th :math:`\gamma` term in :eq:`decay:fd2:leapfrog:filtered`

.. can be recognized as a finite difference approximation to

.. :math:`\Delta t^2 u''(t_n)`.

The 2nd-order Runge-Kutta method
--------------------------------

.. index:: Heun's method

.. index::
   single: Runge-Kutta, 2nd-order method

The two-step scheme

.. math::
   :label: decay:fd2:RK2:s1
        
        u^* = u^n + \Delta t f(u^n, t_n),
        
        

.. math::
   :label: decay:fd2:RK2:s2
          
        u^{n+1} = u^n + \Delta t \frac{1}{2} \left( f(u^n, t_n) + f(u^*, t_{n+1})
        \right),
        
        

essentially applies a Crank-Nicolson method :eq:`decay:fd2:RK2:s2`
to the ODE, but replaces
the term :math:`f(u^{n+1}, t_{n+1})` by a prediction
:math:`f(u^{*}, t_{n+1})` based on a Forward Euler step :eq:`decay:fd2:RK2:s1`.
The scheme :eq:`decay:fd2:RK2:s1`-:eq:`decay:fd2:RK2:s2` is
known as Huen's method, but is also a 2nd-order Runge-Kutta method.
The scheme is explicit, and the error is expected to behave as :math:`\Delta t^2`.

A 2nd-order Taylor-series method
--------------------------------

.. index:: Taylor-series methods (for ODEs)

One way to compute :math:`u^{n+1}` given :math:`u^n` is to use a Taylor polynomial.
We may write up a polynomial of 2nd degree:

.. math::
        
        u^{n+1} = u^n + u'(t_n)\Delta t + {\frac{1}{2}}u''(t_n)\Delta t^2
        {\thinspace .}
        

From the equation :math:`u'=f(u,t)` it follows that the derivatives of :math:`u`
can be expressed in terms of :math:`f` and its derivatives:

.. math::
        
        u'(t_n) &=f(u^n,t_n),\\ 
        u''(t_n) &=
        \frac{\partial f}{\partial u}(u^n,t_n) u'(t_n) + \frac{\partial f}{\partial t}\\ 
        &=  f(u^n,t_n)\frac{\partial f}{\partial u}(u^n,t_n)  +
        \frac{\partial f}{\partial t},
        

resulting in the scheme

.. math::
   :label: decay:fd2:Taylor2
        
        u^{n+1} = u^n + f(u^n,t_n)\Delta t + \frac{1}{2}\left(
        f(u^n,t_n)\frac{\partial f}{\partial u}(u^n,t_n)  +
        \frac{\partial f}{\partial t}\right)\Delta t^2
        {\thinspace .}
        
        

More terms in the series could be included in the Taylor polynomial to
obtain methods of higher order than 2.

The 2nd- and 3rd-order Adams-Bashforth schemes
----------------------------------------------

.. index::
   single: Adams-Bashforth scheme, 2nd-order

The following method is known as the 2nd-order Adams-Bashforth scheme:

.. math::
   :label: decay:fd2:AB2
        
        u^{n+1} = u^n + \frac{1}{2}\Delta t\left( 3f(u^n, t_n) - f(u^{n-1}, t_{n-1})
        \right)
        {\thinspace .}
        
        

The scheme is explicit and requires another one-step scheme to compute
:math:`u^1` (the Forward Euler scheme or Heun's method, for instance).
As the name implies, the scheme is of order :math:`\Delta t^2`.

.. index::
   single: Adams-Bashforth scheme, 3rd order

Another explicit scheme, involving four time levels, is the
3rd-order Adams-Bashforth scheme

.. math::
   :label: decay:fd2:AB3
        
        u^{n+1} = u^n + \frac{1}{12}\left( 23f(u^n, t_n) - 16 f(u^{n-1},t_{n-1})
        + 5f(u^{n-2}, t_{n-2})\right)
        {\thinspace .}
        
        

The numerical error is of order :math:`\Delta t^3`, and the scheme needs
some method for computing :math:`u^1` and :math:`u^2`.

More general, higher-order Adams-Bashforth schemes (also called
*explicit Adams methods*) compute :math:`u^{n+1}` as a linear combination
of :math:`f` at :math:`k` previous time steps:

.. math::
         u^{n+1} = u^n + \sum_{j=0}^k \beta_jf(u^{n-j},t_{n-j}),

where :math:`\beta_j` are known coefficients.

.. _decay:fd2:RK4:

The 4th-order Runge-Kutta method
--------------------------------

.. index::
   single: Runge-Kutta, 4th-order method

.. index:: RK4

The perhaps most widely used method to solve ODEs is the 4th-order
Runge-Kutta method, often called RK4.
Its derivation is a nice illustration of common
numerical approximation strategies, so let us go through the
steps in detail.

The starting point is to integrate the ODE
:math:`u'=f(u,t)` from :math:`t_n` to :math:`t_{n+1}`:

.. math::
         u(t_{n+1}) - u(t_n) = \int\limits_{t_{n}}^{t_{n+1}} f(u(t),t)dt{\thinspace .} 

We want to compute :math:`u(t_{n+1})` and regard :math:`u(t_n)` as known.
The task is to find good approximations for the integral, since the
integrand involves the unknown :math:`u` between :math:`t_n` and :math:`t_{n+1}`.

The integral can be approximated by the famous
`Simpson's rule <http://en.wikipedia.org/wiki/Simpson's_rule>`__:

.. math::
         \int\limits_{t_{n}}^{t_{n+1}} f(u(t),t)dt
        \approx \frac{\Delta t}{6}\left( f^n + 4f^{n+\frac{1}{2}} + f^{n+1}\right){\thinspace .}

The problem now is that we do not know :math:`f^{n+\frac{1}{2}}=f(u^{n+\frac{1}{2}},t_{n+1/2})`
and :math:`f^{n+1}=(u^{n+1},t_{n+1})` as we know only :math:`u^n` and hence :math:`f^n`.
The idea is to use various approximations for :math:`f^{n+\frac{1}{2}}` and
:math:`f^{n+1}` based on using well-known schemes for the ODE in the
intervals :math:`[t_n,t_{n+1/2}]` and :math:`[t_n, t_{n+1}]`.
We split the integral approximation into four terms:

.. math::
         \int\limits_{t_{n}}^{t_{n+1}} f(u(t),t)dt
        \approx \frac{\Delta t}{6}\left( f^n + 2\hat{f}^{n+\frac{1}{2}}
        + 2\tilde{f}^{n+\frac{1}{2}} + \bar{f}^{n+1}\right),

where :math:`\hat{f}^{n+\frac{1}{2}}`, :math:`\tilde{f}^{n+\frac{1}{2}}`, and :math:`\bar{f}^{n+1}`
are approximations to :math:`f^{n+\frac{1}{2}}` and
:math:`f^{n+1}` that can be based on already computed quantities.
For :math:`\hat{f}^{n+\frac{1}{2}}` we can apply
an approximation to :math:`u^{n+\frac{1}{2}}` using the Forward Euler
method with step :math:`\frac{1}{2}\Delta t`:

.. math::
   :label: decay:fd2:RK4:hatf
        
        \hat{f}^{n+\frac{1}{2}} = f(u^n + \frac{1}{2}{\Delta t} f^n, t_{n+1/2})
        
        

Since this gives us a prediction of :math:`f^{n+\frac{1}{2}}`, we can for
:math:`\tilde{f}^{n+\frac{1}{2}}` try a Backward Euler method to approximate :math:`u^{n+\frac{1}{2}}`:

.. math::
   :label: decay:fd2:RK4:tildef
        
        \tilde{f}^{n+\frac{1}{2}} = f(u^n + \frac{1}{2}\Delta t\hat{f}^{n+\frac{1}{2}}, t_{n+1/2}){\thinspace .}
        
        

With :math:`\tilde{f}^{n+\frac{1}{2}}` as a hopefully good approximation to
:math:`f^{n+\frac{1}{2}}`, we can for the final term :math:`\bar{f}^{n+1}` use
a Crank-Nicolson method to approximate :math:`u^{n+1}`:

.. math::
   :label: decay:fd2:RK4:barf
        
        \bar{f}^{n+1} = f(u^n + \Delta t \hat{f}^{n+\frac{1}{2}}, t_{n+1}){\thinspace .}
        
        

We have now used the Forward and Backward Euler methods as well as the
Crank-Nicolson method in the context of Simpson's rule. The hope is
that the combination of these methods yields an overall time-stepping
scheme from :math:`t_n` to :math:`t_n{+1}` that is much more accurate than the
:math:`{\mathcal{O}(\Delta t)}` and :math:`{\mathcal{O}(\Delta t^2)}` of the individual steps.
This is indeed true: the overall accuracy is :math:`{\mathcal{O}(\Delta t^4)}`!

To summarize, the 4th-order Runge-Kutta method becomes

.. math::
        
        u^{n+1} = u^n +
        \frac{\Delta t}{6}\left( f^n + 2\hat{f}^{n+\frac{1}{2}}
        + 2\tilde{f}^{n+\frac{1}{2}} + \bar{f}^{n+1}\right),
        

where the quantities on the right-hand side are computed from
:eq:`decay:fd2:RK4:hatf`-:eq:`decay:fd2:RK4:barf`. Note that
the scheme is fully explicit so there is never any need to solve linear or
nonlinear algebraic
equations. However, the stability is conditional and depends on :math:`f`.
There is a whole range of *implicit* Runge-Kutta methods that
are unconditionally stable, but require solution of algebraic
equations involving :math:`f` at each time step.

The simplest way to explore more sophisticated methods for ODEs is to
apply one of the many high-quality software packages that exist, as the
next section explains.

The Odespy software
-------------------

A wide range of the methods and software exist for solving :eq:`decay:ode:general`.
Many of methods are accessible through a unified Python interface offered
by the `Odespy <https://github.com/hplgit/odespy>`__ package.
Odespy features simple Python implementations of the most fundamental
schemes as well as Python interfaces to several famous packages for
solving ODEs: `ODEPACK <https://computation.llnl.gov/casc/odepack/odepack_home.html>`__, `Vode <https://computation.llnl.gov/casc/odepack/odepack_home.html>`__,
`rkc.f <http://www.netlib.org/ode/rkc.f>`__, `rkf45.f <http://www.netlib.org/ode/rkf45.f>`__, `Radau5 <http://www.unige.ch/~hairer/software.html>`__, as well
as the ODE solvers in `SciPy <http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.ode.html>`__, `SymPy <http://docs.sympy.org/dev/modules/mpmath/calculus/odes.html>`__, and `odelab <http://olivierverdier.github.com/odelab/>`__.

The usage of Odespy follows this setup for the ODE :math:`u'=-au`,
:math:`u(0)=I`, :math:`t\in (0,T]`, here solved
by the famous 4th-order Runge-Kutta method, using :math:`\Delta t=1`
and :math:`N_t=6` steps:

.. code-block:: text

        def f(u, t):
            return -a*u
        
        import odespy
        import numpy as np
        
        I = 1; a = 0.5; Nt = 6; dt = 1
        solver = odespy.RK4(f)
        solver.set_initial_condition(I)
        t_mesh = np.linspace(0, Nt*dt, Nt+1)
        u, t = solver.solve(t_mesh)

The previously listed methods for ODEs are all accessible in
Odespy:

 * the :math:`\theta`-rule: ``ThetaRule``

 * special cases of the :math:`\theta`-rule: ``ForwardEuler``, ``BackwardEuler``,
   ``CrankNicolson``

 * the 2nd- and 4th-order Runge-Kutta methods: ``RK2`` and ``RK4``

 * The BDF methods and the Adam-Bashforth methods:
   ``Vode``, ``Lsode``, ``Lsoda``, ``lsoda_scipy``

 * The Leapfrog scheme: ``Leapfrog`` and ``LeapfrogFiltered``

Example: Runge-Kutta methods
----------------------------

Since all solvers have the same interface in Odespy, modulo different set of
parameters to the solvers' constructors, one can easily make a list of
solver objects and run a loop for comparing (a lot of) solvers. The
code below, found in complete form in `decay_odespy.py <http://tinyurl.com/jvzzcfn/decay/decay_odespy.py>`__,
compares the famous Runge-Kutta methods of orders 2, 3, and 4
with the exact solution of the decay equation
:math:`u'=-au`.
Since we have quite long time steps, we have included the only
relevant :math:`\theta`-rule for large time steps, the Backward Euler scheme
(:math:`\theta=1`), as well.
Figure :ref:`decay:odespy:fig1` shows the results.

.. code-block:: python

        import numpy as np
        import scitools.std as plt
        import sys
        
        def f(u, t):
            return -a*u
        
        I = 1; a = 2; T = 6
        dt = float(sys.argv[1]) if len(sys.argv) >= 2 else 0.75
        Nt = int(round(T/dt))
        t = np.linspace(0, Nt*dt, Nt+1)
        
        solvers = [odespy.RK2(f),
                   odespy.RK3(f),
                   odespy.RK4(f),
                   odespy.BackwardEuler(f, nonlinear_solver='Newton')]
        
        legends = []
        for solver in solvers:
            solver.set_initial_condition(I)
            u, t = solver.solve(t)
        
            plt.plot(t, u)
            plt.hold('on')
            legends.append(solver.__class__.__name__)
        
        # Compare with exact solution plotted on a very fine mesh
        t_fine = np.linspace(0, T, 10001)
        u_e = I*np.exp(-a*t_fine)
        plt.plot(t_fine, u_e, '-') # avoid markers by specifying line type
        legends.append('exact')
        
        plt.legend(legends)
        plt.title('Time step: %g' % dt)
        plt.show()


.. admonition:: Visualization tip

   We use SciTools for
   plotting here, but importing ``matplotlib.pyplot`` as ``plt`` instead
   also works. However, plain use of Matplotlib as done here results in
   curves with different colors, which may be hard to distinguish on
   black-and-white paper. Using SciTools, curves are
   automatically given colors *and* markers, thus making curves easy
   to distinguish on screen with colors and on black-and-white paper.
   The automatic adding of markers is normally a bad idea for a
   very fine mesh since all the markers get cluttered, but SciTools limits
   the number of markers in such cases.
   For the exact solution we use a very fine mesh, but in the code
   above we specify the line type as a solid line (``-``), which means
   no markers and just a color to be automatically determined by
   the backend used for plotting (Matplotlib by default, but
   SciTools gives the opportunity to use other backends
   to produce the plot, e.g., Gnuplot or Grace).
   
   Also note the that the legends
   are based on the class names of the solvers, and in Python the name of
   a the class type (as a string) of an object ``obj`` is obtained by
   ``obj.__class__.__name__``.




.. _decay:odespy:fig1:

.. figure:: decay_odespy1_png.png
   :width: 600

   *Behavior of different schemes for the decay equation*

The runs in Figure :ref:`decay:odespy:fig1`
and other experiments reveal that the 2nd-order Runge-Kutta
method (``RK2``) is unstable for :math:`\Delta t>1` and decays slower than the
Backward Euler scheme for large and moderate :math:`\Delta t` (see :ref:`decay:exer:RK2:Taylor:analysis` for an analysis).  However, for
fine :math:`\Delta t = 0.25` the 2nd-order Runge-Kutta method approaches
the exact solution faster than the Backward Euler scheme.  That is,
the latter scheme does a better job for larger :math:`\Delta t`, while the
higher order scheme is superior for smaller :math:`\Delta t`. This is a
typical trend also for most schemes for ordinary and partial
differential equations.

The 3rd-order Runge-Kutta method (``RK3``) has also artifacts in form
of oscillatory behavior for the larger :math:`\Delta t` values, much
like that of the Crank-Nicolson scheme. For finer :math:`\Delta t`,
the 3rd-order Runge-Kutta method converges quickly to the exact
solution.

The 4th-order Runge-Kutta method (``RK4``) is slightly inferior
to the Backward Euler scheme on the coarsest mesh, but is then
clearly superior to all the other schemes. It is definitely the
method of choice for all the tested schemes.

Remark about using the :math:`\theta`-rule in Odespy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Odespy package assumes that the ODE is written as :math:`u'=f(u,t)` with
an :math:`f` that is possibly nonlinear in :math:`u`. The :math:`\theta`-rule for
:math:`u'=f(u,t)` leads to

.. math::
         u^{n+1} = u^{n} + \Delta t\left(\theta f(u^{n+1}, t_{n+1})
        + (1-\theta) f(u^{n}, t_{n})\right),

which is a *nonlinear equation* in :math:`u^{n+1}`. Odespy's implementation
of the :math:`\theta`-rule (``ThetaRule``) and the specialized Backward Euler
(``BackwardEuler``) and Crank-Nicolson (``CrankNicolson``) schemes
must invoke iterative methods for
solving the nonlinear equation in :math:`u^{n+1}`. This is done even when
:math:`f` is linear in :math:`u`, as in the model problem :math:`u'=-au`, where we can
easily solve for :math:`u^{n+1}` by hand.  Therefore, we need to specify
use of Newton's method to the equations.
(Odespy allows other methods than Newton's to be used, for instance
Picard iteration, but that method is not suitable. The reason is that it
applies the Forward Euler scheme to generate a start value for
the iterations. Forward Euler may give very wrong solutions
for large :math:`\Delta t` values. Newton's method, on the other hand,
is insensitive to the start value in *linear problems*.)

.. _decay:fd2:adaptiveRK:

Example: Adaptive Runge-Kutta methods
-------------------------------------

.. index:: adaptive time stepping

Odespy offers solution methods that can adapt the size of :math:`\Delta t`
with time to match a desired accuracy in the solution. Intuitively,
small time steps will be chosen in areas where the solution is changing
rapidly, while larger time steps can be used where the solution
is slowly varying. Some kind of *error estimator* is used to
adjust the next time step at each time level.

.. index:: ode45

.. index:: Dormand-Prince Runge-Kutta 4-5 method

A very popular adaptive method for solving ODEs is the Dormand-Prince
Runge-Kutta method of order 4 and 5. The 5th-order method is used as a
reference solution and the difference between the 4th- and 5th-order
methods is used as an indicator of the error in the numerical
solution.  The Dormand-Prince method is the default choice in MATLAB's
widely used ``ode45`` routine.

We can easily set up Odespy to use the Dormand-Prince method and
see how it selects the optimal time steps. To this end, we request
only one time step from :math:`t=0` to :math:`t=T` and ask the method to
compute the necessary non-uniform time mesh to meet a certain
error tolerance. The code goes like

.. code-block:: python

        import odespy
        import numpy as np
        import decay_mod
        import sys
        #import matplotlib.pyplot as plt
        import scitools.std as plt
        
        def f(u, t):
            return -a*u
        
        def exact_solution(t):
            return I*np.exp(-a*t)
        
        I = 1; a = 2; T = 5
        tol = float(sys.argv[1])
        solver = odespy.DormandPrince(f, atol=tol, rtol=0.1*tol)
        
        Nt = 1  # just one step - let the scheme find its intermediate points
        t_mesh = np.linspace(0, T, Nt+1)
        t_fine = np.linspace(0, T, 10001)
        
        solver.set_initial_condition(I)
        u, t = solver.solve(t_mesh)
        
        # u and t will only consist of [I, u^Nt] and [0,T]
        # solver.u_all and solver.t_all contains all computed points
        plt.plot(solver.t_all, solver.u_all, 'ko')
        plt.hold('on')
        plt.plot(t_fine, exact_solution(t_fine), 'b-')
        plt.legend(['tol=%.0E' % tol, 'exact'])
        plt.savefig('tmp_odespy_adaptive.png')
        plt.show()

Running four cases with tolerances :math:`10^{-1}`, :math:`10^{-3}`, :math:`10^{-5}`,
and :math:`10^{-7}`, gives the results in Figure :ref:`decay:odespy:fig2`.
Intuitively, one would expect denser points in the beginning of
the decay and larger time steps when the solution flattens out.

.. _decay:odespy:fig2:

.. figure:: decay_DormandPrince_adaptivity.png
   :width: 800

   *Choice of adaptive time mesh by the Dormand-Prince method for different tolerances*

Exercises  (2)
==============

.. --- begin exercise ---

.. _decay:fd2:exer:precision:

Exercise 3: Experiment with precision in tests and the size of :math:`u`
------------------------------------------------------------------------

It is claimed in the section :ref:`decay:MMS` that most numerical methods will
reproduce a linear exact solution to machine precision. Test this
assertion using the test function ``test_linear_solution`` in the
`decay_vc.py <http://tinyurl.com/jvzzcfn/decay/decay_vc.py>`__ program.
Vary the parameter ``c`` from very small, via ``c=1`` to many larger values,
and print out the maximum difference between the numerical solution
and the exact solution. What is the relevant value of the tolerance
in the float comparison in each case?
Filename: ``test_precision.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:fd2:exer:bw2:

Exercise 4: Implement the 2-step backward scheme
------------------------------------------------

Implement the 2-step backward method :eq:`decay:fd2:bw:2step` for the
model :math:`u'(t) = -a(t)u(t) + b(t)`, :math:`u(0)=I`.  Allow the first step to
be computed by either the Backward Euler scheme or the Crank-Nicolson
scheme. Verify the implementation by choosing :math:`a(t)` and :math:`b(t)` such
that the exact solution is linear in :math:`t` (see the section :ref:`decay:MMS`). Show mathematically that a linear solution is indeed a
solution of the discrete equations.

Compute convergence rates (see the section :ref:`decay:convergence:rate`) in
a test case :math:`a=\hbox{const}` and :math:`b=0`, where we easily have an exact
solution, and determine if the choice of a first-order scheme
(Backward Euler) for the first step has any impact on the overall
accuracy of this scheme. The expected error goes like :math:`{\mathcal{O}(\Delta t^2)}`.
Filename: ``decay_backward2step.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:fd2:exer:AB2:

Exercise 5: Implement the 2nd-order Adams-Bashforth scheme
----------------------------------------------------------

Implement the 2nd-order Adams-Bashforth method :eq:`decay:fd2:AB2`
for the decay problem :math:`u'=-a(t)u + b(t)`, :math:`u(0)=I`, :math:`t\in (0, T]`.
Use the Forward Euler method for the first step such that the overall
scheme is explicit. Verify the implementation using an exact
solution that is linear in time.
Analyze the scheme by searching for solutions :math:`u^n=A^n` when :math:`a=\hbox{const}`
and :math:`b=0`. Compare this second-order secheme to the Crank-Nicolson scheme.
Filename: ``decay_AdamsBashforth2.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:fd2:exer:AB3:

Exercise 6: Implement the 3rd-order Adams-Bashforth scheme
----------------------------------------------------------

Implement the 3rd-order Adams-Bashforth method :eq:`decay:fd2:AB3`
for the decay problem :math:`u'=-a(t)u + b(t)`, :math:`u(0)=I`, :math:`t\in (0, T]`.
Since the scheme is explicit, allow it to be started by two steps with
the Forward Euler method.  Investigate experimentally the case where
:math:`b=0` and :math:`a` is a constant: Can we have oscillatory solutions for
large :math:`\Delta t`?
Filename: ``decay_AdamsBashforth3.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:exer:RK2:Taylor:analysis:

Exercise 7: Analyze explicit 2nd-order methods
----------------------------------------------

Show that the schemes :eq:`decay:fd2:RK2:s2` and
:eq:`decay:fd2:Taylor2` are identical in the case :math:`f(u,t)=-a`, where
:math:`a>0` is a constant. Assume that the numerical solution reads
:math:`u^n=A^n` for some unknown amplification factor :math:`A` to be determined.
Find :math:`A` and derive stability criteria. Can the scheme produce
oscillatory solutions of :math:`u'=-au`? Plot the numerical and exact
amplification factor.
Filename: ``decay_RK2_Taylor2.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:fd2:exer:leapfrog1:

Problem 8: Implement and investigate the Leapfrog scheme
--------------------------------------------------------

A Leapfrog scheme
for the ODE :math:`u'(t) = -a(t)u(t) + b(t)` is defined by

.. math::
         \lbrack D_{2t}u = -au+b\rbrack^n{\thinspace .}

A separate method is needed to compute :math:`u^1`. The Forward Euler
scheme is a possible candidate.

.. A possible test case is

.. :math:`u'=-au + b`, :math:`u(0)=0`, where :math:`{u_{\small\mbox{e}}}(t)=b/a + (I - b/a)e^{-at}` if

.. :math:`a` and :math:`b` are constants.

**a)**
Implement the Leapfrog scheme for the model equation.
Plot the solution in the case :math:`a=1`, :math:`b=0`, :math:`I=1`,
:math:`\Delta t = 0.01`, :math:`t\in [0,4]`. Compare with the exact
solution :math:`{u_{\small\mbox{e}}}(t)=e^{-t}`.

**b)**
Show mathematically that a linear solution in :math:`t` fulfills the
Forward Euler scheme for the first step and the Leapfrog scheme
for the subsequent steps. Use this linear solution to verify
the implementation, and automate the verification through a test
function.

.. --- begin hint in exercise ---

**Hint.**
It can be wise to automate the calculations such that it is easy to
redo the calculations for other types of solutions. Here is
a possible ``sympy`` function that takes a symbolic expression ``u``
(implemented as a Python function of ``t``), fits the ``b`` term, and
checks if ``u`` fulfills the discrete equations:

.. code-block:: python

        import sympy as sp
        
        def analyze(u):
            t, dt, a = sp.symbols('t dt a')
        
            print 'Analyzing u_e(t)=%s' % u(t)
            print 'u(0)=%s' % u(t).subs(t, 0)
        
            # Fit source term to the given u(t)
            b = sp.diff(u(t), t) + a*u(t)
            b = sp.simplify(b)
            print 'Source term b:', b
        
            # Residual in discrete equations; Forward Euler step
            R_step1 = (u(t+dt) - u(t))/dt + a*u(t) - b
            R_step1 = sp.simplify(R_step1)
            print 'Residual Forward Euler step:', R_step1
        
            # Residual in discrete equations; Leapfrog steps
            R = (u(t+dt) - u(t-dt))/(2*dt) + a*u(t) - b
            R = sp.simplify(R)
            print 'Residual Leapfrog steps:', R
        
        def u_e(t):
            return c*t + I
        
        analyze(u_e)
        # or short form: analyze(lambda t: c*t + I)

.. --- end hint in exercise ---

**c)**
Show that a second-order polynomial in :math:`t` cannot be a solution of the discrete
equations. However, if a Crank-Nicolson scheme is used for the first
step, a second-order polynomial solves the equations exactly.

**d)**
Create a manufactured solution :math:`u(t)=\sin(t)` for the ODE
:math:`u'=-au+b`.
Compute the convergence rate of the Leapfrog scheme using this
manufactured solution. The expected convergence rate of the
Leapfrog scheme is :math:`{\mathcal{O}(\Delta t^2)}`. Does the use of a
1st-order method for the first step impact the convergence rate?

**e)**
Set up a set of experiments to demonstrate that the Leapfrog scheme
:eq:`decay:fd2:leapfrog` is associated with numerical artifacts
(instabilities). Document the main results from this investigation.

**f)**
Analyze and explain the
instabilities of the Leapfrog scheme :eq:`decay:fd2:leapfrog`:

1. Choose :math:`a=\mbox{const}` and :math:`b=0`. Assume that an exact solution
   of the discrete equations has
   the form :math:`u^n=A^n`, where :math:`A` is an amplification factor to
   be determined. Derive an equation for :math:`A` by inserting :math:`u^n=A^n`
   in the Leapfrog scheme.

2. Compute :math:`A` either by hand and/or with the aid of ``sympy``.
   The polynomial for :math:`A` has two roots, :math:`A_1` and :math:`A_2`. Let
   :math:`u^n` be a linear combination :math:`u^n=C_1A_1^n + C_2A_2^n`.

3. Show that one of the roots is the explanation of the instability.

4. Compare :math:`A` with the exact expression, using a Taylor series approximation.

5. How can :math:`C_1` and :math:`C_2` be determined?

**g)**
Since the original Leapfrog scheme is unconditionally unstable as time
grows, it demands some stabilization.  This can be done by filtering,
where we first find :math:`u^{n+1}` from the original Leapfrog scheme and
then replace :math:`u^{n}` by :math:`u^n + \gamma (u^{n-1} - 2u^n +
u^{n+1})`, where :math:`\gamma` can be taken as 0.6.  Implement the filtered
Leapfrog scheme and check that it can handle tests where the original
Leapfrog scheme is unstable.

Filenames: ``decay_leapfrog.py``, ``decay_leapfrog.pdf``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:fd2:exer:uni:

Problem 9: Make a unified implementation of many schemes
--------------------------------------------------------

Consider the linear ODE problem :math:`u'(t)=-a(t)u(t) + b(t)`, :math:`u(0)=I`.
Explicit schemes for this problem can be written in the general form

.. math::
   :label: decay:analysis:exer:sumcj
        
        u^{n+1} = \sum_{j=0}^m c_ju^{n-j},
        
        

for some choice of :math:`c_0,\ldots,c_m`.
Find expressions for the :math:`c_j` coefficients in case of the
:math:`\theta`-rule, the three-level backward scheme,
the Leapfrog scheme, the 2nd-order Runge-Kutta method,
and the 3rd-order Adams-Bashforth scheme.

Make a class ``ExpDecay`` that implements the
general updating formula :eq:`decay:analysis:exer:sumcj`.
The formula cannot be applied for :math:`n<m`, and for those :math:`n` values, other
schemes must be used. Assume for simplicity that we just
repeat Crank-Nicolson steps until :eq:`decay:analysis:exer:sumcj` can be used.
Use a subclass
to specify the list :math:`c_0,\ldots,c_m` for a particular method, and
implement subclasses for all the mentioned schemes.
Verify the implementation by testing with a linear solution, which should
be exactly reproduced by all methods.
Filename: ``decay_schemes_oo.py``.

.. --- end exercise ---

.. !split

.. _decay:app:

Applications of exponential decay models
========================================

This section presents many mathematical models that all
end up with ODEs of the type :math:`u'=-au+b`.
The applications are taken from biology,
finance, and physics, and cover population growth or decay, compound
interest and inflation, radioactive decay, cooling of objects,
compaction of geological media, pressure
variations in the atmosphere, and air resistance on falling or rising
bodies.

.. _decay:app:scaling:

Scaling  (1)
------------

Real applications of a model :math:`u'=-au+b` will often involve a lot
of parameters in the expressions for :math:`a` and :math:`b`. It can be quite
a challenge to find relevant values of all parameters. In simple
problems, however, it turns out that it is not always necessary
to estimate all parameters because we can lump them into one or
a few *dimensionless* numbers by using a very attractive technique
called scaling. It simply means to stretch the :math:`u` and :math:`t` axis
is the present problem - and suddenly all parameters in the problem
are lumped one parameter if :math:`b\neq 0` and no parameter when :math:`b=0`!

Scaling means that we introduce a new function :math:`\bar u(\bar t)`,
with

.. math::
         \bar u = \frac{u - u_m}{u_c},\quad \bar t = \frac{t}{t_c},

where :math:`u_m` is a characteristic value of :math:`u`, :math:`u_c` is a characteristic
size of the range of :math:`u` values, and :math:`t_c` is a characteristic
size of the range of :math:`t_c` where :math:`u` varies significantly.
Choosing :math:`u_m`, :math:`u_c`, and :math:`t_c` is not always easy and often an art
in complicated problems. We just state one choice first:

.. math::
         u_c = I,\quad u_m = b/a,\quad t_c = 1/a{\thinspace .}

Inserting :math:`u=u_m + u_c\bar u` and :math:`t=t_c\bar t` in the problem
:math:`u'=-au + b`, assuming :math:`a` and :math:`b` are constants, results after some
algebra in the *scaled problem*

.. math::
         \frac{d\bar u}{d\bar t} = -\bar u,\quad \bar u(0)=1 - \beta,

where :math:`\beta` is a dimensionless number

.. math::
         \beta = \frac{b}{Ia}{\thinspace .}

That is, only the special combination of :math:`b/(Ia)` matters, not what
the individual values of :math:`b`, :math:`a`, and :math:`I` are. Moreover, if :math:`b=0`,
the scaled problem is independent of :math:`a` and :math:`I`! In practice this means
that we can perform one numerical simulation of the scaled problem and
recover the solution of any problem for a given :math:`a` and :math:`I` by
stretching the axis in the plot: :math:`u=I\bar u` and :math:`t =\bar t/a`.
For :math:`b\neq 0`, we simulate the scaled problem for a few :math:`\beta` values
and recover the physical solution :math:`u` by translating and stretching the :math:`u`
axis and stretching the :math:`t` axis.

The scaling breaks down if :math:`I=0`. In that case we may choose :math:`u_m=0`,
:math:`u_c=b/a`, and :math:`t_c=1/b`, resulting in a slightly different scaled problem:

.. math::
         \frac{d\bar u}{d\bar t} = 1 -\bar u,\quad \bar u(0)=0{\thinspace .}

As with :math:`b=0`, the case :math:`I=0` has a scaled problem with no physical
parameters!

It is common to drop the bars after scaling and write the scaled
problem as :math:`u'=-u`, :math:`u(0)=1-\beta`, or :math:`u'=1-u`, :math:`u(0)=0`.
Any implementation of the problem :math:`u'=-au+b`, :math:`u(0)=I`, can be
reused for the scaled problem by setting :math:`a=1`, :math:`b=0`, and :math:`I=1-\beta`
in the code, if :math:`I\neq 0`, or one sets
:math:`a=1`, :math:`b=1`, and :math:`I=0` when the physical :math:`I` is zero.
Falling bodies in fluids, as described in the section :ref:`decay:app:drag`,
involves :math:`u'=-au+b` with seven physical parameters. All these vanish
in the scaled version of the problem if we start the motion from rest!

.. _decay:app:pop:

Evolution of a population
-------------------------

.. index:: population dynamics

Let :math:`N` be the number of individuals in a population occupying some
spatial domain.
Despite :math:`N` being an integer in this problem,
we shall compute with :math:`N` as a real number
and view :math:`N(t)` as a continuous function of time.
The basic model assumption is that in a time interval :math:`\Delta t` the number of
newcomers to the populations (newborns) is proportional to
:math:`N`, with proportionality constant :math:`\bar b`. The amount of
newcomers will increase the population and result in
to

.. math::
         N(t+\Delta t) = N(t) + \bar bN(t){\thinspace .}  

It is obvious that a long time interval :math:`\Delta t` will result in
more newcomers and hence a larger :math:`\bar b`. Therefore, we introduce
:math:`b=\bar b/\Delta t`: the number of newcomers per unit time and per
individual. We must then multiply :math:`b` by the length of the time
interval considered and by the population size to get the
total number of new individuals, :math:`b\Delta t N`.

If the number of removals from the population (deaths) is also
proportional to :math:`N`, with proportionality constant :math:`d\Delta t`,
the population evolves according to

.. math::
         N(t+\Delta t) = N(t) + b\Delta t N(t) - d\Delta t N(t){\thinspace .}  

Dividing by :math:`\Delta t` and letting :math:`\Delta t \rightarrow 0`,
we get the ODE

.. math::
        
        N' = (b-d)N,\quad N(0)=N_0{\thinspace .}
        

In a population where the death rate (:math:`d`) is larger than
then newborn rate (:math:`b`), :math:`a>0`, and the population experiences
exponential decay rather than exponential growth.

In some populations there is an immigration of individuals into the
spatial domain. With :math:`I` individuals coming in per time unit,
the equation for the population change becomes

.. math::
         N(t+\Delta t) = N(t) + b\Delta t N(t) - d\Delta t N(t) + \Delta t I{\thinspace .}  

The corresponding ODE reads

.. math::
        
        N' = (b-d)N + I,\quad N(0)=N_0
        {\thinspace .}
        

Some simplification arises if we introduce a fractional measure
of the population: :math:`u=N/N_0` and set :math:`r=b-d`. The ODE problem
now becomes

.. math::
   :label: decay:app:pop:ueq
        
        u' = ru + f,\quad u(0)=1,
        
        

where :math:`f=I/N_0` measures the net immigration per time unit as
the fraction of the initial population. Very often, :math:`r` is approximately
constant, but :math:`f` is usually a function of time.

.. index:: logistic model

The growth rate :math:`r` of a population decreases if the environment
has limited resources. Suppose the environment can sustain at
most :math:`N_{\max}` individuals. We may then assume that the growth rate
approaches zero as :math:`N` approaches :math:`N_{\max}`, i.e., as :math:`u` approaches
:math:`M=N_{\max}/N_0`. The simplest possible evolution of :math:`r` is then a
linear function: :math:`r(t)=r_0(1-u(t)/M)`, where :math:`r_0`
is the initial growth rate when the population is small relative to the
maximum size and there is enough resources. Using this :math:`r(t)` in
:eq:`decay:app:pop:ueq` results in the *logistic model* for the
evolution of a population (assuming for the moment that :math:`f=0`):

.. math::
   :label: decay:app:pop:logistic
        
        u' = r_0(1-u/M)u,\quad u(0)=1
        {\thinspace .}
        
        

Initially, :math:`u` will grow at rate :math:`r_0`, but the growth will decay
as :math:`u` approaches :math:`M`, and then there is no more change in :math:`u`, causing
:math:`u\rightarrow M` as :math:`t\rightarrow\infty`.
Note that the logistic equation :math:`u'=r_0(1-u/M)u` is *nonlinear* because
of the quadratic term :math:`-u^2r_0/M`.

.. _decay:app:interest:

Compound interest and inflation
-------------------------------

Say the annual interest rate is :math:`r` percent and that the bank
adds the interest once a year to your investment.
If :math:`u^n` is the investment in year :math:`n`, the investment in year :math:`u^{n+1}`
grows to

.. math::
         u^{n+1} = u^n + \frac{r}{100}u^n
        {\thinspace .}  

In reality, the interest rate is added every day. We therefore introduce
a parameter :math:`m` for the number of periods per year when the interest
is added. If :math:`n` counts the periods, we have the fundamental model
for compound interest:

.. math::
   :label: decay:app:interest:eq1
        
        u^{n+1} = u^n + \frac{r}{100 m}u^n
        {\thinspace .}
        
        

This model is a *difference equation*, but it can be transformed to a
continuous differential equation through a limit process.
The first step is to derive a formula for the growth of the investment
over a time :math:`t`.
Starting with an investment :math:`u^0`, and assuming that :math:`r` is constant in time,
we get

.. math::
        
        u^{n+1} &= \left(1 + \frac{r}{100 m}\right)u^{n}\\ 
        &= \left(1 + \frac{r}{100 m}\right)^2u^{n-1}\\ 
        &\ \ \vdots\\ 
        &= \left(1 +\frac{r}{100 m}\right)^{n+1}u^{0}
        

Introducing time :math:`t`, which here is a real-numbered counter for years,
we have that :math:`n=mt`, so we can write

.. math::
         u^{mt} = \left(1 + \frac{r}{100 m}\right)^{mt} u^0{\thinspace .}  

The second step is to assume *continuous compounding*, meaning that the
interest is added continuously. This implies :math:`m\rightarrow\infty`, and
in the limit one gets the formula

.. math::
        
        u(t) = u_0e^{rt/100},
        

which is nothing but the solution of the ODE problem

.. math::
   :label: decay:app:interest:eq2
        
        u' = \frac{r}{100}u,\quad u(0)=u_0
        {\thinspace .}
        
        

This is then taken as the ODE model for compound interest if :math:`r>0`.
However, the reasoning applies equally well to inflation, which is
just the case :math:`r<0`. One may also take the :math:`r` in :eq:`decay:app:interest:eq2`
as the net growth of an investemt, where :math:`r` takes both compound interest
and inflation into account. Note that for real applications we must
use a time-dependent :math:`r` in :eq:`decay:app:interest:eq2`.

Introducing :math:`a=\frac{r}{100}`, continuous inflation of an initial
fortune :math:`I` is then
a process exhibiting exponential decay according to

.. math::
         u' = -au,\quad u(0)=I{\thinspace .}  

.. _decay:app:nuclear:

Radioactive Decay
-----------------

.. index:: radioactive decay

An atomic nucleus of an unstable atom may lose energy by emitting
ionizing particles and thereby be transformed to a nucleus with a
different number of protons and neutrons.  This process is known as
`radioactive decay <http://en.wikipedia.org/wiki/Radioactive_decay>`__.
Actually, the process is stochastic when viewed for a single atom,
because it is impossible to predict exactly when a particular atom
emits a particle. Nevertheless, with a large number of atoms, :math:`N`, one
may view the process as deterministic and compute the mean behavior of
the decay. Below we reason intuitively about an ODE for the mean
behavior. Thereafter, we show mathematically that a detailed stochastic model
for single atoms leads the same mean behavior.

Deterministic model
~~~~~~~~~~~~~~~~~~~

Suppose at time :math:`t`, the number of the original atom type is :math:`N(t)`.
A basic model assumption is that the transformation of the atoms of the original
type in a small time interval :math:`\Delta t` is proportional to
:math:`N`, so that

.. math::
         N(t+\Delta t) = N(t) - a\Delta t N(t),

where :math:`a>0` is a constant. Introducing :math:`u=N(t)/N(0)`, dividing by
:math:`\Delta t` and letting :math:`\Delta t\rightarrow 0` gives the
following ODE:

.. math::
        
        u' = -au,\quad u(0)=1
        {\thinspace .}
        

The parameter :math:`a` can for a given nucleus be expressed through the
*half-life* :math:`t_{1/2}`, which is the time taken for the decay to reduce the
initial amount by one half, i.e., :math:`u(t_{1/2}) = 0.5`.
With :math:`u(t)=e^{-at}`, we get :math:`t_{1/2}=a^{-1}\ln 2` or :math:`a=\ln 2/t_{1/2}`.

.. `<http://en.wikipedia.org/wiki/Exponential_decay>`_

Stochastic model
~~~~~~~~~~~~~~~~

We have originally :math:`N_0` atoms. Each atom may have decayed or
survived at a particular time :math:`t`. We want to count how many original
atoms that are left, i.e., how many atoms that have survived.
The survival of a single atom at time :math:`t` is a random event. Since there
are only two outcomes, survival or decay, we have a
`Bernoulli trial <http://en.wikipedia.org/wiki/Bernoulli_trial>`__.
Let :math:`p` be the
probability of survival (implying that the probability of decay
is :math:`1-p`). If each atom survives independently of
the others, and the probability of survival is the same for every
atom, we have :math:`N_0` statistically Bernoulli trials, known as
a *binomial experiment* from probability theory.
The probability :math:`P(N)` that :math:`N` out
of the :math:`N_0` atoms have survived at time :math:`t` is then given by the
famous *binomial distribution*

.. math::
         P(N) = \frac{N_0!}{N! (N_0-N)!}p^N (1-p)^{N_0-N}{\thinspace .} 

The mean (or expected) value :math:`{\hbox{E}\lbrack P \rbrack}` of :math:`P(N)` is known to be :math:`N_0p`.

It remains to estimate :math:`p`. Let the interval :math:`[0,t]` be divided into :math:`m`
small subintervals of length :math:`\Delta t`. We make the assumption that
the probability of decay of a single atom in an interval of length :math:`\Delta t`
is :math:`\tilde p`, and that this probability is proportional to :math:`\Delta t`:
:math:`\tilde p = \lambda\Delta t` (it sounds natural that the probability
of decay increases with :math:`\Delta t`). The corresponding probability of survival
is :math:`1-\lambda\Delta t`. Believing that :math:`\lambda` is independent
of time, we have, for each interval of length :math:`\Delta t`,
a Bernoulli trial: the atom either survives or
decays in that interval. Now, :math:`p` should be the probability that the atom
survives in all the intervals, i.e., that we have :math:`m` successful
Bernoulli trials in a row and therefore

.. math::
         p = (1-\lambda\Delta t)^m{\thinspace .}

The expected number of atoms of the original type at time :math:`t` is

.. math::
        
        {\hbox{E}\lbrack P \rbrack} = N_0p = N_0(1-\lambda\Delta t)^m,\quad m=t/\Delta t{\thinspace .}
        

To see the relation between the two types of Bernoulli trials and the
ODE above, we go to the limit :math:`\Delta t\rightarrow t`, :math:`m\rightarrow\infty`.
One can show that

.. math::
         p = \lim_{m\rightarrow\infty} (1-\lambda\Delta t)^m
        = \lim_{m\rightarrow\infty} \left(1-\lambda\frac{t}{m}\right)^m = e^{-\lambda t}
        

This is the famous exponential waiting time (or arrival time) distribution for a
Poisson process in probability theory (obtained here, as often done, as
the limit of a binomial experiment). The probability of decay,
:math:`1-e^{-\lambda t}`, follows an `exponential distribution <http://en.wikipedia.org/wiki/Exponential_distribution>`__.
The limit means that :math:`m` is very
large, hence :math:`\Delta t` is very small, and :math:`\tilde p=\lambda\Delta t`
is very small since the intensity of the events, :math:`\lambda`, is assumed
finite. This situation corresponds to a very small probability
that an atom will decay in a very short time interval, which is a
reasonable model.
The same model occurs in lots of different applications, e.g.,
when waiting for a taxi, or when finding defects along a rope.

Relation between stochastic and deterministic models
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With :math:`p=e^{-\lambda t}` we get the expected number of original atoms
at :math:`t` as :math:`N_0p=N_0e^{-\lambda t}`, which is exactly the solution of
the ODE model :math:`N'=-\lambda N`. This gives also an interpretation
of :math:`a` via :math:`\lambda` or vice versa. Our important finding here
is that the ODE model
captures the mean behavior of the underlying stochastic model. This
is, however, not always the common relation between microscopic stochastic
models and macroscopic "averaged" models.

Also of interest is to see that a Forward Euler discretization of
:math:`N'=-\lambda N`, :math:`N(0)=N_0`, gives :math:`N^m = N_0(1-\lambda\Delta t)^m`
at time :math:`t_m=m\Delta t`, which is exactly the
expected value of the stochastic experiment with :math:`N_0` atoms
and :math:`m` small intervals of length :math:`\Delta t`, where each atom can
decay with probability :math:`\lambda\Delta t` in an interval.

A fundamental question is how accurate the ODE model is. The underlying
stochastic model fluctuates around its expected value. A measure
of the fluctuations is the standard deviation of the binomial experiment with
:math:`N_0` atoms, which can be shown to be :math:`{\hbox{Std}\lbrack P \rbrack}=\sqrt{N_0p(1-p)}`. Compared
to the size of the expectation, we get
the normalized standard deviation

.. math::
         \frac{\sqrt{{\hbox{Var}\lbrack P \rbrack}}}{{\hbox{E}\lbrack P \rbrack}} = N_0^{-1/2}\sqrt{p^{-1}-1}
        = N_0^{-1/2}\sqrt{(1-e^{-\lambda t})^{-1}-1}\approx
        (N_0\lambda t)^{-1/2},
        

showing that the normalized fluctuations are very small if :math:`N_0` is
very large, which is usually the case.

.. _decay:app:Newton:cooling:

Newton's law of cooling
-----------------------

.. `<http://web.bham.ac.uk/winterhs/Newton.htm>`_

.. I. Newton, Scala Graduum Caloris, Philosophical Transactions of the Royal Society of London, 1701

.. explanation: `<http://www.madsci.org/posts/archives/2000-11/973522810.Ph.r.html>`_

When a body at some temperature is placed in a cooling environment,
experience shows that the temperature falls rapidly in the beginning,
and then the changes in temperature levels off until the body's
temperature equals that of the surroundings. Newton carried out some
experiments on cooling hot iron and found that the temperature
evolved as a "geometric progression at times in arithmetic progression",
meaning that the temperature decayed exponentially.
Later, this result was formulated as a differential equation:
the rate of change of the temperature in a body is proportional to
the temperature difference between the body and its surroundings.
This statement is known as *Newton's law of cooling*, which
can be mathematically expressed as

.. math::
   :label: decay:Newton:cooling
        
        {dT\over dt} = -k(T-T_s),
        
        

where :math:`T` is the temperature of the body, :math:`T_s` is the temperature
of the surroundings, :math:`t` is time, and :math:`k` is a positive constant.
Equation :eq:`decay:Newton:cooling` is primarily viewed as an
empirical law, valid when heat is efficiently convected away
from the surface of the body by a flowing fluid such as air
at constant temperature :math:`T_s`.
The *heat transfer coefficient* :math:`k` reflects the transfer of
heat from the body to
the surroundings and must be determined from physical experiments.

We must obviously have an initial condition :math:`T(0)=T_0` in addition
to the cooling law :eq:`decay:Newton:cooling`.

.. _decay:app:atm:

Decay of atmospheric pressure with altitude
-------------------------------------------

.. The Barometric Formula

.. `<http://en.wikipedia.org/wiki/Barometric_formula>`_

Vertical equilibrium of air in the atmosphere is governed by
the equation

.. math::
   :label: decay:app:atm:dpdz
        
        \frac{dp}{dz} = -\varrho g
        {\thinspace .}
        
        

Here, :math:`p(z)` is the air pressure, :math:`\varrho` is the density of
air, and :math:`g=9.807\hbox{ m/s}^2` is a standard value of
the acceleration of gravity.
(Equation :eq:`decay:app:atm:dpdz` follows directly from the general
Navier-Stokes equations for fluid motion, with
the assumption that the air does not move.)

The pressure is related to density and temperature through the ideal gas law

.. math::
   :label: decay:app:atm:rho
        
        \varrho = \frac{Mp}{R^*T}, 
        

where :math:`M` is the molar mass of the Earth's air (0.029 kg/mol),
:math:`R^*` is the universal
gas constant (:math:`8.314` Nm/(mol K)), and :math:`T` is the temperature.
All variables :math:`p`, :math:`\varrho`, and :math:`T` vary with the height :math:`z`.
Inserting :eq:`decay:app:atm:rho` in :eq:`decay:app:atm:dpdz` results
in an ODE with a variable coefficient:

.. math::
   :label: decay:app:atm:ode
        
        \frac{dp}{dz} = -\frac{Mg}{R^*T(z)} p
        
        \thinspace  .
        

Multiple atmospheric layers
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The atmosphere can be approximately modeled by seven layers.
In each layer, :eq:`decay:app:atm:ode` is applied with
a linear temperature of the form

.. math::
         T(z) = \bar T_i + L_i(z-h_i),

where :math:`z=h_i` denotes the bottom of layer number :math:`i`,
having temperature :math:`\bar T_i`,
and :math:`L_i` is a constant in layer number :math:`i`. The table below
lists :math:`h_i` (m), :math:`\bar T_i` (K), and :math:`L_i` (K/m) for the layers
:math:`i=0,\ldots,6`.

================  ================  ================  ================  
   :math:`i`        :math:`h_i`     :math:`\bar T_i`    :math:`L_i`     
================  ================  ================  ================  
0                                0               288           -0.0065  
1                           11,000               216               0.0  
2                           20,000               216             0.001  
3                           32,000               228            0.0028  
4                           47,000               270               0.0  
5                           51,000               270           -0.0028  
6                           71,000               214            -0.002  
================  ================  ================  ================  

For implementation it might be convenient to write :eq:`decay:app:atm:ode`
on the form

.. math::
        
        \frac{dp}{dz} = -\frac{Mg}{R^*(\bar T(z) + L(z)(z-h(z)))} p,
        

where :math:`\bar T(z)`, :math:`L(z)`, and :math:`h(z)` are piecewise constant
functions with values given in the table.
The value of the pressure at the sea level :math:`z=0`, :math:`p_0=p(0)`, is :math:`101325` Pa.

Simplification: :math:`L=0`
~~~~~~~~~~~~~~~~~~~~~~~~~~~

One commonly used simplification is to assume that the temperature is
constant within each layer. This means that :math:`L=0`.

Simplification: one-layer model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Another commonly used approximation is to work with one layer instead of
seven. This `one-layer model <http://en.wikipedia.org/wiki/Density_of_air>`__
is based on :math:`T(z)=T_0 - Lz`, with
sea level standard temperature :math:`T_0=288` K and
temperature lapse rate :math:`L=0.0065` K/m.

.. _decay:app:sediment:

Compaction of sediments
-----------------------

Sediments, originally made from materials like sand and mud, get
compacted through geological time by the weight of new material that
is deposited on the sea bottom. The porosity :math:`\phi` of the sediments
tells how much void (fluid) space there is between the sand and
mud grains. The porosity reduces with depth because the weight of
the sediments above and causes the void space to shrink and thereby
increase the compaction.

A typical assumption is that the change in :math:`\phi` at some depth :math:`z`
is negatively proportional to :math:`\phi`. This assumption leads to
the differential equation problem

.. math::
   :label: decay:app:sediment:phi:eq
        
        \frac{d\phi}{dz} = -c\phi,\quad \phi(0)=\phi_0,
        
        

where the :math:`z` axis points downwards, :math:`z=0` is the surface with known
porosity, and :math:`c>0` is a constant.

The upper part of the Earth's crust consists of many geological layers
stacked on top of each other, as indicated in Figure
:ref:`decay:app:sediment:fig:layers`.  The model
:eq:`decay:app:sediment:phi:eq` can be applied for each layer. In
layer number :math:`i`, we have the unknown porosity function :math:`\phi_i(z)`
fulfilling :math:`\phi_i'(z)=-c_iz`, since the constant :math:`c` in the model
:eq:`decay:app:sediment:phi:eq` depends on the type of sediment in
the layer. From the figure we see that new layers of sediments are
deposited on top of older ones as time progresses. The compaction,
as measured by :math:`\phi`, is
rapid in the beginning and then decreases (exponentially) with depth
in each layer.

.. _decay:app:sediment:fig:layers:

.. figure:: Compaction_of_Sediment.png
   :width: 600

   *Illustration of the compaction of geological layers (with different colors) through time*

When we drill a well at present time through the right-most column of
sediments in Figure :ref:`decay:app:sediment:fig:layers`, we can measure
the thickness of the sediment in (say) the bottom layer. Let :math:`L_1` be
this thickness.  Assuming that the volume of sediment remains constant
through time, we have that the initial volume, :math:`\int_0^{L_{1,0}}
\phi_1 dz`, must equal the volume seen today,
:math:`\int_{\ell-L_1}^{\ell}\phi_1 dz`, where :math:`\ell` is the depth of the
bottom of the sediment in the present day configuration.  After having
solved for :math:`\phi_1` as a function of :math:`z`, we can then find the
original thickness :math:`L_{1,0}` of the sediment from the equation

.. math::
         \int_0^{L_{1,0}} \phi_1 dz = \int_{\ell-L_1}^{\ell}\phi_1 dz {\thinspace .} 

In hydrocarbon exploration it is important to know :math:`L_{1,0}` and the
compaction history of the various layers of sediments.

.. _decay:app:drag:

Vertical motion of a body in a viscous fluid
--------------------------------------------

A body moving vertically through a fluid (liquid or gas) is subject to
three different types of forces: the gravity force, `the drag force <http://en.wikipedia.org/wiki/Drag_(physics)>`__,
and the buoyancy force.

Overview of forces
~~~~~~~~~~~~~~~~~~

The gravity force is :math:`F_g= -mg`, where :math:`m` is the mass of the body and
:math:`g` is the acceleration of gravity.
The uplift or buoyancy force ("Archimedes force") is :math:`F_b = \varrho gV`,
where :math:`\varrho` is the density of the fluid and
:math:`V` is the volume of the body.
Forces and other quantities are taken as positive in the upward
direction.

The drag force is of two types, depending on the Reynolds number

.. math::
        
        \hbox{Re} = \frac{\varrho d|v|}{\mu},
        

where :math:`d` is the diameter of the body in
the direction perpendicular to the flow, :math:`v` is the velocity of the
body, and :math:`\mu` is the dynamic viscosity of the fluid.
When :math:`\hbox{Re} < 1`, the drag force is fairly well modeled by
the so-called Stokes' drag,
which for a spherical body of diameter :math:`d` reads

.. math::
        
        F_d^{(S)} = - 3\pi d\mu v
        {\thinspace .}
        

For large Re, typically :math:`\hbox{Re} > 10^3`, the drag force is quadratic
in the velocity:

.. math::
        
        F_d^{(q)} = -{1\over2}C_D\varrho A|v|v,
        

where :math:`C_D` is a dimensionless drag coefficient depending on the body's shape,
and :math:`A` is the cross-sectional area as
produced by a cut plane, perpendicular to the motion, through the thickest
part of the body. The superscripts :math:`\,{}^q` and :math:`\,{}^S` in
:math:`F_d^{(S)}` and :math:`F_d^{(q)}` indicate Stokes drag and quadratic drag,
respectively.

Equation of motion
~~~~~~~~~~~~~~~~~~

All the mentioned forces act in the vertical direction.
Newton's second law of motion applied to the body says that the sum of
these forces must equal the mass of the body times its acceleration
:math:`a` in the vertical direction.

.. math::
         ma = F_g + F_d^{(S)} + F_b {\thinspace .}

Here we have chosen to model the fluid resistance by the Stokes drag.
Inserting the expressions for the forces yields

.. math::
          ma = -mg - 3\pi d\mu v + \varrho gV
        {\thinspace .}
        

The unknowns here are :math:`v` and :math:`a`, i.e., we have two unknowns but only
one equation. From kinematics in physics we know that
the acceleration is the time derivative of the velocity: :math:`a = dv/dt`.
This is our second equation.
We can easily eliminate :math:`a` and get a single differential equation for :math:`v`:

.. math::
         m{dv\over dt} = -mg - 3\pi d\mu v + \varrho gV
        {\thinspace .}
        

A small rewrite of this equation is handy: We express :math:`m` as :math:`\varrho_bV`,
where :math:`\varrho_b` is the density of the body, and we divide by
the mass to get

.. math::
   :label: decay:app:fallingbody:model:S
        
        v'(t) = - \frac{3\pi d\mu}{\varrho_b V} v + g\left(\frac{\varrho}{\varrho_b} -1\right)
        
        {\thinspace .}
        

We may introduce the constants

.. math::
        
        a = \frac{3\pi d\mu}{\varrho_b V},\quad
        b = g\left(\frac{\varrho}{\varrho_b} -1\right),
        

so that the structure of the differential equation becomes obvious:

.. math::
   :label: decay:app:fallingbody:gmodel:S
        
        v'(t) = -av(t) + b
        
        {\thinspace .}
        

The corresponding initial condition is :math:`v(0)=v_0` for some prescribed
starting velocity :math:`v_0`.

This derivation can be repeated with the quadratic drag force
:math:`F_d^{(q)}`, leading to the result

.. math::
   :label: decay:app:fallingbody:model:q
        
        v'(t) =
        -{1\over2}C_D{\varrho A\over\varrho_b V}|v|v +
        g\left({\varrho\over\varrho_b} - 1\right)
        {\thinspace .}
        
        

Defining

.. math::
        
        a = {1\over2}C_D{\varrho A\over\varrho_b V},
        

and :math:`b` as above, we can write :eq:`decay:app:fallingbody:model:q` as

.. math::
   :label: decay:app:fallingbody:gmodel:q
        
        v'(t) = -a|v|v + b
        {\thinspace .}
        
        

.. index:: terminal velocity

Terminal velocity
~~~~~~~~~~~~~~~~~

An interesting aspect of :eq:`decay:app:fallingbody:gmodel:S` and
:eq:`decay:app:fallingbody:gmodel:q` is whether :math:`v` will approach
a final constant value,
the so-called *terminal velocity* :math:`v_T`, as :math:`t\rightarrow\infty`.
A constant :math:`v` means that
:math:`v'(t)\rightarrow 0` as :math:`t\rightarrow\infty` and therefore
the terminal velocity :math:`v_T` solves

.. math::
        0 = -av_T + b

and

.. math::
         0 = -a|v_T|v_T + b
        {\thinspace .}
        

The former equation implies :math:`v_T = b/a`, while the latter has solutions
:math:`v_T =-\sqrt{|b|/a}` for a falling body (:math:`v_T<0`) and
:math:`v_T = \sqrt{b/a}` for a rising body (:math:`v_T>0`).

A Crank-Nicolson scheme
~~~~~~~~~~~~~~~~~~~~~~~

Both governing equations, the Stokes' drag model
:eq:`decay:app:fallingbody:gmodel:S` and the quadratic drag model
:eq:`decay:app:fallingbody:gmodel:q`, can be readily solved
by the Forward Euler scheme. For higher accuracy one can use
the Crank-Nicolson method, but a straightforward application
this method results
a nonlinear equation in the new unknown value :math:`v^{n+1}` when applied to
:eq:`decay:app:fallingbody:gmodel:q`:

.. math::
   :label: decay:app:fallingbody:gmodel:CN
        
        \frac{v^{n+1}-v^n}{\Delta t}
        = -a\frac{1}{2}(|v^{n+1}|v^{n+1} + |v^n|v^n) + b
        
        {\thinspace .}
        

However, instead of approximating the term :math:`-|v|v` by an arithmetic
average, we can use a *geometric mean*:

.. index:: geometric mean

.. index::
   single: averaging; geometric

.. math::
        
        (|v|v)^{n+\frac{1}{2}} \approx |v^n|v^{n+1}
        {\thinspace .}
        

The error is of second order in :math:`\Delta t`, just as for the arithmetic
average and the centered finite difference approximation in
:eq:`decay:app:fallingbody:gmodel:CN`. With this approximation trick,
the discrete equation

.. math::
        
        \frac{v^{n+1}-v^n}{\Delta t} = - a|v^{n}|v^{n+1} + b
        

becomes a *linear* equation in :math:`v^{n+1}`, and we can
therefore easily solve for :math:`v^{n+1}`:

.. math::
   :label: decay:app:fallingbody:gmodel:q:CN
        
        v^{n+1} = \frac{v_n + \Delta t b^{n+\frac{1}{2}}}{1 + \Delta t a^{n+\frac{1}{2}}|v^{n}|}{\thinspace .}
        
        

Physical data
~~~~~~~~~~~~~

Suitable values of :math:`\mu` are :math:`1.8\cdot 10^{-5}\hbox{ Pa}\, \hbox{s}` for air
and :math:`8.9\cdot 10^{-4}\hbox{ Pa}\, \hbox{s}` for water.
Densities can be taken as :math:`1.2\hbox{ kg/m}^3` for air and as
:math:`1.0\cdot 10^3\hbox{ kg/m}^3` for water. For considerable vertical
displacement in the atmosphere one should take into account that
the density of air varies with the altitude, see the section :ref:`decay:app:atm`.
One possible density variation arises from the one-layer model
in the mentioned section.

Any density variation makes :math:`b` time dependent and we need
:math:`b^{n+\frac{1}{2}}` in :eq:`decay:app:fallingbody:gmodel:q:CN`.
To compute the density that enters
:math:`b^{n+\frac{1}{2}}` we must also compute the vertical
position :math:`z(t)` of the body. Since :math:`v=dz/dt`, we can use a centered
difference approximation:

.. math::
         \frac{z^{n+\frac{1}{2}} - z^{n-\frac{1}{2}}}{\Delta t} = v^n
        \quad\Rightarrow\quad z^{n+\frac{1}{2}} = z^{n-\frac{1}{2}}+\Delta t\, v^n{\thinspace .}

This :math:`z^{n+\frac{1}{2}}` is used in the expression for :math:`b`
to compute :math:`\varrho(z^{n+\frac{1}{2}})` and then :math:`b^{n+\frac{1}{2}}`.

The `drag coefficient <http://en.wikipedia.org/wiki/Drag_coefficient>`__ :math:`C_D` depends heavily
on the shape of the body.  Some values are: 0.45 for a sphere, 0.42
for a semi-sphere, 1.05 for a cube, 0.82 for a long cylinder (when the
center axis is in the vertical direction), 0.75 for a rocket,
1.0-1.3 for a man in upright position, 1.3 for a flat plate perpendicular
to the flow, and
0.04 for a streamlined, droplet-like body.

Verification
~~~~~~~~~~~~

To verify the program, one may assume a heavy body in air such that the :math:`F_b`
force can be neglected, and further assume a small velocity such that the
air resistance :math:`F_d` can also be neglected. This can be obtained by
setting :math:`\mu` and :math:`\varrho` to zero. The motion then leads to
the velocity
:math:`v(t)=v_0 - gt`, which is linear in :math:`t` and therefore should be
reproduced to machine precision (say tolerance :math:`10^{-15}`) by any
implementation based on the Crank-Nicolson or Forward Euler schemes.

Another verification, but not as powerful as the one above,
can be based on computing the terminal velocity and comparing with
the exact expressions.
The advantage of this verification is that we can also
the test situation :math:`\varrho\neq 0`.

As always, the method of manufactured solutions can be applied to
test the implementation of all terms in the governing equation, but
the solution then has no physical relevance in general.

.. index:: scaling

Scaling  (2)
~~~~~~~~~~~~

Applying scaling, as described in the section :ref:`decay:app:scaling`,
will for the linear case reduce the need to estimate values for
seven parameters down to choosing one value of a single dimensionless parameter

.. math::
         \beta = \frac{\varrho_b gV\left(\frac{\varrho}{\varrho_b} -1\right)}{3\pi d\mu I},

provided :math:`I\neq 0`. If the motion starts from rest, :math:`I=0`, the scaled
problem :math:`\bar u'=1-\bar u`, :math:`\bar u(0)=0`, has no need for
estimating physical parameters.
This means that there is a single universal solution to the problem
of a falling body starting from rest:
:math:`\bar u(t) = 1 - e^{-\bar t}`. All real
physical cases correspond to stretching the :math:`\bar t` axis and the :math:`\bar u`
axis in this dimensionless solution. More precisely, the physical velocity
:math:`u(t)` is related to the dimensionless velocity :math:`\bar u(\bar t)` through

.. math::
         u = \frac{\varrho_bgV\left(\frac{\varrho}{\varrho_b} -1\right)}{3\pi d\mu}\bar u(t/(g(\varrho/\varrho_b -1))){\thinspace .}

.. _decay:app:diffusion:Fourier:

Decay ODEs from solving a PDE by Fourier expansions
---------------------------------------------------

.. Maybe move to diffusion part? Makes sense there too, or refer...or

.. repeat, or make one exer with two k's and then generalize in diffusion

Suppose we have a partial differential equation

.. math::
         \frac{\partial u}{\partial t} = \alpha\frac{\partial^2u}{\partial x^2}
        + f(x,t),
        

with boundary conditions :math:`u(0,t)=u(L,t)=0` and initial condition
:math:`u(x,0)=I(x)`. One may express the solution as

.. math::
         u(x,t) = \sum_{k=1}^m A_k(t)e^{ikx\pi/L},

for appropriate unknown functions :math:`A_k`, :math:`k=1,\ldots,m`.
We use the complex exponential :math:`e^{ikx\pi/L}` for easy algebra, but
the physical :math:`u` is taken as the real part of any complex expression.
Note that the expansion in terms of :math:`e^{ikx\pi/L}` is compatible with
the boundary conditions: all functions :math:`e^{ikx\pi/L}` vanish for
:math:`x=0` and :math:`x=L`. Suppose we can express :math:`I(x)` as

.. math::
         I(x) = \sum_{k=1}^m I_ke^{ikx\pi/L}
        {\thinspace .}
        

Such an expansion can be computed by well-known Fourier expansion techniques,
but the details are not important here.
Also, suppose we can express the given :math:`f(x,t)` as

.. math::
         f(x,t) = \sum_{k=1}^m b_k(t)e^{ikx\pi/L}
        {\thinspace .}
        

Inserting the expansions for :math:`u`
and :math:`f` in the differential equations demands that all terms corresponding
to a given :math:`k` must be equal. The calculations results in the follow
system of ODEs:

.. math::
        
        A_k'(t) = -\alpha\frac{k^2\pi^2}{L^2} + b_k(t),\quad k=1,\ldots,m
        {\thinspace .}
        

From the initial condition

.. math::
         u(x,0)=\sum_k A_k(0)e^{ikx\pi/L}=I(x)=\sum_k I_k e^{(ikx\pi/L)},

it follows that :math:`A_k(0)=I_k`, :math:`k=1,\ldots,m`. We then have :math:`m`
equations of the form :math:`A_k'=-a A_k +b`, :math:`A_k(0)=I_k`, for
appropriate definitions of :math:`a` and :math:`b`. These ODE problems
independent each other such that we can solve one problem
at a time. The outline technique is a quite common approach for solving
partial differential equations.

**Remark.**
Since :math:`a_k` depends on :math:`k` and the stability of
the Forward Euler scheme demands :math:`a_k\Delta t \leq 1`, we get that
:math:`\Delta t \leq \alpha^{-1}L^2\pi^{-2} k^{-2}`. Usually, quite large
:math:`k` values are needed to accurately represent the given functions
:math:`I` and :math:`f` and then :math:`\Delta t` needs to be very small for these large
values of :math:`k`.
Therefore, the Crank-Nicolson and Backward Euler schemes, which
allow larger :math:`\Delta t` without any growth in the solutions, are
more popular choices when creating time-stepping algorithms for
partial differential equations of the type considered in this
example.

Exercises  (3)
==============

.. --- begin exercise ---

.. _decay:app:exer:cooling:schemes:

Exercise 10: Derive schemes for Newton's law of cooling
-------------------------------------------------------

Show in detail how we can apply the ideas of the Forward Euler,
Backward Euler, Crank-Nicolson, and :math:`\theta`-rule
discretizations to derive explicit
computational formulas for new temperature values in Newton's law of
cooling (see the section :ref:`decay:app:Newton:cooling`):

.. math::
   :label: decay:Newton:cooling
        
        {dT\over dt} = -k(T-T_s),\quad T(0)=T_0{\thinspace .}
        
        

Here, :math:`T` is the temperature of the body, :math:`T_s` is the temperature
of the surroundings, :math:`t` is time, :math:`k` is the heat transfer
coefficient, and :math:`T_0` is the initial temperature of the body.
Filename: ``schemes_cooling.pdf``.

.. thinking: operate directly on the T equation

.. Can introduce :math:`u=T-T_s`, or much better for illustration of the

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:cooling:py:

Exercise 11: Implement schemes for Newton's law of cooling
----------------------------------------------------------

Formulate a :math:`\theta`-rule for the three schemes in :ref:`decay:app:exer:cooling:schemes` such that you can get the three
schemes from a single formula by varying the :math:`\theta` parameter.
Implement the method in a function ``cooling(T0, k, T_s,
t_end, dt, theta=0.5)``, where ``T0`` is the initial temperature, ``k`` is
the heat transfer coefficient, ``T_s`` is the temperature of the
surroundings, ``t_end`` is the end time of the simulation, ``dt`` is the
time step, and ``theta`` corresponds to :math:`\theta`.  The ``cooling``
function should return the temperature as an array ``T`` of values at
the mesh points and the time mesh ``t``.  Construct verification
examples to check that the implementation works.

.. --- begin hint in exercise ---

**Hint.**
For verification, try to find an exact solution of the
discrete equations. A trick is to introduce :math:`u=T-T_s`, observe
that :math:`u^{n}=(T_0-T_s)A^n` for some amplification factor :math:`A`,
and then express this formula in terms of :math:`T^n`.

.. --- end hint in exercise ---

Filename: ``cooling.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:cooling:murder:

Exercise 12: Find time of murder from body temperature
------------------------------------------------------

.. `<http://www.biology.arizona.edu/BioMath/tutorials/Applications/Cooling.html>`_

A detective measures the temperature of a dead body to be 26.7 C at 2
pm. One hour later the temperature is 25.8 C. The question is when
death occurred.

Assume that Newton's law of cooling :eq:`decay:Newton:cooling` is an
appropriate mathematical model for the evolution of the temperature in
the body.  First, determine :math:`k` in :eq:`decay:Newton:cooling` by
formulating a Forward Euler approximation with one time steep from
time 2 am to time 3 am, where knowing the two temperatures allows for
finding :math:`k`. Assume the temperature in the air to be 20 C. Thereafter,
simulate the temperature evolution from the time of murder, taken as
:math:`t=0`, when :math:`T=37\hbox{ C}`, until the temperature reaches 25.8 C. The
corresponding time allows for answering when death occurred.
Filename: ``detective.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:cooling:osc:

Exercise 13: Simulate an oscillating cooling process
----------------------------------------------------

The surrounding temperature :math:`T_s` in Newton's law of cooling
:eq:`decay:Newton:cooling` may vary in time. Assume that the
variations are periodic with period :math:`P` and amplitude :math:`a` around
a constant mean temperature :math:`T_m`:

.. math::
        
        T_s(t) = T_m + a\sin\left(\frac{2\pi}{P}t\right)
        {\thinspace .}
        

Simulate a process with the following data: :math:`k=20 \hbox{ min}^{-1}`,
:math:`T(0)=5` C, :math:`T_m=25` C, :math:`a=2.5` C, and :math:`P=1` h. Also experiment with
:math:`P=10` min and :math:`P=3` h. Plot :math:`T` and :math:`T_s` in the same plot.
Filename: ``osc_cooling.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:radio:C14:

Exercise 14: Radioactive decay of Carbon-14
-------------------------------------------

The `Carbon-14 <http://en.wikipedia.org/wiki/Carbon-14>`__ isotope,
whose radioactive decay is used extensively in dating organic material
that is tens of thousands of years old, has a half-life of :math:`5,730`
years.  Determine the age of an organic material that contains 8.4 percent
of its initial amount of Carbon-14.  Use a time unit of 1 year in the
computations.  The uncertainty in the half time of Carbon-14 is :math:`\pm
40` years.  What is the corresponding uncertainty in the estimate of
the age?

.. --- begin hint in exercise ---

**Hint.**
Use simulations with :math:`5,730\pm 40` y as input
and find the corresponding interval for the result.

.. --- end hint in exercise ---

Filename: ``carbon14.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:stoch:nuclear:

Exercise 15: Simulate stochastic radioactive decay
--------------------------------------------------

The purpose of this exercise is to implement the stochastic model
described in the section :ref:`decay:app:nuclear` and show that its
mean behavior approximates the solution of the corresponding
ODE model.

The simulation goes on for a time interval :math:`[0,T]` divided into
:math:`N_t` intervals of length :math:`\Delta t`. We start with :math:`N_0`
atoms. In some time interval, we have :math:`N` atoms that have survived.
Simulate :math:`N` Bernoulli trials with probability :math:`\lambda\Delta t`
in this interval by drawing :math:`N` random numbers, each being 0 (survival)
or 1 (decay), where the probability of getting 1 is :math:`\lambda\Delta t`.
We are interested in the number of decays, :math:`d`, and the number of
survived atoms in the next interval is then :math:`N-d`.
The Bernoulli trials
are simulated by drawing :math:`N` uniformly distributed real numbers on
:math:`[0,1]` and saying that 1 corresponds to a value less than :math:`\lambda\Delta t`:

.. code-block:: python

        # Given lambda_, dt, N
        import numpy as np
        uniform = np.random.uniform(N)
        Bernoulli_trials = np.asarray(uniform < lambda_*dt, dtype=np.int)
        d = Bernoulli_trials.size

Observe that ``uniform < lambda_*dt`` is a boolean array whose true
and false values become 1 and 0, respectively, when converted to an
integer array.

Repeat the simulation over :math:`[0,T]` a large number of times, compute the average
value of :math:`N` in each interval, and compare with the solution of
the corresponding ODE model.
Filename: ``stochastic_decay.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:radio:twosubst:

Exercise 16: Radioactive decay of two substances
------------------------------------------------

Consider two radioactive substances A and B. The nuclei in substance A
decay to form nuclei of type B with a half-life :math:`A_{1/2}`, while
substance B decay to form type A nuclei with a half-life :math:`B_{1/2}`.
Letting :math:`u_A` and :math:`u_B` be the fractions of the initial amount of
material in substance A and B, respectively, the following system of
ODEs governs the evolution of :math:`u_A(t)` and :math:`u_B(t)`:

.. math::
        
        \frac{1}{\ln 2} u_A' = u_B/B_{1/2} - u_A/A_{1/2},
        

.. math::
          
        \frac{1}{\ln 2} u_B' = u_A/A_{1/2} - u_B/B_{1/2},
        

with :math:`u_A(0)=u_B(0)=1`.

Make a simulation program that solves for :math:`u_A(t)` and :math:`u_B(t)`.
Verify the implementation by computing analytically
the limiting values of
:math:`u_A` and :math:`u_B` as :math:`t\rightarrow \infty` (assume :math:`u_A',u_B'\rightarrow 0`)
and comparing these with those obtained numerically.

Run the program for the case of :math:`A_{1/2}=10` minutes and :math:`B_{1/2}=50` minutes.
Use a time unit of 1 minute. Plot :math:`u_A` and :math:`u_B` versus time in the same
plot.
Filename: ``radioactive_decay_2subst.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:atm1:

Exercise 17: Simulate the pressure drop in the atmosphere
---------------------------------------------------------

We consider the models for atmospheric pressure in
the section :ref:`decay:app:atm`.
Make a program with three functions,

 * one computing the pressure :math:`p(z)` using a seven-layer model
   and varying :math:`L`,

 * one computing :math:`p(z)` using a seven-layer model,
   but with constant temperature in each layer, and

 * one computing :math:`p(z)` based on the
   one-layer model.

How can these implementations be verified? Should ease of verification
impact how you code the functions?
Compare the three models in a plot.
Filename: ``atmospheric_pressure.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:drag:prog:

Exercise 18: Make a program for vertical motion in a fluid
----------------------------------------------------------

Implement the Stokes' drag model :eq:`decay:app:fallingbody:model:S`
and the quadratic drag model :eq:`decay:app:fallingbody:model:q` from
the section :ref:`decay:app:drag`, using the Crank-Nicolson
scheme and a geometric mean for :math:`|v|v` as explained, and assume
constant fluid density.
At each time level, compute the Reynolds number
Re and choose the Stokes' drag model if :math:`\hbox{Re} < 1` and the
quadratic drag model otherwise.

The computation of the numerical solution should take place either in
a stand-alone function (as in the section :ref:`decay:py1`) or in a solver class
that looks up a problem class for physical data (as in the section :ref:`decay:prog:se:class`). Create a module (see the section :ref:`decay:prog:se:module`) and equip it with nose tests (see the section :ref:`decay:prog:se:nose`) for automatically verifying the code.

Verification tests can be based on

 * the terminal velocity (see the section :ref:`decay:app:drag`),

 * the exact solution when the drag force is neglected
   (see the section :ref:`decay:app:drag`),

 * the method of manufactured solutions (see the section :ref:`decay:MMS`)
   combined with computing
   convergence rates (see the section :ref:`decay:convergence:rate`).

Use, e.g., a quadratic polynomial for the velocity in the method of
manufactured solutions. The expected error is :math:`{\mathcal{O}(\Delta t^2)}`
from the centered finite difference approximation and the geometric
mean approximation for :math:`|v|v`.

A solution that is linear in :math:`t` will also be an exact solution of the
discrete equations in many problems.  Show that this is true for
linear drag (by adding a source term that depends on :math:`t`), but not
for quadratic drag because of the geometric mean approximation.  Use
the method of manufactured solutions to add a source term *in the
discrete equations for quadratic drag* such that a linear function of
:math:`t` is a solution. Add a nose test for checking that the linear
function is reproduced to machine precision in the case of both linear
and quadratic drag.

Apply the software to a case where a ball rises in water.  The
buoyancy force is here the driving force, but the drag will be
significant and balance the other forces after a short time.  A soccer
ball has radius 11 cm and mass 0.43 kg.  Start the motion from rest, set
the density of water, :math:`\varrho`, to :math:`1000\hbox{ kg/m}^3`, set the
dynamic viscosity, :math:`\mu`, to :math:`10^{-3}\hbox{ Pa s}`, and use a drag
coefficient for a sphere: 0.45. Plot the velocity of the rising ball.
Filename: ``vertical_motion.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:parachute:

Project 19: Simulate parachuting
--------------------------------

The aim of this project is to develop a general solver for the
vertical motion of a body with quadratic air drag, verify the solver,
apply the solver to a skydiver in free fall, and finally apply the
solver to a complete parachute jump.

All the pieces of software implemented in this project
should be realized as Python functions and/or classes and collected
in one module.

**a)**
Set up the differential equation problem that governs the velocity
of the motion.
The parachute jumper is subject to the gravity force and a quadratic
drag force. Assume constant density.
Add an extra source term be used for program verification.
Identify the input data to the problem.

**b)**
Make a Python module for computing the velocity of the motion.
Also equip the module with functionality for plotting the velocity.

.. --- begin hint in exercise ---

**Hint 1.**
Use the Crank-Nicolson scheme with a geometric mean of :math:`|v|v` in time to
linearize the equation of motion with quadratic drag.

.. --- end hint in exercise ---

.. --- begin hint in exercise ---

**Hint 2.**
You can either use functions or classes for implementation.
If you choose functions, make a function
``solver`` that takes all the input data in the problem as
arguments and that returns the velocity (as a mesh function) and
the time mesh. In case of a class-based implementation, introduce
a problem class with the physical data
and a solver class with the numerical data and a ``solve`` method
that stores the velocity and the mesh in the class.

Allow for a time-dependent area and drag coefficient in the
formula for the drag force.

.. --- end hint in exercise ---

**c)**
Show that a linear function of :math:`t` does not fulfill the discrete
equations because of the geometric mean approximation
used for the quadratic drag
term.  Fit a source term, as in the method of manufactured solutions,
such that a linear function of :math:`t` is a solution of the discrete
equations. Make a nose test to check that this solution is reproduced
to machine precision.

**d)**
The expected error in this problem goes like :math:`\Delta t^2` because we
use a centered finite difference approximation with error :math:`{\mathcal{O}(\Delta t^2)}`
and a geometric mean approximation with error :math:`{\mathcal{O}(\Delta t^2)}`.
Use the method of manufactured solutions combined with computing
convergence rate to verify the code. Make a nose test for checking
that the convergence rate is correct.

**e)**
Compute the drag force, the gravity
force, and the buoyancy force as a function of time. Create
a plot with these three forces.

.. --- begin hint in exercise ---

**Hint.**
You can either make a function ``forces(v, t, plot=None)``
that returns the forces (as mesh functions) and ``t`` and shows
a plot on the screen and also saves the plot to a file with name ``plot``
if ``plot`` is not ``None``, or you can extend the solver class with
computation of forces and include plotting of forces in the
visualization class.

.. --- end hint in exercise ---

**f)**
Compute the velocity of
a skydiver in free fall before the parachute opens.

.. --- begin hint in exercise ---

**Hint.**
Meade and Struthers [Ref6]_ provide some data relevant
to `skydiving <http://en.wikipedia.org/wiki/Parachuting>`__.
The mass of the human body and equipment
can be set to :math:`100` kg.
A skydiver in spread-eagle formation has a cross-section of 0.5 :math:`\hbox{m}^2`
in the horizontal plane.
The density of air decreases varies altitude, but can be taken
as constant, 1 :math:`\hbox{kg/m}^3`, for altitudes relevant to
skydiving (0-4000 m).
The drag coefficient for a man in upright position can be set to 1.2.
Start with a zero velocity.
A free fall typically has a terminating velocity of 45 m/s. (This value
can be used to tune other parameters.)

.. --- end hint in exercise ---

**g)**
The next task is to simulate
a parachute jumper during free fall and after the parachute opens.
At time :math:`t_p`, the parachute opens and
the drag coefficient and the cross-sectional
area change dramatically.
Use the program to simulate a jump from :math:`z=3000` m to the ground :math:`z=0`.
What is the maximum acceleration, measured in units of :math:`g`,
experienced by the jumper?

.. --- begin hint in exercise ---

**Hint.**
Following Meade and Struthers [Ref6]_, one can set the
cross-section area perpendicular to the motion to 44 :math:`\hbox{m}^2`
when the parachute is open. Assume that it takes 8 s to increase
the area linearly from the original to the final value.
The drag coefficient for an open
parachute can be taken as 1.8, but tuned using the known value
of the typical terminating velocity reached before landing:
5.3 m/s. One can take the drag coefficient as a piecewise constant
function with an abrupt change at :math:`t_p`.
The parachute is typically released after :math:`t_p=60` s, but
larger values of :math:`t_p` can be used to make plots more illustrative.

.. --- end hint in exercise ---

Filename: ``skydiving.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:drag:atm1:

Exercise 20: Formulate vertical motion in the atmosphere
--------------------------------------------------------

Vertical motion of a body in the atmosphere needs to take into
account a varying air density if the range of altitudes is
many kilometers. In this case, :math:`\varrho` varies with the altitude :math:`z`.
The equation of motion for the body is given in
the section :ref:`decay:app:drag`. Let us assume quadratic drag force
(otherwise the body has to be very, very small).
A differential equation problem for the air density, based on
the information for the one-layer atmospheric model in
the section :ref:`decay:app:atm`, can be set up as

.. math::
        
        p'(z) = -\frac{Mg}{R^*(T_0+Lz)} p,
        

.. math::
          
        \varrho = p \frac{M}{R^*T}
        {\thinspace .}
        

To evaluate :math:`p(z)` we need the altitude :math:`z`. From the principle that the
velocity is the derivative of the position we have that

.. math::
        
        z'(t) = v(t),
        

where :math:`v` is the velocity of the body.

Explain in detail how the governing equations can be discretized
by the Forward Euler and the Crank-Nicolson methods.
Filename: ``falling_in_variable_density.pdf``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:drag:atm2:

Exercise 21: Simulate vertical motion in the atmosphere
-------------------------------------------------------

Implement the Forward Euler or the Crank-Nicolson scheme
derived in :ref:`decay:app:exer:drag:atm1`.
Demonstrate the effect of air density variation on a falling
human, e.g., the famous fall of `Felix Baumgartner <http://en.wikipedia.org/wiki/Felix_Baumgartner>`__. The drag coefficient can be set to 1.2.

**Remark.**
In the Crank-Nicolson scheme one must solve a :math:`3\times 3` system of
equations at each time level, since :math:`p`, :math:`\varrho`, and :math:`v` are
coupled, while each equation can be stepped forward at a time with the
Forward Euler scheme.
Filename: ``falling_in_variable_density.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:signum:

Exercise 22: Compute :math:`y=|x|` by solving an ODE
----------------------------------------------------

Consider the ODE problem

.. math::
        
        y'(x) = \left\lbrace\begin{array}{ll}
        -1, & x < 0,\\ 
        1, & x \geq 0
        \end{array}\right.\quad x\in (-1, 1],
        \quad y(1-)=1,
        

which has the solution :math:`y(x)=|x|`.
Using a mesh :math:`x_0=-1`, :math:`x_1=0`, and :math:`x_2=1`, calculate by hand
:math:`y_1` and :math:`y_2` from the Forward Euler, Backward Euler, Crank-Nicolson,
and Leapfrog methods. Use all of the former three methods for computing
the :math:`y_1` value to be used in the Leapfrog calculation of :math:`y_2`.
Thereafter, visualize how these schemes perform for a uniformly partitioned
mesh with :math:`N=10` and :math:`N=11` points.
Filename: ``signum.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:interest:

Exercise 23: Simulate growth of a fortune with random interest rate
-------------------------------------------------------------------

The goal of this exercise is to compute the value of a fortune subject
to inflation and a random interest rate.
Suppose that the inflation is constant at :math:`i` percent per year and that the
annual interest rate, :math:`p`, changes randomly at each time step,
starting at some value :math:`p_0` at :math:`t=0`.
The random change is from a value :math:`p^n` at :math:`t=t_n` to
:math:`p_n +\Delta p` with probability 0.25 and :math:`p_n -\Delta p` with probability 0.25.
No change occurs with probability 0.5. There is also no change if
:math:`p^{n+1}` exceeds 15 or becomes below 1.
Use a time step of one month, :math:`p_0=i`, initial fortune scaled to 1,
and simulate 1000 scenarios of
length 20 years. Compute the mean evolution of one unit of money and the
corresponding
standard deviation. Plot the mean curve along with the mean plus one
standard deviation and the mean minus one standard deviation. This will
illustrate the uncertainty in the mean curve.

.. --- begin hint in exercise ---

**Hint 1.**
The following code snippet computes :math:`p^{n+1}`:

.. code-block:: python

        import random
        
        def new_interest_rate(p_n, dp=0.5):
            r = random.random()  # uniformly distr. random number in [0,1)
            if 0 <= r < 0.25:
                p_np1 = p_n + dp
            elif 0.25 <= r < 0.5:
                p_np1 = p_n - dp
            else:
                p_np1 = p_n
            return (p_np1 if 1 <= p_np1 <= 15 else p_n)

.. --- end hint in exercise ---

.. --- begin hint in exercise ---

**Hint 2.**
If :math:`u_i(t)` is the value of the fortune in experiment number :math:`i`,
:math:`i=0,\ldots,N-1`,
the mean evolution of the fortune is

.. math::
         \bar u(t)= \frac{1}{N}\sum_{i=0}^{N-1} u_i(t),
        

and the standard deviation is

.. math::
         s(t) = \sqrt{\frac{1}{N-1}\left(- (\bar u(t))^2 +
                        \sum_{i=0}^{N-1} (u_i(t))^2\right)}
        {\thinspace .}
        

Suppose :math:`u_i(t)` is stored in an array ``u``.
The mean and the standard deviation of the fortune
is most efficiently computed by
using two accumulation arrays, ``sum_u`` and ``sum_u2``, and
performing ``sum_u += u`` and ``sum_u2 += u**2`` after every experiment.
This technique avoids storing all the :math:`u_i(t)` time series for
computing the statistics.

.. --- end hint in exercise ---

Filename: ``random_interest.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:pop:at:

Exercise 24: Simulate a population in a changing environment
------------------------------------------------------------

We shall study a population modeled by :eq:`decay:app:pop:ueq` where
the environment, represented by :math:`r` and :math:`f`, undergoes changes with time.

**a)**
Assume that there is a sudden drop (increase) in the birth (death)
rate at time :math:`t=t_r`,
because of limited nutrition or food supply:

.. math::
         a(t) =\left\lbrace\begin{array}{ll}
        r_0, & t< t_r,\\ 
        r_0 - A, & t\geq t_r,\end{array}\right.
        

This drop in population growth is compensated by a sudden net immigration
at time :math:`t_f>t_r`:

.. math::
         f(t) =\left\lbrace\begin{array}{ll}
        0, & t< t_f,\\ 
        f_0, & t\geq t_a,\end{array}\right.
        

Start with :math:`r_0` and make :math:`A>r_0`. Experiment with
these and other parameters to
illustrate the interplay of growth and decay in such a problem.
Filename: ``population_drop.py``.

**b)**
Now we assume that the environmental conditions changes periodically with
time so that we may take

.. math::
         r(t) = r_0 + A\sin\left(\frac{2\pi}{P}t\right)
        {\thinspace .}
        

That is, the combined birth and death rate oscillates around :math:`r_0` with
a maximum change of :math:`\pm A` repeating over a period of length :math:`P` in time.
Set :math:`f=0` and experiment with the other parameters to illustrate typical
features of the solution.
Filename: ``population_osc.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:pop:logistic1:

Exercise 25: Simulate logistic growth
-------------------------------------

Solve the logistic ODE
:eq:`decay:app:pop:logistic` using a Crank-Nicolson scheme where
:math:`(u^{n+\frac{1}{2}})^2` is approximated by a *geometric mean*:

.. math::
         (u^{n+\frac{1}{2}})^2 \approx u^{n+1}u^n
        {\thinspace .}
        

This trick makes the discrete equation linear in :math:`u^{n+1}`.
Filename: ``logistic_CN.py``.

.. --- end exercise ---

.. --- begin exercise ---

.. _decay:app:exer:interest:derive:

Exercise 26: Rederive the equation for continuous compound interest
-------------------------------------------------------------------

The ODE model :eq:`decay:app:interest:eq2` was derived under the assumption
that :math:`r` was constant. Perform an alternative derivation without
this assumption: 1) start with :eq:`decay:app:interest:eq1`;
2) introduce a time step :math:`\Delta t` instead of :math:`m`: :math:`\Delta t = 1/m` if
:math:`t` is measured in years; 3) divide by :math:`\Delta t` and take the
limit :math:`\Delta t\rightarrow 0`. Simulate a case where the inflation is
at a constant level :math:`I` percent per year and the interest rate oscillates:
:math:`r=-I/2 + r_0\sin(2\pi t)`.
Compare solutions for :math:`r_0=I, 3I/2, 2I`.
Filename: ``interest_modeling.py``.

.. --- end exercise ---

.. !split

Bibliography
============

.. [Ref1]
   **H. P. Langtangen**. *A Primer on Scientific Programming With Python*,
   Springer,
   2012.

.. [Ref2]
   **L. Petzold and U. M. Ascher**. *Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations*,
   SIAM,
   1998.

.. [Ref3]
   **D. Griffiths, F. David and D. J. Higham**. *Numerical Methods for Ordinary Differential Equations: Initial Value Problems*,
   Springer,
   2010.

.. [Ref4]
   **E. Hairer, S. P. N\orsett and G. Wanner**. *Solving Ordinary Differential Equations I. Nonstiff Problems*,
   Springer,
   1993.

.. [Ref5]
   **G. Hairer and E. Wanner**. *Solving Ordinary Differential Equations II*,
   Springer,
   2010.

.. [Ref6]
   **D. B. Meade and A. A. Struthers**. Differential Equations in the New Millenium: the Parachute Problem,
   *International Journal of Engineering Education*,
   15(6),
   pp. 417-424,
   1999,
   `http://www.matematicas.unam.mx/gfgf/ode/ode_files/parachute.pdf <http://www.matematicas.unam.mx/gfgf/ode/ode_files/parachute.pdf>`_.

